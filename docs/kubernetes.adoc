= Curso Completo de Kubernetes
:doctype: book
:toc:
:toclevels: 4
:sectnums:
:icons: font
:source-highlighter: highlight.js
:highlightjsdir: https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0
:highlightjs-theme: atom-one-light
:data-uri:

== Módulo 1: Introducción a Kubernetes

=== ¿Qué es Kubernetes?

Kubernetes, también conocido como K8s (por las 8 letras entre la 'K' y la 's'), es un sistema de orquestación de contenedores de código abierto que automatiza el despliegue, escalado y gestión de aplicaciones en contenedores.

==== Historia y origen del proyecto

Kubernetes fue originalmente diseñado por Google y es el resultado de más de una década de experiencia ejecutando cargas de trabajo de producción a escala usando sistemas internos como Borg y Omega.

**Cronología:**

* *2003-2004*: Google desarrolla internamente Borg, su primer sistema de gestión de contenedores
* *2013*: Google comienza a desarrollar Kubernetes basándose en las lecciones aprendidas de Borg
* *Junio 2014*: Google libera Kubernetes como proyecto de código abierto
* *Julio 2015*: Se lanza Kubernetes v1.0 y Google dona el proyecto a la Cloud Native Computing Foundation (CNCF)
* *2016-presente*: Kubernetes se convierte en el estándar de facto para orquestación de contenedores

==== Problemas que resuelve Kubernetes

Kubernetes aborda varios desafíos críticos en la gestión de aplicaciones modernas:

**1. Gestión de contenedores a escala**

Sin Kubernetes, gestionar cientos o miles de contenedores manualmente es prácticamente imposible. K8s automatiza:

* Programación y distribución de contenedores en un cluster
* Reinicio automático de contenedores fallidos
* Reemplazo y reprogramación de contenedores cuando los nodos fallan

**2. Alta disponibilidad**

Kubernetes garantiza que las aplicaciones estén siempre disponibles mediante:

* Réplicas automáticas de aplicaciones
* Distribución de carga entre múltiples instancias
* Health checks y auto-recuperación

**3. Escalabilidad**

Permite escalar aplicaciones de forma:

* *Horizontal*: Agregar o quitar instancias automáticamente según la demanda
* *Vertical*: Ajustar recursos (CPU, memoria) asignados a contenedores
* *De cluster*: Agregar o quitar nodos del cluster según sea necesario

**4. Portabilidad**

Kubernetes funciona en cualquier infraestructura:

* On-premise (en centros de datos propios)
* Cloud público (AWS, GCP, Azure)
* Cloud híbrido
* Multi-cloud

**5. Despliegues y actualizaciones sin tiempo de inactividad**

* Rolling updates: actualizaciones graduales
* Rollbacks automáticos si algo falla
* Canary deployments para probar nuevas versiones

**6. Gestión de configuración y secretos**

* Separación de código y configuración
* Gestión segura de credenciales y secretos
* ConfigMaps para configuraciones dinámicas

==== Comparación con otras soluciones de orquestación

[cols="1,3,3,3", options="header"]
|===
|Característica
|Kubernetes
|Docker Swarm
|Apache Mesos

|Complejidad
|Alta, pero muy flexible
|Baja, fácil de comenzar
|Muy alta

|Escalabilidad
|Excelente (miles de nodos)
|Buena (cientos de nodos)
|Excelente (decenas de miles)

|Comunidad
|Muy grande y activa
|Moderada
|Pequeña

|Ecosistema
|Muy rico (CNCF)
|Limitado
|Moderado

|Soporte empresarial
|Amplio (todos los principales clouds)
|Limitado
|Limitado

|Curva de aprendizaje
|Empinada
|Suave
|Muy empinada

|Auto-recuperación
|Excelente
|Buena
|Buena

|Load balancing
|Integrado y avanzado
|Básico
|Requiere configuración

|Casos de uso
|General, microservicios
|Aplicaciones simples
|Big data, analytics
|===

**Ventajas de Kubernetes:**

* Estándar de la industria con amplio soporte
* Ecosistema maduro con miles de herramientas
* Soporte nativo de todos los principales proveedores cloud
* Arquitectura extensible mediante CRDs y Operators
* Comunidad activa y documentación exhaustiva

**Desventajas:**

* Curva de aprendizaje pronunciada
* Mayor complejidad operacional
* Requiere más recursos para clusters pequeños
* Puede ser excesivo para aplicaciones simples

==== El ecosistema Cloud Native Computing Foundation (CNCF)

La Cloud Native Computing Foundation es una organización sin fines de lucro fundada en 2015 como parte de la Linux Foundation. Su misión es hacer que la computación nativa de la nube sea ubicua.

**Kubernetes en el contexto de CNCF:**

Kubernetes es el proyecto "graduado" más importante de CNCF, pero el ecosistema incluye muchos otros proyectos complementarios:

**Proyectos CNCF relacionados con Kubernetes:**

[cols="1,2,3", options="header"]
|===
|Categoría
|Proyectos
|Propósito

|Container Runtime
|containerd, CRI-O
|Ejecutar contenedores

|Networking
|Calico, Cilium, Flannel
|Redes de contenedores

|Service Mesh
|Istio, Linkerd, Envoy
|Gestión de tráfico entre servicios

|Monitoring
|Prometheus, Grafana
|Métricas y visualización

|Logging
|Fluentd
|Agregación de logs

|Tracing
|Jaeger, OpenTelemetry
|Trazado distribuido

|Storage
|Rook, Longhorn
|Almacenamiento persistente

|CI/CD
|Argo, Flux, Tekton
|Despliegue continuo

|Security
|Falco, OPA
|Seguridad y políticas

|Package Management
|Helm
|Gestión de aplicaciones K8s
|===

**Niveles de madurez en CNCF:**

1. *Sandbox*: Proyectos experimentales
2. *Incubating*: Proyectos en crecimiento con adopción moderada
3. *Graduated*: Proyectos maduros con adopción amplia (como Kubernetes, Prometheus, Envoy)

**Cloud Native principles:**

La CNCF promueve aplicaciones que son:

* *Containerized*: Empaquetadas en contenedores
* *Dynamically orchestrated*: Gestionadas activamente por plataformas de orquestación
* *Microservices-oriented*: Construidas como servicios pequeños e independientes

**Beneficios del ecosistema:**

* Interoperabilidad entre herramientas
* Estándares abiertos
* Neutralidad de proveedores
* Innovación rápida
* Soporte comunitario

=== Arquitectura de Kubernetes

Kubernetes sigue una arquitectura cliente-servidor distribuida que separa claramente el plano de control (control plane) de los nodos de trabajo (worker nodes). Esta separación permite una alta disponibilidad y escalabilidad.

**Arquitectura general:**

----
┌─────────────────────────────────────────────────────────┐
│                    CONTROL PLANE                        │
│  ┌──────────┐  ┌──────┐  ┌───────────┐  ┌────────────┐  │
│  │   API    │  │      │  │ Scheduler │  │ Controller │  │
│  │  Server  │  │ etcd │  │           │  │  Manager   │  │
│  └──────────┘  └──────┘  └───────────┘  └────────────┘  │
└─────────────────────────────────────────────────────────┘
                          │
        ┌─────────────────┼─────────────────┐
        │                 │                 │
┌───────▼──────┐  ┌───────▼──────┐  ┌──────▼───────┐
│   WORKER     │  │   WORKER     │  │   WORKER     │
│    NODE 1    │  │    NODE 2    │  │    NODE 3    │
│              │  │              │  │              │
│  ┌────────┐  │  │  ┌────────┐  │  │  ┌────────┐  │
│  │Kubelet │  │  │  │Kubelet │  │  │  │Kubelet │  │
│  ├────────┤  │  │  ├────────┤  │  │  ├────────┤  │
│  │Kube-   │  │  │  │Kube-   │  │  │  │Kube-   │  │
│  │proxy   │  │  │  │proxy   │  │  │  │proxy   │  │
│  ├────────┤  │  │  ├────────┤  │  │  ├────────┤  │
│  │Runtime │  │  │  │Runtime │  │  │  │Runtime │  │
│  └────────┘  │  │  └────────┘  │  │  └────────┘  │
│              │  │              │  │              │
│  [Pods...]   │  │  [Pods...]   │  │  [Pods...]   │
└──────────────┘  └──────────────┘  └──────────────┘
----

==== Componentes del Control Plane

El Control Plane (plano de control) es el cerebro del cluster de Kubernetes. Toma decisiones globales sobre el cluster y detecta y responde a eventos del cluster.

===== API Server

**kube-apiserver** es el componente central del Control Plane y actúa como frontend para Kubernetes.

**Funciones principales:**

* Expone la API de Kubernetes (RESTful)
* Punto de entrada para todos los comandos administrativos
* Valida y configura datos para objetos de la API
* Única interfaz que interactúa directamente con etcd
* Gestiona autenticación, autorización y admission control

**Características:**

* Diseñado para escalar horizontalmente (múltiples instancias)
* Maneja todas las operaciones CRUD sobre objetos del cluster
* Sirve como puente entre todos los componentes del cluster

**Ejemplo de interacción:**

[source,bash]
----
# Cuando ejecutas un comando kubectl, este se comunica con el API Server
kubectl get pods

# Internamente hace una petición HTTP como:
GET /api/v1/namespaces/default/pods
Host: api-server.example.com
Authorization: Bearer <token>
----

**Puerto por defecto:** 6443 (HTTPS)

===== etcd

**etcd** es un almacén de datos distribuido, consistente y de alta disponibilidad que Kubernetes usa como base de datos para toda la información del cluster.

**Funciones principales:**

* Almacena toda la configuración y estado del cluster
* Única fuente de verdad (single source of truth)
* Almacén clave-valor distribuido
* Proporciona consistencia mediante algoritmo Raft

**Datos almacenados:**

* Configuración del cluster
* Estado actual y deseado de todos los objetos
* Secretos y ConfigMaps
* Información de nodos, pods, servicios, etc.

**Características importantes:**

* Altamente disponible y distribuido
* Soporta replicación (normalmente 3, 5 o 7 instancias)
* Comunicación cifrada (TLS)
* Backups críticos para disaster recovery

**Ejemplo de estructura de datos:**

----
/registry/pods/default/nginx-pod
/registry/services/default/nginx-service
/registry/secrets/default/db-password
/registry/configmaps/default/app-config
----

**Puerto por defecto:** 2379 (cliente), 2380 (peer)

[IMPORTANT]
====
etcd es crítico para el funcionamiento del cluster. La pérdida de datos de etcd significa la pérdida de todo el estado del cluster. Siempre mantén backups regulares.
====

===== Scheduler

**kube-scheduler** es responsable de asignar pods recién creados a nodos específicos del cluster.

**Funciones principales:**

* Monitorea pods sin nodo asignado
* Selecciona el mejor nodo para cada pod
* Considera múltiples factores en la decisión
* No ejecuta los pods (eso lo hace kubelet)

**Factores de decisión:**

1. *Requisitos de recursos:*
   - CPU y memoria solicitadas
   - Recursos disponibles en cada nodo

2. *Restricciones:*
   - Node selectors
   - Affinity/Anti-affinity
   - Taints y tolerations

3. *Optimización:*
   - Distribución de carga
   - Localidad de datos
   - Hardware específico (GPU, SSD)

**Proceso de scheduling:**

----
1. Filtrado (Filtering)
   └─> Elimina nodos que no cumplen requisitos

2. Scoring (Puntuación)
   └─> Asigna puntuación a nodos viables

3. Binding (Vinculación)
   └─> Asigna el pod al nodo con mayor puntuación
----

**Ejemplo de decisión:**

[source,yaml]
----
# Pod solicitando 500m CPU y 1Gi memoria
apiVersion: v1
kind: Pod
metadata:
  name: resource-pod
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        cpu: "500m"
        memory: "1Gi"

# Scheduler evaluará:
# - Nodo A: 2 CPU disponibles, 4Gi RAM → Viable ✓
# - Nodo B: 0.3 CPU disponibles, 2Gi RAM → Descartado ✗
# - Nodo C: 1 CPU disponible, 512Mi RAM → Descartado ✗
----

===== Controller Manager

**kube-controller-manager** ejecuta múltiples controladores que regulan el estado del cluster.

**Concepto de controlador:**

Un controlador es un bucle de control que observa el estado del cluster y hace cambios para mover el estado actual hacia el estado deseado.

----
while true:
  estado_actual = observar_cluster()
  estado_deseado = leer_especificacion()

  if estado_actual != estado_deseado:
    realizar_cambios()

  sleep(intervalo)
----

**Controladores principales:**

1. *Node Controller*
   - Monitorea el estado de los nodos
   - Detecta nodos caídos
   - Responde cuando los nodos fallan

2. *Replication Controller*
   - Mantiene el número correcto de pods para cada ReplicaSet
   - Crea o destruye pods según sea necesario

3. *Endpoints Controller*
   - Popula objetos Endpoints (une Services y Pods)

4. *Service Account & Token Controllers*
   - Crea cuentas y tokens por defecto para namespaces

5. *Deployment Controller*
   - Gestiona actualizaciones y rollbacks de Deployments

6. *Job Controller*
   - Supervisa Jobs y crea Pods para ejecutarlos

**Ejemplo de reconciliación:**

[source,yaml]
----
# Usuario especifica: "Quiero 3 réplicas de nginx"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3  # Estado deseado
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21

# Replication Controller observa:
# - Estado actual: 2 pods corriendo
# - Estado deseado: 3 pods
# - Acción: Crear 1 pod adicional
----

===== Cloud Controller Manager

**cloud-controller-manager** permite que el código de Kubernetes y el del proveedor cloud evolucionen independientemente.

**Funciones principales:**

* Integración con APIs de proveedores cloud
* Gestión de recursos específicos del cloud
* Abstracción de la infraestructura subyacente

**Controladores cloud-specific:**

1. *Node Controller*
   - Verifica si los nodos eliminados han sido borrados en el cloud
   - Actualiza nodos con información del cloud (zona, región, tipo de instancia)

2. *Route Controller*
   - Configura rutas en la infraestructura cloud
   - Necesario para comunicación entre pods en diferentes nodos

3. *Service Controller*
   - Crea/actualiza/elimina load balancers del cloud
   - Asigna IPs públicas a Services tipo LoadBalancer

**Ejemplo - Service LoadBalancer en AWS:**

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web-app
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
spec:
  type: LoadBalancer  # Cloud Controller Manager crea un ELB/NLB
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 8080

# Cloud Controller Manager:
# 1. Detecta Service tipo LoadBalancer
# 2. Llama a AWS API para crear Network Load Balancer
# 3. Configura health checks
# 4. Actualiza Service con IP pública del NLB
----

==== Componentes de los Nodos

Los nodos worker son las máquinas (físicas o virtuales) donde se ejecutan las aplicaciones containerizadas.

===== Kubelet

**kubelet** es el agente principal que se ejecuta en cada nodo worker.

**Funciones principales:**

* Registra el nodo en el API Server
* Monitorea Pods asignados a su nodo
* Ejecuta contenedores a través del container runtime
* Reporta el estado de pods y nodos al API Server
* Ejecuta health checks (liveness, readiness probes)

**Responsabilidades:**

1. *Pod Lifecycle Management*
   - Descarga imágenes de contenedores
   - Inicia y detiene contenedores
   - Reinicia contenedores fallidos

2. *Volume Management*
   - Monta volúmenes según especificación
   - Gestiona plugins de volumen

3. *Resource Monitoring*
   - Monitorea uso de CPU, memoria, disco
   - Reporta métricas al API Server

**Flujo de trabajo:**

----
1. kubelet consulta API Server cada X segundos
   └─> "¿Hay pods asignados a mí?"

2. API Server responde con PodSpecs

3. kubelet compara estado actual vs deseado

4. Si hay diferencias:
   └─> Llama al Container Runtime para crear/actualizar/eliminar contenedores

5. kubelet reporta estado de vuelta al API Server
----

**Configuración importante:**

[source,yaml]
----
# /var/lib/kubelet/config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
address: 0.0.0.0
port: 10250
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
authorization:
  mode: Webhook
clusterDomain: cluster.local
clusterDNS:
  - 10.96.0.10
----

**Puerto por defecto:** 10250

===== Kube-proxy

**kube-proxy** es un proxy de red que mantiene reglas de red en cada nodo, permitiendo la comunicación con Pods.

**Funciones principales:**

* Implementa el concepto de Services
* Gestiona reglas de red/iptables en el nodo
* Balancea carga de tráfico hacia Pods
* Mantiene conectividad de red

**Modos de operación:**

1. *iptables mode* (por defecto)
   - Usa reglas iptables para redirección
   - Selección aleatoria de pods backend
   - Mejor rendimiento que userspace

2. *IPVS mode*
   - Usa Linux IPVS (IP Virtual Server)
   - Mejor rendimiento a gran escala
   - Más algoritmos de balanceo (round-robin, least-connection, etc.)

3. *userspace mode* (legacy)
   - kube-proxy actúa como proxy real
   - Menos eficiente

**Ejemplo de funcionamiento:**

[source,yaml]
----
# Service definition
apiVersion: v1
kind: Service
metadata:
  name: backend
spec:
  selector:
    app: backend
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP
  clusterIP: 10.96.100.50

# kube-proxy crea reglas iptables en cada nodo:
# "Todo tráfico a 10.96.100.50:80 → distribuir a pods backend:8080"

# Reglas iptables generadas (simplificado):
-A KUBE-SERVICES -d 10.96.100.50/32 -p tcp --dport 80 -j KUBE-SVC-BACKEND
-A KUBE-SVC-BACKEND -m statistic --mode random --probability 0.33 -j KUBE-SEP-POD1
-A KUBE-SVC-BACKEND -m statistic --mode random --probability 0.50 -j KUBE-SEP-POD2
-A KUBE-SVC-BACKEND -j KUBE-SEP-POD3
----

**Diagrama de tráfico:**

----
Cliente (en Pod A)
     │
     │ Solicita: backend:80
     ▼
[kube-proxy iptables]
     │
     ├─> 33% → Pod Backend 1 (10.244.1.5:8080)
     ├─> 33% → Pod Backend 2 (10.244.2.3:8080)
     └─> 34% → Pod Backend 3 (10.244.3.7:8080)
----

===== Container Runtime

El **Container Runtime** es el software responsable de ejecutar contenedores.

**Interfaz CRI (Container Runtime Interface):**

Kubernetes usa CRI para comunicarse con diferentes runtimes sin acoplamientos estrictos.

**Runtimes soportados:**

1. *containerd*
   - Runtime más popular actualmente
   - Graduado de CNCF
   - Usado por Docker Desktop, GKE, EKS
   - Ligero y eficiente

2. *CRI-O*
   - Diseñado específicamente para Kubernetes
   - Implementación ligera de OCI
   - Usado por OpenShift

3. *Docker Engine* (via cri-dockerd)
   - Soporte legacy mediante adaptador
   - Deprecated desde Kubernetes 1.24

**Responsabilidades del runtime:**

* Descargar imágenes de registros
* Desempaquetar imágenes
* Ejecutar contenedores
* Gestionar el ciclo de vida de contenedores
* Proporcionar interfaz CRI para kubelet

**Flujo de ejecución:**

----
kubelet
  │
  │ (CRI API calls)
  ▼
Container Runtime (containerd/CRI-O)
  │
  │ (OCI runtime spec)
  ▼
runc / crun / kata-containers
  │
  ▼
Linux Kernel (cgroups, namespaces)
  │
  ▼
[Container running]
----

**Ejemplo de configuración containerd:**

[source,toml]
----
# /etc/containerd/config.toml
[plugins."io.containerd.grpc.v1.cri"]
  sandbox_image = "registry.k8s.io/pause:3.9"

[plugins."io.containerd.grpc.v1.cri".containerd]
  snapshotter = "overlayfs"

  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
    runtime_type = "io.containerd.runc.v2"

    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
      SystemdCgroup = true
----

==== Addons

Además de los componentes core, Kubernetes usa Addons para extender funcionalidad:

**DNS (CoreDNS)**

* Servicio DNS para el cluster
* Permite resolución de nombres de Services
* Crítico para service discovery

**Dashboard**

* Interfaz web para gestionar el cluster
* Visualización de recursos
* No recomendado para producción (usar CLI/GitOps)

**Container Resource Monitoring**

* Recopila métricas de contenedores
* Metrics Server para HPA
* Prometheus para monitoring avanzado

**Cluster-level Logging**

* Agregación de logs
* EFK Stack (Elasticsearch, Fluentd, Kibana)
* Loki, Splunk, etc.

==== Flujo completo: De kubectl a Pod en ejecución

Veamos cómo interactúan todos los componentes cuando creas un Pod:

----
1. Usuario ejecuta:
   $ kubectl run nginx --image=nginx

2. kubectl → API Server
   └─> POST /api/v1/namespaces/default/pods

3. API Server:
   └─> Valida la petición
   └─> Autentica y autoriza
   └─> Ejecuta admission controllers
   └─> Escribe el Pod en etcd (estado: Pending, sin nodo asignado)

4. Scheduler (observando API Server):
   └─> Detecta Pod sin nodo
   └─> Evalúa nodos disponibles
   └─> Selecciona mejor nodo (ej: node-2)
   └─> Actualiza Pod en API Server: nodeName=node-2

5. API Server actualiza etcd con la asignación

6. kubelet en node-2 (polling API Server):
   └─> Detecta nuevo Pod asignado a su nodo
   └─> Llama a Container Runtime (containerd)
   └─> Container Runtime:
       └─> Descarga imagen nginx desde Docker Hub
       └─> Crea contenedor
       └─> Inicia contenedor

7. kubelet reporta estado a API Server:
   └─> Pod status: Running

8. API Server actualiza etcd

9. kube-proxy en todos los nodos:
   └─> Si el Pod es parte de un Service
   └─> Actualiza reglas iptables/IPVS
----

**Diagrama temporal:**

----
Tiempo  kubectl  API Server  etcd  Scheduler  kubelet  Runtime
  │       │         │         │        │         │        │
  0       ├────────>│ Crear   │        │         │        │
  │       │         │ Pod     │        │         │        │
  1       │         ├────────>│ Save   │         │        │
  │       │         │         │ Pending│         │        │
  2       │         │<────────┤        │         │        │
  │       │         │                  │         │        │
  3       │         │         │    Watch         │        │
  │       │         │         │<───────┘         │        │
  4       │         │         │        │ Assign  │        │
  │       │         │         │        │ node-2  │        │
  5       │         │ Update  │<───────┤         │        │
  │       │         ├────────>│ nodeName         │        │
  6       │         │         │        │       Watch      │
  │       │         │         │        │<────────┘        │
  7       │         │         │        │         │ Start  │
  │       │         │         │        │         ├───────>│
  8       │         │         │        │         │  Pull  │
  │       │         │         │        │         │  Run   │
  9       │         │ Update  │        │   Report│        │
  │       │         │<────────┼────────┼─────────┤        │
 10       │         ├────────>│ Running│         │        │
  │       │         │         │        │         │        │
  ▼       ▼         ▼         ▼        ▼         ▼        ▼
----

=== Conceptos Fundamentales

Kubernetes introduce varios conceptos fundamentales que son esenciales para entender cómo funciona el sistema. Estos conceptos son la base sobre la que se construyen todas las funcionalidades avanzadas.

==== Clusters, Nodos y Pods

**Cluster**

Un cluster de Kubernetes es un conjunto de máquinas (nodos) que ejecutan aplicaciones containerizadas gestionadas por Kubernetes.

----
┌─────────────────────────────────────────┐
│         KUBERNETES CLUSTER              │
│                                         │
│  ┌────────────────┐   ┌──────────────┐  │
│  │ Control Plane  │   │ Worker Nodes │  │
│  │    Nodes       │   │              │  │
│  │  (1 o más)     │   │  (1 o más)   │  │
│  └────────────────┘   └──────────────┘  │
└─────────────────────────────────────────┘
----

**Componentes de un cluster:**

* *Control Plane Nodes*: Gestionan el cluster
* *Worker Nodes*: Ejecutan las aplicaciones
* *Networking*: Comunicación entre componentes
* *Storage*: Sistemas de almacenamiento

**Nodos (Nodes)**

Un nodo es una máquina de trabajo (física o virtual) en Kubernetes donde se ejecutan los Pods.

**Tipos de nodos:**

1. *Master/Control Plane Nodes*
   - Ejecutan componentes del control plane
   - Generalmente no ejecutan aplicaciones de usuario
   - Pueden ser múltiples para alta disponibilidad

2. *Worker Nodes*
   - Ejecutan los Pods con las aplicaciones
   - Contienen kubelet, kube-proxy y container runtime
   - Pueden ser escalados según demanda

**Información de un nodo:**

[source,bash]
----
kubectl get nodes

# Salida:
NAME           STATUS   ROLES           AGE   VERSION
control-plane  Ready    control-plane   30d   v1.28.0
worker-1       Ready    <none>          30d   v1.28.0
worker-2       Ready    <none>          30d   v1.28.0
worker-3       Ready    <none>          30d   v1.28.0

# Ver detalles de un nodo:
kubectl describe node worker-1
----

**Información que muestra:**

* Capacidad (CPU, memoria, pods máximos)
* Condiciones (Ready, DiskPressure, MemoryPressure)
* Pods en ejecución
* Recursos asignados vs disponibles
* Sistema operativo e información de kernel

**Pods**

Un Pod es la unidad más pequeña desplegable en Kubernetes. Representa una o más contenedores que deben ejecutarse juntos en el mismo nodo.

**Características de los Pods:**

* Comparten dirección IP
* Comparten namespace de red
* Pueden compartir volúmenes
* Se programan juntos en el mismo nodo
* Escalan como unidad
* Efímeros por naturaleza (no persistentes)

**Ciclo de vida:**

----
Pending → Running → Succeeded/Failed
                  ↓
              CrashLoopBackOff (si falla repetidamente)
----

**Ejemplo de Pod simple:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx
    env: production
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    ports:
    - containerPort: 80
----

**Crear y gestionar el Pod:**

[source,bash]
----
# Crear el Pod
kubectl apply -f nginx-pod.yaml

# Ver Pods
kubectl get pods

# Ver detalles
kubectl describe pod nginx-pod

# Ver logs
kubectl logs nginx-pod

# Eliminar el Pod
kubectl delete pod nginx-pod
----

==== Contenedores vs Pods

Es crucial entender la diferencia entre contenedores y Pods:

[cols="1,2,2", options="header"]
|===
|Aspecto
|Contenedor
|Pod

|Definición
|Unidad de empaquetado (Docker, containerd)
|Unidad de despliegue en Kubernetes

|Networking
|Necesita configuración de red
|IP compartida entre todos los contenedores

|Almacenamiento
|Volúmenes propios
|Volúmenes compartidos entre contenedores

|Lifecycle
|Gestionado por runtime
|Gestionado por Kubernetes

|Scaling
|N/A en contexto K8s
|Pods se replican

|Uso típico
|Una aplicación
|Una o más aplicaciones relacionadas
|===

**¿Cuándo usar múltiples contenedores en un Pod?**

*Casos de uso válidos:*

1. *Sidecar pattern*: Contenedor helper que extiende funcionalidad
   - Ejemplo: Contenedor de logs que envía logs a sistema centralizado

2. *Ambassador pattern*: Proxy para comunicación externa
   - Ejemplo: Proxy local para base de datos remota

3. *Adapter pattern*: Normaliza output/interfaces
   - Ejemplo: Adaptador que convierte métricas a formato estándar

**Ejemplo de Pod multi-contenedor (Sidecar):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-sidecar
spec:
  containers:
  # Contenedor principal
  - name: main-app
    image: myapp:1.0
    volumeMounts:
    - name: logs
      mountPath: /var/log/app

  # Sidecar para procesar logs
  - name: log-processor
    image: fluentd:latest
    volumeMounts:
    - name: logs
      mountPath: /var/log/app

  volumes:
  - name: logs
    emptyDir: {}
----

**Regla general:**

* *Un contenedor por Pod*: Si los contenedores pueden ejecutarse independientemente
* *Múltiples contenedores por Pod*: Solo si están estrechamente acoplados y deben compartir recursos

==== Namespaces

Los Namespaces proporcionan un mecanismo para aislar grupos de recursos dentro de un mismo cluster.

**Propósito:**

* Dividir recursos del cluster entre múltiples usuarios/equipos
* Proporcionar scope para nombres (los nombres deben ser únicos por namespace)
* Aplicar políticas de recursos (quotas, network policies)
* Organización lógica de recursos

**Namespaces por defecto:**

[source,bash]
----
kubectl get namespaces

# Salida:
NAME              STATUS   AGE
default           Active   30d   # Namespace por defecto
kube-system       Active   30d   # Componentes del sistema
kube-public       Active   30d   # Recursos públicos
kube-node-lease   Active   30d   # Heartbeats de nodos
----

**Descripción de namespaces del sistema:**

* *default*: Namespace por defecto para recursos sin namespace especificado
* *kube-system*: Objetos creados por el sistema Kubernetes (DNS, Dashboard, etc.)
* *kube-public*: Recursos públicos accesibles para todos (incluso no autenticados)
* *kube-node-lease*: Objetos de lease para heartbeats de nodos

**Crear namespace:**

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: desarrollo
  labels:
    env: dev
    team: backend
----

[source,bash]
----
# Crear namespace
kubectl create namespace desarrollo

# O con archivo YAML
kubectl apply -f namespace.yaml

# Crear recursos en un namespace
kubectl apply -f app.yaml -n desarrollo

# Listar recursos en namespace específico
kubectl get pods -n desarrollo

# Listar recursos en todos los namespaces
kubectl get pods --all-namespaces
# O abreviado:
kubectl get pods -A
----

**Cambiar namespace por defecto en contexto:**

[source,bash]
----
# Ver contexto actual
kubectl config current-context

# Cambiar namespace por defecto
kubectl config set-context --current --namespace=desarrollo

# Verificar
kubectl config view --minify | grep namespace:
----

**Ejemplo de uso - Entornos separados:**

[source,yaml]
----
# Namespace para desarrollo
apiVersion: v1
kind: Namespace
metadata:
  name: dev
---
# Namespace para producción
apiVersion: v1
kind: Namespace
metadata:
  name: prod
---
# La misma app puede existir en ambos
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp  # Mismo nombre, diferentes namespaces
  namespace: dev
spec:
  replicas: 1
  # ... especificación
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp  # Mismo nombre, diferentes namespaces
  namespace: prod
spec:
  replicas: 3  # Más réplicas en producción
  # ... especificación
----

[IMPORTANT]
====
No todos los objetos están en un namespace. Nodos, PersistentVolumes, y Namespaces mismos son cluster-wide y no pertenecen a ningún namespace.
====

**Ver qué recursos están en namespace:**

[source,bash]
----
# Recursos en namespace
kubectl api-resources --namespaced=true

# Recursos cluster-wide
kubectl api-resources --namespaced=false
----

==== Labels y Selectors

Labels son pares clave-valor que se adjuntan a objetos como Pods, Services, Deployments, etc. Son fundamentales para la organización y selección de recursos.

**Labels**

**Características:**

* Pares clave-valor arbitrarios
* Pueden añadirse en tiempo de creación o modificarse después
* Múltiples labels por objeto
* No proporcionan unicidad (múltiples objetos pueden tener los mismos labels)

**Sintaxis:**

* Clave: `[prefix/]name`
* Prefijo (opcional): max 253 caracteres (DNS subdomain)
* Nombre: max 63 caracteres (alfanumérico, `-`, `_`, `.`)
* Valor: max 63 caracteres

**Ejemplos de buenos labels:**

[source,yaml]
----
metadata:
  labels:
    # Entorno
    environment: production
    env: prod

    # Aplicación
    app: nginx
    app.kubernetes.io/name: nginx
    app.kubernetes.io/version: "1.21"
    app.kubernetes.io/component: frontend

    # Equipo/Ownership
    team: platform
    owner: devops

    # Release tracking
    release: stable
    version: v2.1.0

    # Tier
    tier: frontend
    layer: presentation
----

**Ejemplo de Pod con labels:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: nginx-prod-frontend
  labels:
    app: nginx
    environment: production
    tier: frontend
    version: "1.21"
spec:
  containers:
  - name: nginx
    image: nginx:1.21
----

**Gestionar labels con kubectl:**

[source,bash]
----
# Ver labels
kubectl get pods --show-labels

# Ver pods con labels específicos como columnas
kubectl get pods -L app,environment

# Añadir label
kubectl label pods nginx-prod-frontend region=us-west

# Modificar label existente
kubectl label pods nginx-prod-frontend version=1.22 --overwrite

# Eliminar label
kubectl label pods nginx-prod-frontend version-
----

**Selectors**

Los Selectors permiten filtrar/seleccionar objetos basándose en sus labels. Son usados extensivamente por Services, Deployments, etc.

**Tipos de selectors:**

1. *Equality-based* (basados en igualdad):
   - `=` o `==`: igualdad
   - `!=`: desigualdad

2. *Set-based* (basados en conjuntos):
   - `in`: valor en conjunto
   - `notin`: valor no en conjunto
   - `exists`: label existe
   - `!`: label no existe

**Ejemplos con kubectl:**

[source,bash]
----
# Equality-based
kubectl get pods -l environment=production
kubectl get pods -l environment!=production
kubectl get pods -l tier=frontend,environment=production  # AND

# Set-based
kubectl get pods -l 'environment in (production,staging)'
kubectl get pods -l 'tier notin (frontend,backend)'
kubectl get pods -l 'version'  # Label exists
kubectl get pods -l '!version'  # Label doesn't exist

# Combinaciones
kubectl get pods -l 'environment=production,tier in (frontend,backend)'
----

**Selectors en manifiestos:**

*Ejemplo 1: Service seleccionando Pods*

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
spec:
  selector:
    app: nginx      # Selecciona Pods con app=nginx
    tier: frontend  # Y tier=frontend
  ports:
  - port: 80
----

*Ejemplo 2: Deployment (matchLabels y matchExpressions)*

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    # Equality-based (más común)
    matchLabels:
      app: nginx
      tier: frontend

    # Set-based (más flexible)
    matchExpressions:
    - key: environment
      operator: In
      values:
      - production
      - staging
    - key: legacy
      operator: DoesNotExist
  template:
    metadata:
      labels:
        app: nginx
        tier: frontend
        environment: production
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
----

**Caso de uso común: Blue-Green deployment**

[source,yaml]
----
# Deployment BLUE
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: blue
  template:
    metadata:
      labels:
        app: myapp
        version: blue
    spec:
      containers:
      - name: app
        image: myapp:1.0
---
# Deployment GREEN
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: green
  template:
    metadata:
      labels:
        app: myapp
        version: green
    spec:
      containers:
      - name: app
        image: myapp:2.0
---
# Service - cambiar selector para switch
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp
    version: blue  # Cambiar a 'green' para switch
  ports:
  - port: 80
----

==== Annotations

Las Annotations son similares a los labels, pero están diseñadas para metadatos no identificatorios que no se usan para seleccionar objetos.

**Diferencias entre Labels y Annotations:**

[cols="1,2,2", options="header"]
|===
|Aspecto
|Labels
|Annotations

|Propósito
|Identificar y seleccionar objetos
|Añadir metadata arbitrario

|Usados por
|Kubernetes (selectors)
|Herramientas externas, usuarios

|Limitaciones de valor
|63 caracteres
|Sin límite práctico

|Consultas
|Pueden filtrarse con selectors
|No filtrables

|Ejemplos de uso
|`app=nginx`, `env=prod`
|URLs, configuración JSON, descripciones largas
|===

**Casos de uso de Annotations:**

1. *Información de build/release*
   - Git commit SHA
   - Build timestamp
   - Release notes URL

2. *Configuración de herramientas*
   - Ingress controllers (NGINX, Traefik)
   - Service meshes (Istio)
   - Monitoring (Prometheus)

3. *Contacto y documentación*
   - Responsable del recurso
   - Documentación
   - Enlaces a dashboards

4. *Metadata de integración*
   - IDs de sistemas externos
   - Configuración específica de cloud provider

**Ejemplo con múltiples annotations:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
  labels:
    app: myapp
    version: "2.1.0"
  annotations:
    # Build information
    build.date: "2024-01-15T10:30:00Z"
    git.commit: "a3f5c8d"
    git.branch: "main"

    # Contact information
    owner: "platform-team@example.com"
    documentation: "https://wiki.example.com/myapp"
    oncall: "https://oncall.example.com/platform"

    # Integration metadata
    monitoring.prometheus.io/scrape: "true"
    monitoring.prometheus.io/port: "9090"
    monitoring.prometheus.io/path: "/metrics"

    # Description
    description: |
      Main application pod running customer-facing API.
      Handles authentication and request routing.
spec:
  containers:
  - name: app
    image: myapp:2.1.0
----

**Ejemplo: Ingress con annotations (NGINX)**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  annotations:
    # NGINX Ingress Controller específico
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/rate-limit: "100"

    # Cert-manager para TLS automático
    cert-manager.io/cluster-issuer: "letsencrypt-prod"

    # Configuración custom
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers "X-Frame-Options: DENY";
      more_set_headers "X-Content-Type-Options: nosniff";
spec:
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-service
            port:
              number: 80
----

**Gestionar annotations con kubectl:**

[source,bash]
----
# Ver annotations
kubectl describe pod app-pod | grep -A 10 Annotations

# Añadir annotation
kubectl annotate pod app-pod contact="team@example.com"

# Modificar annotation
kubectl annotate pod app-pod contact="newteam@example.com" --overwrite

# Eliminar annotation
kubectl annotate pod app-pod contact-

# Añadir annotation con valor multilínea
kubectl annotate pod app-pod description="This is a long
description that spans
multiple lines"
----

**Ejemplo práctico: Deployment completo con labels y annotations**

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  namespace: production
  labels:
    # Labels para identificación
    app: webapp
    component: frontend
    environment: production
  annotations:
    # Annotations para metadata
    deployment.kubernetes.io/revision: "3"
    kubernetes.io/change-cause: "Update to version 2.1.0 with bug fixes"
    description: "Main web application serving customer traffic"
    owner: "frontend-team@example.com"
spec:
  replicas: 5
  selector:
    matchLabels:
      app: webapp
      component: frontend
  template:
    metadata:
      labels:
        # Labels en Pods (deben coincidir con selector)
        app: webapp
        component: frontend
        environment: production
        version: "2.1.0"
      annotations:
        # Annotations en Pods
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
        build.git.commit: "a3f5c8d9"
        build.timestamp: "2024-01-15T10:30:00Z"
    spec:
      containers:
      - name: webapp
        image: webapp:2.1.0
        ports:
        - containerPort: 8080
        env:
        - name: ENVIRONMENT
          value: "production"
----

**Best Practices:**

**Para Labels:**

* Usa labels consistentes y bien definidos
* Incluye información de identificación crítica (app, version, component)
* Mantén un esquema de naming consistente
* Usa prefijos para labels organizacionales (`myorg.com/team`)

**Para Annotations:**

* Usa annotations para metadata que no afecta la programación
* Documenta el significado de annotations personalizadas
* No uses annotations para datos sensibles (usa Secrets)
* Mantén el tamaño razonable (aunque no hay límite estricto)

=== Instalación y Configuración del Entorno

Para aprender y desarrollar con Kubernetes, existen varias opciones para configurar un entorno local. Cada opción tiene sus ventajas y casos de uso específicos.

==== Opciones de instalación local

Comparación rápida de las opciones disponibles:

[cols="1,2,2,2,2", options="header"]
|===
|Herramienta
|Ventajas
|Desventajas
|Casos de uso
|Sistemas

|Minikube
|Fácil de usar, múltiples drivers, addons
|Overhead de VM
|Aprendizaje, desarrollo
|Linux, macOS, Windows

|Kind
|Muy rápido, ligero, ideal para CI
|Solo Docker, básico
|CI/CD, testing
|Linux, macOS, Windows

|Docker Desktop
|Integrado, fácil setup
|Solo 1 nodo
|Desarrollo simple
|macOS, Windows

|MicroK8s
|Producción-ready, snap packages
|Solo Linux nativamente
|Edge, IoT, desarrollo
|Linux (nativo), macOS/Win (VM)
|===

===== Minikube

Minikube es la herramienta más popular para ejecutar Kubernetes localmente. Crea una VM o contenedor que ejecuta un cluster de un solo nodo.

**Instalación:**

*Linux:*

[source,bash]
----
# Usando binario directo
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube

# O usando package manager
# Debian/Ubuntu
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb
sudo dpkg -i minikube_latest_amd64.deb

# Fedora/RHEL
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-latest.x86_64.rpm
sudo rpm -Uvh minikube-latest.x86_64.rpm
----

*macOS:*

[source,bash]
----
# Usando Homebrew
brew install minikube

# O usando binario
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64
sudo install minikube-darwin-amd64 /usr/local/bin/minikube
----

*Windows:*

[source,powershell]
----
# Usando Chocolatey
choco install minikube

# O descargar el instalador desde:
# https://github.com/kubernetes/minikube/releases/latest
----

**Drivers disponibles:**

Minikube soporta varios drivers para ejecutar el cluster:

* *docker*: Usa Docker containers (recomendado)
* *virtualbox*: Usa VirtualBox VM
* *hyperv*: Hyper-V en Windows
* *kvm2*: KVM en Linux
* *hyperkit*: HyperKit en macOS

**Iniciar Minikube:**

[source,bash]
----
# Iniciar con driver por defecto (docker)
minikube start

# Iniciar con driver específico
minikube start --driver=docker

# Iniciar con configuración personalizada
minikube start \
  --cpus=4 \
  --memory=8192 \
  --disk-size=50g \
  --kubernetes-version=v1.28.0

# Iniciar con múltiples nodos
minikube start --nodes=3
----

**Comandos útiles:**

[source,bash]
----
# Ver estado
minikube status

# Detener el cluster
minikube stop

# Eliminar el cluster
minikube delete

# SSH al nodo
minikube ssh

# Ver dashboard
minikube dashboard

# Ver IP del cluster
minikube ip

# Listar addons disponibles
minikube addons list

# Habilitar addon
minikube addons enable ingress
minikube addons enable metrics-server

# Ver logs
minikube logs
----

**Addons populares:**

[source,bash]
----
# Ingress Controller
minikube addons enable ingress

# Metrics Server (para HPA)
minikube addons enable metrics-server

# Dashboard
minikube addons enable dashboard

# Registry
minikube addons enable registry

# Storage provisioner
minikube addons enable storage-provisioner
----

**Trabajar con imágenes Docker locales:**

[source,bash]
----
# Configurar shell para usar Docker de Minikube
eval $(minikube docker-env)

# Ahora docker build construirá dentro de Minikube
docker build -t myapp:local .

# Usar en Pod (importante: imagePullPolicy: Never)
kubectl run myapp --image=myapp:local --image-pull-policy=Never

# Volver a Docker local
eval $(minikube docker-env -u)
----

===== Kind (Kubernetes in Docker)

Kind (Kubernetes IN Docker) ejecuta clusters de Kubernetes usando contenedores Docker como nodos. Es extremadamente rápido y ligero.

**Instalación:**

*Linux:*

[source,bash]
----
# Usando binario
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind
----

*macOS:*

[source,bash]
----
# Usando Homebrew
brew install kind

# O usando binario
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-darwin-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind
----

*Windows:*

[source,powershell]
----
# Usando Chocolatey
choco install kind

# O descargar desde GitHub releases
----

**Crear cluster:**

[source,bash]
----
# Crear cluster simple (1 nodo)
kind create cluster

# Crear cluster con nombre específico
kind create cluster --name dev-cluster

# Crear cluster con versión específica
kind create cluster --image kindest/node:v1.28.0

# Ver clusters
kind get clusters

# Eliminar cluster
kind delete cluster --name dev-cluster
----

**Configuración avanzada con archivo:**

[source,yaml]
----
# kind-config.yaml - Cluster multi-nodo
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  # Control plane node
  - role: control-plane
    kubeadmConfigPatches:
    - |
      kind: InitConfiguration
      nodeRegistration:
        kubeletExtraArgs:
          node-labels: "ingress-ready=true"
    extraPortMappings:
    - containerPort: 80
      hostPort: 80
      protocol: TCP
    - containerPort: 443
      hostPort: 443
      protocol: TCP

  # Worker nodes
  - role: worker
  - role: worker
  - role: worker
----

[source,bash]
----
# Crear cluster con configuración
kind create cluster --config kind-config.yaml --name multi-node
----

**Cargar imágenes locales:**

[source,bash]
----
# Construir imagen
docker build -t myapp:1.0 .

# Cargar en Kind
kind load docker-image myapp:1.0 --name dev-cluster

# Ahora se puede usar en Pods
kubectl run myapp --image=myapp:1.0 --image-pull-policy=Never
----

**Configurar Ingress en Kind:**

[source,bash]
----
# 1. Crear cluster con port mappings (ver config arriba)

# 2. Instalar NGINX Ingress Controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml

# 3. Esperar a que esté listo
kubectl wait --namespace ingress-nginx \
  --for=condition=ready pod \
  --selector=app.kubernetes.io/component=controller \
  --timeout=90s

# 4. Ahora Ingress funcionará en localhost:80/443
----

===== Docker Desktop

Docker Desktop incluye una instalación de Kubernetes de un solo nodo que se integra perfectamente con Docker.

**Instalación:**

1. Descargar Docker Desktop:
   - macOS: https://www.docker.com/products/docker-desktop
   - Windows: https://www.docker.com/products/docker-desktop

2. Instalar Docker Desktop

3. Habilitar Kubernetes:
   - Abrir Docker Desktop
   - Settings/Preferences → Kubernetes
   - Marcar "Enable Kubernetes"
   - Click "Apply & Restart"

**Características:**

* Integración nativa con Docker
* Cluster de un solo nodo
* kubectl incluido
* Automáticamente configura kubeconfig
* Fácil reset del cluster

**Configuración de recursos:**

En Docker Desktop Settings:

[source,yaml]
----
Resources:
  CPUs: 4
  Memory: 8 GB
  Swap: 2 GB
  Disk: 64 GB
----

**Reset de Kubernetes:**

Si necesitas empezar desde cero:

* Settings → Kubernetes → Reset Kubernetes Cluster

**Limitaciones:**

* Solo un nodo (no multi-nodo)
* Menos flexible que otras opciones
* Solo disponible en macOS y Windows

===== MicroK8s

MicroK8s es una distribución ligera de Kubernetes diseñada para IoT, edge computing y desarrollo. Es especialmente popular en Ubuntu.

**Instalación:**

*Linux (Ubuntu/Debian):*

[source,bash]
----
# Instalar usando snap
sudo snap install microk8s --classic

# Añadir usuario al grupo
sudo usermod -a -G microk8s $USER
sudo chown -f -R $USER ~/.kube

# Recargar grupos (o logout/login)
newgrp microk8s

# Verificar
microk8s status --wait-ready
----

*macOS/Windows:*

MicroK8s usa Multipass para crear una VM:

[source,bash]
----
# macOS
brew install ubuntu/microk8s/microk8s
microk8s install

# Windows - descargar desde:
# https://microk8s.io/docs/install-windows
----

**Comandos útiles:**

[source,bash]
----
# Iniciar/Detener
microk8s start
microk8s stop

# Estado
microk8s status

# kubectl (prefijo microk8s)
microk8s kubectl get nodes

# O crear alias
alias kubectl='microk8s kubectl'

# Configurar kubeconfig para kubectl normal
microk8s config > ~/.kube/config
----

**Addons:**

[source,bash]
----
# Listar addons
microk8s enable --help

# Habilitar addons comunes
microk8s enable dns
microk8s enable dashboard
microk8s enable storage
microk8s enable ingress
microk8s enable registry
microk8s enable metrics-server

# Istio
microk8s enable istio

# Prometheus
microk8s enable prometheus
----

**High Availability:**

MicroK8s soporta clustering:

[source,bash]
----
# En el primer nodo
microk8s add-node
# Esto genera un comando join

# En los nodos adicionales
microk8s join <ip>:<port>/<token>

# Ver nodos
microk8s kubectl get nodes
----

==== Instalación de kubectl

kubectl es la herramienta de línea de comandos para interactuar con clusters Kubernetes.

**Instalación:**

*Linux:*

[source,bash]
----
# Descargar última versión
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

# Validar binario (opcional)
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check

# Instalar
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# Verificar
kubectl version --client
----

*macOS:*

[source,bash]
----
# Usando Homebrew
brew install kubectl

# O usando binario
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl"
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl

# Verificar
kubectl version --client
----

*Windows:*

[source,powershell]
----
# Usando Chocolatey
choco install kubernetes-cli

# O usando Scoop
scoop install kubectl

# Verificar
kubectl version --client
----

**Autocompletado:**

*Bash:*

[source,bash]
----
# Instalar bash-completion primero (si no está)
sudo apt-get install bash-completion  # Debian/Ubuntu
sudo yum install bash-completion       # RHEL/CentOS

# Configurar autocompletado
echo 'source <(kubectl completion bash)' >>~/.bashrc
echo 'alias k=kubectl' >>~/.bashrc
echo 'complete -o default -F __start_kubectl k' >>~/.bashrc

# Aplicar cambios
source ~/.bashrc
----

*Zsh:*

[source,bash]
----
echo 'source <(kubectl completion zsh)' >>~/.zshrc
echo 'alias k=kubectl' >>~/.zshrc
echo 'compdef __start_kubectl k' >>~/.zshrc

source ~/.zshrc
----

**Plugins útiles:**

*krew (plugin manager):*

[source,bash]
----
# Instalar krew
(
  set -x; cd "$(mktemp -d)" &&
  OS="$(uname | tr '[:upper:]' '[:lower:]')" &&
  ARCH="$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\(arm\)\(64\)\?.*/\1\2/' -e 's/aarch64$/arm64/')" &&
  KREW="krew-${OS}_${ARCH}" &&
  curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz" &&
  tar zxvf "${KREW}.tar.gz" &&
  ./"${KREW}" install krew
)

# Añadir al PATH
export PATH="${KREW_ROOT:-$HOME/.krew}/bin:$PATH"

# Instalar plugins útiles
kubectl krew install ctx      # Cambiar contextos
kubectl krew install ns       # Cambiar namespaces
kubectl krew install tree     # Ver recursos en árbol
kubectl krew install tail     # Tail logs de múltiples pods
----

==== Configuración del archivo kubeconfig

El archivo kubeconfig contiene la información necesaria para conectarse a clusters de Kubernetes.

**Ubicación por defecto:**

* Linux/macOS: `~/.kube/config`
* Windows: `%USERPROFILE%\.kube\config`

**Estructura del kubeconfig:**

[source,yaml]
----
apiVersion: v1
kind: Config
current-context: minikube  # Contexto activo

# Clusters - información de conexión
clusters:
- cluster:
    certificate-authority: /path/to/ca.crt
    server: https://192.168.49.2:8443
  name: minikube

- cluster:
    certificate-authority-data: LS0tLS1CRUdJTi...
    server: https://production-cluster.example.com:6443
  name: production

# Users - credenciales de autenticación
users:
- name: minikube
  user:
    client-certificate: /path/to/client.crt
    client-key: /path/to/client.key

- name: admin-prod
  user:
    token: eyJhbGciOiJSUzI1NiIsImtpZCI6...

# Contexts - combinación de cluster + user + namespace
contexts:
- context:
    cluster: minikube
    user: minikube
    namespace: default
  name: minikube

- context:
    cluster: production
    user: admin-prod
    namespace: production
  name: prod-context
----

**Gestionar contextos:**

[source,bash]
----
# Ver configuración actual
kubectl config view

# Ver configuración sin datos sensibles ocultos
kubectl config view --raw

# Listar clusters
kubectl config get-clusters

# Listar contextos
kubectl config get-contexts

# Ver contexto actual
kubectl config current-context

# Cambiar contexto
kubectl config use-context prod-context

# Crear nuevo contexto
kubectl config set-context dev-context \
  --cluster=minikube \
  --user=developer \
  --namespace=development

# Cambiar namespace del contexto actual
kubectl config set-context --current --namespace=kube-system

# Eliminar contexto
kubectl config delete-context old-context
----

**Múltiples archivos kubeconfig:**

[source,bash]
----
# Usar archivo específico
kubectl --kubeconfig=/path/to/custom-config get pods

# O configurar variable de entorno
export KUBECONFIG=/path/to/config1:/path/to/config2
# kubectl fusionará ambos archivos

# Hacer permanente
echo 'export KUBECONFIG=$HOME/.kube/config:$HOME/.kube/config-prod' >> ~/.bashrc
----

**Ejemplo: Añadir cluster manualmente:**

[source,bash]
----
# 1. Añadir cluster
kubectl config set-cluster my-cluster \
  --server=https://k8s.example.com:6443 \
  --certificate-authority=/path/to/ca.crt

# 2. Añadir usuario
kubectl config set-credentials my-user \
  --client-certificate=/path/to/client.crt \
  --client-key=/path/to/client.key

# 3. Crear contexto
kubectl config set-context my-context \
  --cluster=my-cluster \
  --user=my-user \
  --namespace=default

# 4. Usar contexto
kubectl config use-context my-context
----

==== Verificación de la instalación

Una vez instalado todo, verifica que funciona correctamente:

**1. Verificar kubectl:**

[source,bash]
----
# Versión de cliente
kubectl version --client

# Salida esperada:
# Client Version: v1.28.0
# Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
----

**2. Verificar cluster:**

[source,bash]
----
# Versión de cliente y servidor
kubectl version

# Información del cluster
kubectl cluster-info

# Salida esperada:
# Kubernetes control plane is running at https://127.0.0.1:xxxxx
# CoreDNS is running at https://127.0.0.1:xxxxx/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
----

**3. Verificar nodos:**

[source,bash]
----
kubectl get nodes

# Salida esperada:
# NAME       STATUS   ROLES           AGE   VERSION
# minikube   Ready    control-plane   5m    v1.28.0
----

**4. Verificar componentes del sistema:**

[source,bash]
----
kubectl get pods -n kube-system

# Deberías ver:
# - coredns
# - etcd
# - kube-apiserver
# - kube-controller-manager
# - kube-proxy
# - kube-scheduler
----

**5. Verificar namespaces:**

[source,bash]
----
kubectl get namespaces

# Salida esperada:
# NAME              STATUS   AGE
# default           Active   5m
# kube-node-lease   Active   5m
# kube-public       Active   5m
# kube-system       Active   5m
----

**6. Prueba de despliegue:**

[source,bash]
----
# Crear un deployment de prueba
kubectl create deployment nginx --image=nginx

# Esperar a que esté listo
kubectl wait --for=condition=available --timeout=60s deployment/nginx

# Verificar
kubectl get deployments
kubectl get pods

# Exponer como servicio
kubectl expose deployment nginx --port=80 --type=NodePort

# Ver servicio
kubectl get services nginx

# Limpiar
kubectl delete deployment nginx
kubectl delete service nginx
----

**7. Verificar acceso al dashboard (si está habilitado):**

*Minikube:*

[source,bash]
----
minikube dashboard
# Abrirá el dashboard en el navegador
----

*Otros clusters:*

[source,bash]
----
# Instalar dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

# Crear usuario admin para acceder
kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard
kubectl create clusterrolebinding dashboard-admin \
  --clusterrole=cluster-admin \
  --serviceaccount=kubernetes-dashboard:dashboard-admin

# Obtener token
kubectl -n kubernetes-dashboard create token dashboard-admin

# Iniciar proxy
kubectl proxy

# Acceder en:
# http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
# Usar el token para login
----

**Troubleshooting común:**

*Problema: kubectl no encuentra el cluster*

[source,bash]
----
# Verificar kubeconfig
echo $KUBECONFIG
kubectl config view

# Si está vacío, configurar
export KUBECONFIG=~/.kube/config

# O generar desde tu herramienta
minikube update-context
# o
kind export kubeconfig --name <cluster-name>
----

*Problema: Connection refused al API server*

[source,bash]
----
# Verificar que el cluster está corriendo
minikube status
# o
kind get clusters
# o
docker ps  # Ver contenedores de Kind

# Reiniciar si es necesario
minikube start
# o
kind create cluster
----

*Problema: Permisos insuficientes*

[source,bash]
----
# Verificar contexto y usuario
kubectl config current-context
kubectl auth can-i get pods

# Ver permisos del usuario actual
kubectl auth can-i --list
----

**Recursos para verificación:**

[source,bash]
----
# Estado general del cluster
kubectl get all --all-namespaces

# Eventos del cluster (útil para debugging)
kubectl get events --all-namespaces --sort-by='.lastTimestamp'

# Información de recursos
kubectl top nodes    # Requiere metrics-server
kubectl top pods -A

# Verificar API resources disponibles
kubectl api-resources

# Verificar versiones de API
kubectl api-versions
----

Con esto, tienes un entorno de Kubernetes completamente funcional y verificado, listo para comenzar a aprender y desarrollar aplicaciones.

== Módulo 2: Trabajando con Pods

=== Creación y Gestión de Pods

Los Pods son la unidad fundamental de despliegue en Kubernetes. En esta sección, aprenderemos cómo crear, gestionar y entender el ciclo de vida de los Pods.

==== Anatomía de un Pod

Un Pod encapsula uno o más contenedores, almacenamiento compartido (volúmenes), una dirección IP única y opciones que controlan cómo deben ejecutarse los contenedores.

**Estructura de un Pod:**

----
┌─────────────────────────────────────────┐
│              POD (IP: 10.244.1.5)       │
│                                         │
│  ┌─────────────────┐  ┌──────────────┐ │
│  │   Container 1   │  │ Container 2  │ │
│  │                 │  │              │ │
│  │  App Process    │  │ Sidecar      │ │
│  │  Port: 8080     │  │ Port: 9090   │ │
│  │                 │  │              │ │
│  └────────┬────────┘  └──────┬───────┘ │
│           │                  │         │
│           └──────┬───────────┘         │
│                  │                     │
│         ┌────────▼────────┐            │
│         │  Shared Volume  │            │
│         │   (emptyDir)    │            │
│         └─────────────────┘            │
│                                         │
│  Network Namespace (localhost)          │
│  IPC Namespace                          │
│  UTS Namespace                          │
└─────────────────────────────────────────┘
----

**Componentes principales de un Pod:**

1. *Metadata*
   - Nombre único en el namespace
   - Labels y annotations
   - Namespace
   - UID generado automáticamente

2. *Spec (Especificación)*
   - Lista de contenedores
   - Volúmenes
   - Restart policy
   - DNS policy
   - Service account
   - Security context

3. *Status (Estado actual)*
   - Phase (Pending, Running, Succeeded, Failed, Unknown)
   - Conditions
   - Container statuses
   - IP del Pod
   - Start time

==== Manifiestos YAML para Pods

Un manifiesto YAML describe el estado deseado de un Pod. Veamos ejemplos desde simple a complejo.

**Ejemplo 1: Pod básico**

[source,yaml]
----
apiVersion: v1           # Versión de la API
kind: Pod                # Tipo de recurso
metadata:
  name: nginx-simple     # Nombre del Pod
  labels:
    app: nginx
    environment: dev
spec:
  containers:
  - name: nginx          # Nombre del contenedor
    image: nginx:1.21    # Imagen de contenedor
    ports:
    - containerPort: 80  # Puerto expuesto
----

**Ejemplo 2: Pod con configuración de recursos**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: nginx-with-resources
  namespace: production
  labels:
    app: nginx
    tier: frontend
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    ports:
    - containerPort: 80
      protocol: TCP

    # Recursos solicitados y límites
    resources:
      requests:
        memory: "128Mi"    # Mínimo garantizado
        cpu: "250m"        # 0.25 CPU cores
      limits:
        memory: "256Mi"    # Máximo permitido
        cpu: "500m"        # 0.5 CPU cores

    # Variables de entorno
    env:
    - name: ENVIRONMENT
      value: "production"
    - name: LOG_LEVEL
      value: "info"
----

**Ejemplo 3: Pod completo con health checks**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: webapp-complete
  labels:
    app: webapp
    version: "2.0"
  annotations:
    description: "Complete web application pod with health checks"
spec:
  # Contenedores
  containers:
  - name: webapp
    image: myapp:2.0
    ports:
    - name: http
      containerPort: 8080
      protocol: TCP

    # Recursos
    resources:
      requests:
        memory: "256Mi"
        cpu: "500m"
      limits:
        memory: "512Mi"
        cpu: "1000m"

    # Variables de entorno
    env:
    - name: DATABASE_HOST
      value: "postgres.default.svc.cluster.local"
    - name: DATABASE_PORT
      value: "5432"

    # Liveness Probe - determina si el contenedor está vivo
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
      initialDelaySeconds: 30  # Espera antes del primer check
      periodSeconds: 10         # Frecuencia de checks
      timeoutSeconds: 5         # Timeout de cada check
      failureThreshold: 3       # Fallos antes de reiniciar

    # Readiness Probe - determina si el contenedor está listo para tráfico
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      initialDelaySeconds: 10
      periodSeconds: 5
      timeoutSeconds: 3
      successThreshold: 1
      failureThreshold: 2

    # Startup Probe - para aplicaciones con startup lento
    startupProbe:
      httpGet:
        path: /startup
        port: 8080
      initialDelaySeconds: 0
      periodSeconds: 10
      failureThreshold: 30      # Permite hasta 300s para startup

    # Volume mounts
    volumeMounts:
    - name: config
      mountPath: /etc/config
      readOnly: true
    - name: data
      mountPath: /var/data

  # Política de reinicio
  restartPolicy: Always  # Always, OnFailure, Never

  # DNS Policy
  dnsPolicy: ClusterFirst

  # Service Account
  serviceAccountName: webapp-sa

  # Security Context a nivel de Pod
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 2000

  # Volúmenes
  volumes:
  - name: config
    configMap:
      name: webapp-config
  - name: data
    emptyDir: {}

  # Node selector - programa en nodos específicos
  nodeSelector:
    disktype: ssd

  # Tolerations - permite scheduling en nodos con taints
  tolerations:
  - key: "environment"
    operator: "Equal"
    value: "production"
    effect: "NoSchedule"
----

**Campos importantes del spec:**

[cols="1,3,2", options="header"]
|===
|Campo
|Descripción
|Valores comunes

|containers
|Lista de contenedores en el Pod
|Array de container specs

|restartPolicy
|Cuándo reiniciar contenedores
|Always, OnFailure, Never

|nodeSelector
|Selecciona nodos por labels
|Map de key-value

|volumes
|Volúmenes disponibles para el Pod
|Array de volume specs

|imagePullPolicy
|Cuándo descargar la imagen
|Always, IfNotPresent, Never

|hostname
|Hostname del Pod
|String

|subdomain
|Subdomain del Pod
|String

|dnsPolicy
|Política de DNS
|ClusterFirst, Default, None

|serviceAccountName
|Service account a usar
|String

|priority
|Prioridad del Pod
|Integer

|schedulerName
|Scheduler custom
|String
|===

==== Comandos básicos con kubectl

**Crear Pods:**

[source,bash]
----
# Crear desde archivo YAML
kubectl apply -f pod.yaml

# Crear desde archivo con verbose output
kubectl apply -f pod.yaml -v=8

# Crear desde URL
kubectl apply -f https://example.com/pod.yaml

# Crear Pod imperativo (rápido para testing)
kubectl run nginx --image=nginx

# Con puerto expuesto
kubectl run webapp --image=myapp:1.0 --port=8080

# Con variables de entorno
kubectl run myapp --image=busybox --env="KEY=value" --env="ENV=prod"

# Modo dry-run (ver YAML sin crear)
kubectl run nginx --image=nginx --dry-run=client -o yaml

# Guardar YAML generado
kubectl run nginx --image=nginx --dry-run=client -o yaml > pod.yaml
----

**Listar Pods:**

[source,bash]
----
# Listar pods en namespace actual
kubectl get pods

# Listar con más información
kubectl get pods -o wide

# Todos los namespaces
kubectl get pods --all-namespaces
kubectl get pods -A

# Formato de salida específico
kubectl get pods -o yaml        # YAML completo
kubectl get pods -o json        # JSON completo
kubectl get pods -o name        # Solo nombres

# Mostrar labels
kubectl get pods --show-labels

# Filtrar por labels
kubectl get pods -l app=nginx
kubectl get pods -l 'environment in (prod,staging)'

# Watch mode (actualización continua)
kubectl get pods -w

# Ordenar por edad
kubectl get pods --sort-by=.metadata.creationTimestamp

# Custom columns
kubectl get pods -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,IP:.status.podIP
----

**Ver detalles de un Pod:**

[source,bash]
----
# Descripción completa
kubectl describe pod nginx

# Ver YAML completo
kubectl get pod nginx -o yaml

# Ver solo el spec
kubectl get pod nginx -o jsonpath='{.spec}'

# Ver status
kubectl get pod nginx -o jsonpath='{.status.phase}'

# Ver IP
kubectl get pod nginx -o jsonpath='{.status.podIP}'

# Ver condiciones
kubectl get pod nginx -o jsonpath='{.status.conditions[*].type}'
----

**Modificar Pods:**

[source,bash]
----
# Editar en editor
kubectl edit pod nginx

# Aplicar cambios desde archivo
kubectl apply -f pod-updated.yaml

# Patch específico
kubectl patch pod nginx -p '{"spec":{"containers":[{"name":"nginx","image":"nginx:1.22"}]}}'

# Patch desde archivo
kubectl patch pod nginx --patch-file patch.yaml

# Replace (elimina y recrea)
kubectl replace -f pod.yaml --force
----

**Eliminar Pods:**

[source,bash]
----
# Eliminar por nombre
kubectl delete pod nginx

# Eliminar desde archivo
kubectl delete -f pod.yaml

# Eliminar por label
kubectl delete pods -l app=nginx

# Eliminar todos los pods en namespace
kubectl delete pods --all

# Eliminar con grace period
kubectl delete pod nginx --grace-period=30

# Eliminar inmediatamente (forzar)
kubectl delete pod nginx --grace-period=0 --force

# Eliminar y esperar a que termine
kubectl delete pod nginx --wait=true
----

**Comandos de debugging:**

[source,bash]
----
# Ver logs
kubectl logs nginx

# Logs de contenedor específico (si hay múltiples)
kubectl logs nginx -c container-name

# Seguir logs (tail -f)
kubectl logs -f nginx

# Logs previos (si el contenedor crasheó)
kubectl logs nginx --previous

# Últimas N líneas
kubectl logs nginx --tail=50

# Desde hace X tiempo
kubectl logs nginx --since=1h

# Ejecutar comando en contenedor
kubectl exec nginx -- ls /
kubectl exec nginx -- env

# Shell interactivo
kubectl exec -it nginx -- /bin/bash
kubectl exec -it nginx -- sh

# En contenedor específico
kubectl exec -it nginx -c sidecar -- /bin/bash

# Port forwarding
kubectl port-forward nginx 8080:80

# Copiar archivos
kubectl cp nginx:/etc/nginx/nginx.conf ./nginx.conf
kubectl cp ./local-file nginx:/tmp/remote-file
----

==== Ciclo de vida de un Pod

El ciclo de vida de un Pod pasa por varias fases desde su creación hasta su terminación.

**Fases del ciclo de vida:**

----
    ┌─────────────┐
    │   PENDING   │ ← Pod creado, esperando scheduling
    └──────┬──────┘
           │
           ▼
    ┌─────────────┐
    │  RUNNING    │ ← Pod asignado a nodo, contenedores iniciados
    └──────┬──────┘
           │
           ├─────────────────┐
           │                 │
           ▼                 ▼
    ┌─────────────┐   ┌─────────────┐
    │  SUCCEEDED  │   │   FAILED    │
    └─────────────┘   └─────────────┘

    ┌─────────────┐
    │   UNKNOWN   │ ← Estado no puede determinarse
    └─────────────┘
----

**Detalle de cada fase:**

1. *Pending*
   - Pod aceptado por Kubernetes
   - Esperando ser programado en un nodo
   - Descargando imágenes
   - Creando contenedores

2. *Running*
   - Pod asignado a un nodo
   - Al menos un contenedor está ejecutándose
   - O está en proceso de iniciar/reiniciar

3. *Succeeded*
   - Todos los contenedores terminaron exitosamente
   - No se reiniciarán
   - Típico de Jobs

4. *Failed*
   - Todos los contenedores han terminado
   - Al menos uno terminó con error (exit code != 0)

5. *Unknown*
   - Estado del Pod no puede determinarse
   - Generalmente por pérdida de comunicación con el nodo

**Estados de los contenedores:**

Dentro de un Pod, cada contenedor tiene su propio estado:

1. *Waiting*
   - Esperando para iniciar
   - Descargando imagen
   - Aplicando Secrets

2. *Running*
   - Ejecutándose sin problemas
   - PostStart hook completado (si existe)

3. *Terminated*
   - Ha terminado de ejecutarse
   - Ya sea exitosamente o con error

**Ejemplo de verificación de fase:**

[source,bash]
----
# Ver fase del Pod
kubectl get pod nginx -o jsonpath='{.status.phase}'

# Ver estado de contenedores
kubectl get pod nginx -o jsonpath='{.status.containerStatuses[*].state}'

# Ver razón si está en espera
kubectl get pod nginx -o jsonpath='{.status.containerStatuses[0].state.waiting.reason}'
----

==== Estados de los Pods

Los Pods pueden tener varias condiciones que describen su estado actual.

**Pod Conditions:**

[cols="1,3,2", options="header"]
|===
|Condition
|Descripción
|Valores

|PodScheduled
|Pod ha sido programado a un nodo
|True, False

|Initialized
|Init containers han completado
|True, False

|ContainersReady
|Todos los contenedores están listos
|True, False

|Ready
|Pod puede servir requests
|True, False
|===

**Estados comunes y su significado:**

*Running y Ready*

[source,bash]
----
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          5m

# Interpretación:
# - STATUS: Running = Pod ejecutándose
# - READY: 1/1 = 1 de 1 contenedores listo
# - RESTARTS: 0 = No ha tenido reinicios
----

*Pending - ImagePullBackOff*

[source,bash]
----
NAME    READY   STATUS             RESTARTS   AGE
webapp  0/1     ImagePullBackOff   0          2m

# Causa: No puede descargar la imagen
# Soluciones:
# - Verificar nombre de imagen
# - Verificar acceso al registry
# - Verificar imagePullSecrets
----

*Pending - Insufficient resources*

[source,bash]
----
NAME    READY   STATUS    RESTARTS   AGE
webapp  0/1     Pending   0          30s

# Ver razón:
kubectl describe pod webapp
# Events:
#   Warning  FailedScheduling  pod has unbound immediate PersistentVolumeClaims
#   Warning  FailedScheduling  0/3 nodes are available: insufficient cpu

# Causa: No hay nodos con recursos suficientes
----

*CrashLoopBackOff*

[source,bash]
----
NAME    READY   STATUS             RESTARTS   AGE
webapp  0/1     CrashLoopBackOff   5          10m

# Interpretación:
# - Contenedor inicia pero crashea inmediatamente
# - Kubernetes lo reinicia automáticamente
# - Backoff exponencial entre reintentos

# Debugging:
kubectl logs webapp
kubectl logs webapp --previous  # Ver logs del crash anterior
kubectl describe pod webapp
----

*Error - Failed*

[source,bash]
----
NAME    READY   STATUS   RESTARTS   AGE
job-1   0/1     Error    0          1m

# Causa: Contenedor terminó con exit code != 0
# Ver logs para detalles
kubectl logs job-1
----

*CreateContainerConfigError*

[source,bash]
----
NAME    READY   STATUS                       RESTARTS   AGE
webapp  0/1     CreateContainerConfigError   0          1m

# Causa común:
# - ConfigMap o Secret referenciado no existe
# - Permisos incorrectos
kubectl describe pod webapp
----

*ErrImagePull*

[source,bash]
----
NAME    READY   STATUS         RESTARTS   AGE
webapp  0/1     ErrImagePull   0          30s

# Causa: Error al descargar imagen
# - Imagen no existe
# - Registry no accesible
# - Credenciales incorrectas
----

*Evicted*

[source,bash]
----
NAME    READY   STATUS    RESTARTS   AGE
webapp  0/1     Evicted   0          1h

# Causa: Nodo sin recursos (memoria, disco)
# Pod fue desalojado para liberar recursos
# Ver razón:
kubectl get pod webapp -o jsonpath='{.status.reason}'
kubectl get pod webapp -o jsonpath='{.status.message}'
----

**Container Restart Reasons:**

[source,yaml]
----
# Ver razón de último reinicio
kubectl get pod webapp -o jsonpath='{.status.containerStatuses[0].lastState.terminated.reason}'

# Razones comunes:
# - Error: Exit code != 0
# - OOMKilled: Out of Memory
# - Completed: Exit code 0 (para Jobs)
# - ContainerCannotRun: Error al ejecutar
----

**Ejemplo completo de análisis de estado:**

[source,bash]
----
# Ver estado general
kubectl get pods

# Pod específico con detalles
kubectl describe pod problematic-pod

# Salida importante a revisar:
# ========================
# Status: Pending/Running/Failed
# Conditions:
#   Type              Status
#   Initialized       True
#   Ready             False    ← Pod no está listo
#   ContainersReady   False    ← Contenedores no están listos
#   PodScheduled      True
#
# Container States:
#   State:          Waiting
#     Reason:       CrashLoopBackOff  ← Razón del problema
#
# Events:  ← Eventos ordenados cronológicamente
#   Type     Reason     Message
#   Warning  BackOff    Back-off restarting failed container
#   Warning  Failed     Error: container exited with code 1

# Ver logs para entender el error
kubectl logs problematic-pod

# Si hay múltiples contenedores
kubectl logs problematic-pod -c container-name

# Ver logs del crash anterior
kubectl logs problematic-pod --previous
----

**Estados de health checks:**

[source,bash]
----
# Liveness Probe Failed
# Resultado: Kubernetes reinicia el contenedor

# Readiness Probe Failed
# Resultado: Pod removido de endpoints de Service
#            (no recibe tráfico pero sigue corriendo)

# Startup Probe Failed
# Resultado: Si falla después de failureThreshold,
#            contenedor se reinicia

# Ver estado de probes
kubectl describe pod webapp | grep -A 5 "Liveness\|Readiness\|Startup"
----

**Monitoreo continuo:**

[source,bash]
----
# Watch pods en tiempo real
kubectl get pods -w

# Ver eventos en tiempo real
kubectl get events -w

# Ver pods con custom columns útiles
kubectl get pods -o custom-columns=\
NAME:.metadata.name,\
STATUS:.status.phase,\
READY:.status.conditions[?(@.type==\"Ready\")].status,\
RESTARTS:.status.containerStatuses[0].restartCount,\
AGE:.metadata.creationTimestamp

# Filtrar pods no healthy
kubectl get pods --field-selector=status.phase!=Running

# Ver pods con errores
kubectl get pods --field-selector=status.phase=Failed
----

**Best Practices para gestión de Pods:**

1. *Siempre define recursos*
   - Requests y limits para CPU y memoria
   - Previene OutOfMemory kills

2. *Usa health checks apropiados*
   - Liveness: detecta contenedores zombies
   - Readiness: controla tráfico
   - Startup: para apps con inicialización lenta

3. *Configura restart policies correctamente*
   - Always: para servicios de larga duración
   - OnFailure: para jobs
   - Never: para debugging

4. *Usa labels consistentemente*
   - Facilita selección y organización
   - Importante para Services y Deployments

5. *No uses Pods directamente en producción*
   - Usa Deployments, StatefulSets, etc.
   - Pods directos no tienen auto-recuperación completa

=== Pods Multi-contenedor

Aunque la mayoría de los Pods contienen un solo contenedor, hay casos en los que múltiples contenedores deben ejecutarse juntos. Los Pods multi-contenedor permiten agrupar contenedores estrechamente acoplados que comparten recursos.

**¿Cuándo usar múltiples contenedores?**

Usa múltiples contenedores en un Pod cuando:

* Los contenedores DEBEN ejecutarse en el mismo nodo
* Necesitan compartir el mismo ciclo de vida
* Comparten volúmenes/recursos
* Se comunican vía localhost
* Uno extiende la funcionalidad del otro

[IMPORTANT]
====
Si los contenedores pueden ejecutarse independientemente, usa Pods separados. Los Pods multi-contenedor son para contenedores estrechamente acoplados que forman una unidad lógica.
====

==== Patrones de diseño

Hay tres patrones principales para Pods multi-contenedor:

===== Patrón Sidecar

El contenedor sidecar extiende y mejora el contenedor principal sin que este lo sepa.

**Características:**

* Contenedor helper que agrega funcionalidad
* Corre en paralelo con el contenedor principal
* Comparte el mismo ciclo de vida
* Casos de uso: logging, monitoring, proxies, adaptadores

**Diagrama:**

----
┌────────────────────────────────────┐
│          POD                       │
│  ┌──────────────┐  ┌────────────┐ │
│  │   Main App   │  │  Sidecar   │ │
│  │              │  │            │ │
│  │ Escribe logs │─>│Lee y envía │ │
│  │ a archivo    │  │logs        │ │
│  └──────────────┘  └────────────┘ │
│         │              │          │
│         └──────┬───────┘          │
│                │                  │
│         ┌──────▼──────┐           │
│         │   Volume    │           │
│         │   Shared    │           │
│         └─────────────┘           │
└────────────────────────────────────┘
----

**Ejemplo 1: Logging Sidecar**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: webserver-with-logging
spec:
  # Volumen compartido para logs
  volumes:
  - name: shared-logs
    emptyDir: {}

  containers:
  # Contenedor principal - servidor web
  - name: nginx
    image: nginx:1.21
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
    ports:
    - containerPort: 80

  # Sidecar - procesador de logs
  - name: log-shipper
    image: fluent/fluentd:v1.14
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
      readOnly: true
    env:
    - name: FLUENTD_CONF
      value: "fluent.conf"
    - name: FLUENT_ELASTICSEARCH_HOST
      value: "elasticsearch.logging.svc.cluster.local"
    - name: FLUENT_ELASTICSEARCH_PORT
      value: "9200"
----

**Ejemplo 2: Git Sync Sidecar**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: web-with-git-sync
spec:
  volumes:
  - name: html
    emptyDir: {}

  containers:
  # Contenedor principal - servidor web
  - name: nginx
    image: nginx:1.21
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80

  # Sidecar - sincroniza contenido desde Git
  - name: git-sync
    image: k8s.gcr.io/git-sync:v3.1.6
    volumeMounts:
    - name: html
      mountPath: /tmp/git
    env:
    - name: GIT_SYNC_REPO
      value: "https://github.com/example/website.git"
    - name: GIT_SYNC_BRANCH
      value: "main"
    - name: GIT_SYNC_ROOT
      value: "/tmp/git"
    - name: GIT_SYNC_DEST
      value: "html"
    - name: GIT_SYNC_PERIOD
      value: "60s"  # Sincroniza cada 60s
----

**Ejemplo 3: Monitoring Sidecar**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-monitoring
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
spec:
  containers:
  # Contenedor principal
  - name: application
    image: myapp:1.0
    ports:
    - containerPort: 8080
    resources:
      requests:
        cpu: "500m"
        memory: "512Mi"

  # Sidecar - exportador de métricas
  - name: metrics-exporter
    image: prom/statsd-exporter:v0.22.0
    ports:
    - name: metrics
      containerPort: 9090
    args:
    - --web.listen-address=:9090
    - --statsd.listen-udp=:8125
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
----

===== Patrón Ambassador

El contenedor ambassador actúa como proxy para el contenedor principal, simplificando su conexión con servicios externos.

**Características:**

* Proxy que maneja la complejidad de red
* Traduce/encapsula protocolos
* Maneja conexiones externas
* Casos de uso: proxy de DB, API gateways, service mesh

**Diagrama:**

----
┌─────────────────────────────────────────┐
│              POD                        │
│  ┌──────────────┐      ┌─────────────┐ │
│  │   Main App   │      │ Ambassador  │ │
│  │              │      │   Proxy     │ │
│  │ Conecta a   ─┼──────>             │ │
│  │ localhost    │      │             │ │
│  └──────────────┘      └──────┬──────┘ │
│                               │        │
└───────────────────────────────┼────────┘
                                │
                                ▼
                    ┌───────────────────┐
                    │ External Service  │
                    │  (Base de Datos,  │
                    │   API, etc.)      │
                    └───────────────────┘
----

**Ejemplo 1: Database Proxy Ambassador**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-db-proxy
spec:
  containers:
  # Contenedor principal - aplicación
  - name: application
    image: myapp:1.0
    env:
    # App se conecta a localhost (el ambassador)
    - name: DATABASE_HOST
      value: "127.0.0.1"
    - name: DATABASE_PORT
      value: "5432"
    ports:
    - containerPort: 8080

  # Ambassador - proxy de base de datos
  - name: cloudsql-proxy
    image: gcr.io/cloudsql-docker/gce-proxy:1.33.2
    command:
    - "/cloud_sql_proxy"
    - "-instances=my-project:us-central1:my-database=tcp:5432"
    - "-credential_file=/secrets/credentials.json"
    volumeMounts:
    - name: cloudsql-credentials
      mountPath: /secrets
      readOnly: true
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"

  volumes:
  - name: cloudsql-credentials
    secret:
      secretName: cloudsql-credentials
----

**Ejemplo 2: Ambassador para Service Mesh**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-envoy
spec:
  containers:
  # Contenedor principal
  - name: application
    image: myapp:1.0
    ports:
    - containerPort: 8080
    env:
    # Todas las llamadas externas pasan por el proxy
    - name: HTTP_PROXY
      value: "http://127.0.0.1:8001"

  # Ambassador - Envoy proxy
  - name: envoy
    image: envoyproxy/envoy:v1.24.0
    ports:
    - containerPort: 8001  # Proxy port
    - containerPort: 9901  # Admin port
    volumeMounts:
    - name: envoy-config
      mountPath: /etc/envoy
    command:
    - "envoy"
    - "-c"
    - "/etc/envoy/envoy.yaml"
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"

  volumes:
  - name: envoy-config
    configMap:
      name: envoy-config
----

**Ejemplo 3: API Rate Limiting Ambassador**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-rate-limiter
spec:
  containers:
  # Contenedor principal
  - name: api-server
    image: my-api:1.0
    ports:
    - containerPort: 8080

  # Ambassador - rate limiter
  - name: rate-limiter
    image: nginx:1.21
    ports:
    - containerPort: 80  # Puerto público
    volumeMounts:
    - name: nginx-config
      mountPath: /etc/nginx/nginx.conf
      subPath: nginx.conf
    # NGINX configurado con rate limiting
    # Proxy pass a 127.0.0.1:8080

  volumes:
  - name: nginx-config
    configMap:
      name: nginx-rate-limit-config
----

===== Patrón Adapter

El contenedor adapter transforma la salida del contenedor principal a un formato estándar.

**Características:**

* Normaliza/estandariza outputs
* Traduce formatos de datos
* Casos de uso: normalizar logs, métricas, monitoreo

**Diagrama:**

----
┌──────────────────────────────────────────┐
│               POD                        │
│  ┌──────────────┐     ┌──────────────┐  │
│  │   Main App   │     │   Adapter    │  │
│  │              │     │              │  │
│  │ Logs en     ─┼────>│ Convierte a  │  │
│  │ formato      │     │ formato      │  │
│  │ custom       │     │ estándar     │  │
│  └──────────────┘     └──────┬───────┘  │
│                              │          │
└──────────────────────────────┼──────────┘
                               │
                               ▼
                    ┌──────────────────┐
                    │ Monitoring System│
                    │ (espera formato  │
                    │  estándar)       │
                    └──────────────────┘
----

**Ejemplo 1: Log Format Adapter**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-log-adapter
spec:
  volumes:
  - name: logs
    emptyDir: {}

  containers:
  # Contenedor principal - genera logs en formato custom
  - name: application
    image: legacy-app:1.0
    volumeMounts:
    - name: logs
      mountPath: /var/log/app
    # App escribe logs en formato propietario

  # Adapter - convierte logs a JSON estándar
  - name: log-adapter
    image: busybox:1.35
    volumeMounts:
    - name: logs
      mountPath: /var/log/app
      readOnly: true
    command:
    - sh
    - -c
    - |
      tail -f /var/log/app/application.log | while read line; do
        # Convierte formato custom a JSON
        timestamp=$(echo $line | cut -d'|' -f1)
        level=$(echo $line | cut -d'|' -f2)
        message=$(echo $line | cut -d'|' -f3)
        echo "{\"timestamp\":\"$timestamp\",\"level\":\"$level\",\"message\":\"$message\"}"
      done
----

**Ejemplo 2: Metrics Format Adapter**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-metrics-adapter
spec:
  containers:
  # Contenedor principal - expone métricas en formato custom
  - name: application
    image: myapp:1.0
    ports:
    - containerPort: 8080  # App port
    - containerPort: 9999  # Custom metrics port

  # Adapter - convierte a formato Prometheus
  - name: metrics-adapter
    image: custom-metrics-adapter:1.0
    ports:
    - containerPort: 9090  # Prometheus metrics port
    env:
    - name: SOURCE_METRICS_URL
      value: "http://127.0.0.1:9999/metrics"
    - name: TARGET_FORMAT
      value: "prometheus"
    resources:
      requests:
        cpu: "50m"
        memory: "64Mi"
----

**Ejemplo 3: Protocol Adapter**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-protocol-adapter
spec:
  containers:
  # Contenedor principal - protocolo binario custom
  - name: legacy-service
    image: legacy-service:1.0
    ports:
    - containerPort: 9000
      protocol: TCP

  # Adapter - traduce a HTTP/REST
  - name: protocol-adapter
    image: protocol-adapter:1.0
    ports:
    - containerPort: 8080  # HTTP endpoint
    env:
    - name: BACKEND_HOST
      value: "127.0.0.1"
    - name: BACKEND_PORT
      value: "9000"
    - name: PROTOCOL
      value: "binary"
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
----

==== Comunicación entre contenedores

Los contenedores en un Pod comparten el mismo namespace de red, lo que facilita su comunicación.

**Métodos de comunicación:**

1. *Network - localhost*
   - Todos los contenedores comparten IP
   - Pueden comunicarse vía `127.0.0.1` o `localhost`
   - No hay aislamiento de red entre contenedores del mismo Pod

2. *IPC (Inter-Process Communication)*
   - Shared memory
   - Semáforos
   - Message queues

3. *Volúmenes compartidos*
   - Archivos
   - Sockets Unix

**Ejemplo 1: Comunicación vía localhost**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-communication
spec:
  containers:
  # Backend - escucha en puerto 8080
  - name: backend
    image: backend-api:1.0
    ports:
    - containerPort: 8080

  # Frontend - se conecta a localhost:8080
  - name: frontend
    image: frontend-app:1.0
    env:
    - name: BACKEND_URL
      value: "http://localhost:8080"  # Mismo Pod!
    ports:
    - containerPort: 80
----

**Ejemplo 2: Comunicación vía volumen compartido**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: shared-volume-communication
spec:
  volumes:
  - name: shared-data
    emptyDir: {}

  containers:
  # Producer - escribe datos
  - name: data-producer
    image: busybox:1.35
    volumeMounts:
    - name: shared-data
      mountPath: /data
    command:
    - sh
    - -c
    - |
      while true; do
        echo "$(date): New data" >> /data/stream.log
        sleep 5
      done

  # Consumer - lee datos
  - name: data-consumer
    image: busybox:1.35
    volumeMounts:
    - name: shared-data
      mountPath: /data
      readOnly: true
    command:
    - sh
    - -c
    - "tail -f /data/stream.log"
----

**Ejemplo 3: Comunicación vía Unix Socket**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: unix-socket-communication
spec:
  volumes:
  - name: socket-dir
    emptyDir: {}

  containers:
  # Server - crea socket Unix
  - name: socket-server
    image: socket-server:1.0
    volumeMounts:
    - name: socket-dir
      mountPath: /var/run/sockets
    command:
    - /app/server
    - --socket=/var/run/sockets/app.sock

  # Client - se conecta al socket
  - name: socket-client
    image: socket-client:1.0
    volumeMounts:
    - name: socket-dir
      mountPath: /var/run/sockets
    command:
    - /app/client
    - --socket=/var/run/sockets/app.sock
----

**Consideraciones de comunicación:**

[cols="1,2,2", options="header"]
|===
|Método
|Ventajas
|Desventajas

|localhost
|Simple, rápido, sin overhead
|No funciona entre Pods

|Volúmenes
|Persistencia, logs, archivos
|I/O más lento que red

|Unix Sockets
|Muy rápido, bajo overhead
|Solo mismo Pod, requiere volumen

|Shared Memory
|Extremadamente rápido
|Complejo, requiere sincronización
|===

==== Volúmenes compartidos

Los volúmenes permiten compartir datos entre contenedores en un Pod.

**Tipos de volúmenes comunes para compartir:**

1. *emptyDir*
   - Directorio vacío creado al iniciar el Pod
   - Destruido cuando el Pod termina
   - Puede ser en disco o memoria (RAM)

2. *configMap*
   - Configuración como archivos
   - Solo lectura
   - Compartir configuración

3. *secret*
   - Datos sensibles como archivos
   - Solo lectura
   - Compartir credenciales

4. *persistentVolumeClaim*
   - Almacenamiento persistente
   - Sobrevive reinicios del Pod

**Ejemplo 1: emptyDir para datos temporales**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: shared-emptydir
spec:
  volumes:
  - name: cache
    emptyDir: {}

  containers:
  - name: writer
    image: busybox:1.35
    volumeMounts:
    - name: cache
      mountPath: /cache
    command:
    - sh
    - -c
    - |
      while true; do
        echo "Writing to cache at $(date)" > /cache/data.txt
        sleep 10
      done

  - name: reader
    image: busybox:1.35
    volumeMounts:
    - name: cache
      mountPath: /cache
      readOnly: true
    command:
    - sh
    - -c
    - "while true; do cat /cache/data.txt; sleep 5; done"
----

**Ejemplo 2: emptyDir en memoria (para alta velocidad)**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: memory-cache
spec:
  volumes:
  - name: cache
    emptyDir:
      medium: Memory  # Usa RAM en lugar de disco
      sizeLimit: 1Gi  # Límite de 1GB

  containers:
  - name: app
    image: myapp:1.0
    volumeMounts:
    - name: cache
      mountPath: /tmp/cache
    resources:
      limits:
        memory: "2Gi"  # Debe ser > sizeLimit del volumen
----

**Ejemplo 3: ConfigMap compartido**

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: shared-config
data:
  app.conf: |
    server_name=example.com
    port=8080
    debug=false
  nginx.conf: |
    worker_processes 2;
    events { worker_connections 1024; }
---
apiVersion: v1
kind: Pod
metadata:
  name: shared-configmap
spec:
  volumes:
  - name: config
    configMap:
      name: shared-config

  containers:
  - name: app
    image: myapp:1.0
    volumeMounts:
    - name: config
      mountPath: /etc/config
      readOnly: true
    command:
    - /app/server
    - --config=/etc/config/app.conf

  - name: nginx
    image: nginx:1.21
    volumeMounts:
    - name: config
      mountPath: /etc/nginx
      readOnly: true
    # Usa nginx.conf del ConfigMap
----

**Ejemplo 4: Patrón completo - Web app con logging**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: webapp-with-sidecar-logging
  labels:
    app: webapp
spec:
  # Volúmenes compartidos
  volumes:
  # Logs compartidos
  - name: app-logs
    emptyDir: {}
  # Configuración
  - name: app-config
    configMap:
      name: webapp-config
  # Datos persistentes
  - name: data
    persistentVolumeClaim:
      claimName: webapp-pvc

  containers:
  # Contenedor principal - Web Application
  - name: webapp
    image: webapp:2.0
    ports:
    - containerPort: 8080

    # Monta todos los volúmenes
    volumeMounts:
    - name: app-logs
      mountPath: /var/log/app
    - name: app-config
      mountPath: /etc/config
      readOnly: true
    - name: data
      mountPath: /var/data

    # Recursos
    resources:
      requests:
        cpu: "500m"
        memory: "512Mi"
      limits:
        cpu: "1000m"
        memory: "1Gi"

    # Health checks
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
      initialDelaySeconds: 30
      periodSeconds: 10

    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      initialDelaySeconds: 10
      periodSeconds: 5

  # Sidecar - Log Shipper
  - name: log-shipper
    image: fluentd:v1.14
    volumeMounts:
    - name: app-logs
      mountPath: /var/log/app
      readOnly: true

    env:
    - name: FLUENT_ELASTICSEARCH_HOST
      value: "elasticsearch.logging.svc.cluster.local"
    - name: FLUENT_ELASTICSEARCH_PORT
      value: "9200"

    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "200m"
        memory: "256Mi"

  # Sidecar - Metrics Exporter
  - name: metrics-exporter
    image: prometheus-exporter:1.0
    ports:
    - name: metrics
      containerPort: 9090

    volumeMounts:
    - name: app-logs
      mountPath: /var/log/app
      readOnly: true

    resources:
      requests:
        cpu: "50m"
        memory: "64Mi"
      limits:
        cpu: "100m"
        memory: "128Mi"
----

**Best Practices para Pods Multi-contenedor:**

1. *Mantén sidecars ligeros*
   - Limita CPU y memoria
   - Evita sobrecarga innecesaria

2. *Define recursos para TODOS los contenedores*
   - Requests y limits individuales
   - El total cuenta para scheduling

3. *Usa volúmenes apropiados*
   - emptyDir para datos temporales
   - ConfigMaps para configuración
   - Secrets para credenciales
   - PVC para datos persistentes

4. *Considera el orden de inicio*
   - Usa init containers si necesitas orden estricto
   - Sidecars inician en paralelo con el main

5. *Health checks individuales*
   - Cada contenedor debe tener sus propios probes
   - Un contenedor fallido afecta todo el Pod

6. *Logging y debugging*
   - Usa nombres descriptivos para contenedores
   - Especifica `-c container-name` en comandos kubectl

=== Init Containers

Los Init Containers son contenedores especializados que se ejecutan ANTES de que los contenedores principales de la aplicación inicien. Son útiles para tareas de inicialización y setup.

==== Propósito y casos de uso

**Características de los Init Containers:**

* Se ejecutan hasta completarse antes de que los contenedores principales inicien
* Se ejecutan en orden secuencial (uno tras otro)
* Si uno falla, Kubernetes reinicia el Pod
* Tienen su propia imagen, recursos y configuración
* Comparten volúmenes con los contenedores principales

**Diferencias con contenedores regulares:**

[cols="1,2,2", options="header"]
|===
|Aspecto
|Init Containers
|Contenedores Principales

|Ejecución
|Secuencial, antes de los principales
|Paralelo, después de init containers

|Ciclo de vida
|Ejecuta hasta completarse y termina
|Corre continuamente (generalmente)

|Recursos
|Puede usar más recursos temporalmente
|Recursos persistentes

|Probes
|No soporta liveness, readiness
|Soporta todos los probes

|Reinicio
|Reinicia en orden si falla
|Reinicia según restartPolicy
|===

**Casos de uso comunes:**

1. *Esperar dependencias*
   - Esperar a que una base de datos esté lista
   - Verificar que servicios externos estén disponibles
   - Comprobar precondiciones

2. *Setup de configuración*
   - Generar archivos de configuración
   - Procesar templates
   - Descargar configuración desde servicios externos

3. *Clonar repositorios*
   - Git clone de código/configuración
   - Descargar assets/recursos
   - Preparar contenido estático

4. *Registrar el Pod*
   - Registrar en service discovery
   - Actualizar load balancers
   - Notificar sistemas externos

5. *Seguridad y permisos*
   - Cambiar permisos de archivos
   - Generar certificados
   - Configurar secrets

6. *Migraciones de base de datos*
   - Ejecutar scripts de migración
   - Verificar esquema
   - Seed de datos iniciales

==== Configuración de init containers

**Sintaxis básica:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  # Init containers se definen aquí
  initContainers:
  - name: init-myservice
    image: busybox:1.35
    command: ['sh', '-c', 'echo Init container 1']

  - name: init-mydb
    image: busybox:1.35
    command: ['sh', '-c', 'echo Init container 2']

  # Contenedores principales
  containers:
  - name: myapp-container
    image: myapp:1.0
----

**Ejemplo 1: Esperar a que un servicio esté disponible**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: myapp-with-init
spec:
  initContainers:
  # Espera a que el servicio MySQL esté disponible
  - name: wait-for-mysql
    image: busybox:1.35
    command:
    - 'sh'
    - '-c'
    - |
      echo "Waiting for MySQL to be ready..."
      until nslookup mysql.default.svc.cluster.local; do
        echo "MySQL not ready yet, waiting..."
        sleep 2
      done
      echo "MySQL is ready!"

  # Espera a que el servicio Redis esté disponible
  - name: wait-for-redis
    image: busybox:1.35
    command:
    - 'sh'
    - '-c'
    - |
      echo "Waiting for Redis..."
      until nslookup redis.default.svc.cluster.local; do
        echo "Redis not ready, waiting..."
        sleep 2
      done
      echo "Redis is ready!"

  # Contenedor principal
  containers:
  - name: myapp
    image: myapp:1.0
    env:
    - name: MYSQL_HOST
      value: "mysql.default.svc.cluster.local"
    - name: REDIS_HOST
      value: "redis.default.svc.cluster.local"
    ports:
    - containerPort: 8080
----

**Ejemplo 2: Clonar repositorio Git**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: web-with-git-init
spec:
  # Volumen compartido
  volumes:
  - name: workdir
    emptyDir: {}

  initContainers:
  # Clona el repositorio
  - name: git-clone
    image: alpine/git:latest
    args:
    - clone
    - --single-branch
    - --branch=main
    - https://github.com/example/website.git
    - /work-dir
    volumeMounts:
    - name: workdir
      mountPath: /work-dir

  # Instala dependencias
  - name: install-deps
    image: node:18-alpine
    workingDir: /work-dir
    command:
    - npm
    - install
    volumeMounts:
    - name: workdir
      mountPath: /work-dir

  # Construye la aplicación
  - name: build
    image: node:18-alpine
    workingDir: /work-dir
    command:
    - npm
    - run
    - build
    volumeMounts:
    - name: workdir
      mountPath: /work-dir

  # Contenedor principal
  containers:
  - name: web-server
    image: nginx:1.21
    volumeMounts:
    - name: workdir
      mountPath: /usr/share/nginx/html
      subPath: dist  # Usa solo el directorio dist
    ports:
    - containerPort: 80
----

**Ejemplo 3: Generar configuración**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-generated-config
spec:
  volumes:
  - name: config
    emptyDir: {}
  - name: config-template
    configMap:
      name: app-config-template

  initContainers:
  # Genera configuración desde template
  - name: generate-config
    image: busybox:1.35
    volumeMounts:
    - name: config-template
      mountPath: /templates
    - name: config
      mountPath: /config
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    command:
    - sh
    - -c
    - |
      # Reemplaza variables en template
      sed -e "s/\${POD_NAME}/$POD_NAME/g" \
          -e "s/\${POD_NAMESPACE}/$POD_NAMESPACE/g" \
          -e "s/\${POD_IP}/$POD_IP/g" \
          /templates/app.conf.template > /config/app.conf
      echo "Configuration generated:"
      cat /config/app.conf

  containers:
  - name: application
    image: myapp:1.0
    volumeMounts:
    - name: config
      mountPath: /etc/config
    command:
    - /app/server
    - --config=/etc/config/app.conf
----

**Ejemplo 4: Migración de base de datos**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-db-migration
spec:
  initContainers:
  # Ejecuta migraciones de base de datos
  - name: db-migration
    image: migrate/migrate:v4.15.2
    command:
    - migrate
    - -path=/migrations
    - -database=postgres://user:password@postgres:5432/mydb?sslmode=disable
    - up
    volumeMounts:
    - name: migrations
      mountPath: /migrations
    env:
    - name: DB_HOST
      value: "postgres.default.svc.cluster.local"
    - name: DB_PORT
      value: "5432"
    - name: DB_NAME
      value: "mydb"
    - name: DB_USER
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: username
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: password

  containers:
  - name: application
    image: myapp:1.0
    env:
    - name: DATABASE_URL
      value: "postgres://$(DB_USER):$(DB_PASSWORD)@$(DB_HOST):$(DB_PORT)/$(DB_NAME)"
    ports:
    - containerPort: 8080

  volumes:
  - name: migrations
    configMap:
      name: db-migrations
----

**Ejemplo 5: Cambiar permisos y ownership**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-volume-permissions
spec:
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: app-data

  initContainers:
  # Configura permisos correctos
  - name: fix-permissions
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      echo "Fixing permissions on /data..."
      chown -R 1000:1000 /data
      chmod -R 755 /data
      echo "Permissions fixed!"
    volumeMounts:
    - name: data
      mountPath: /data
    securityContext:
      runAsUser: 0  # Necesita root para cambiar ownership

  containers:
  - name: application
    image: myapp:1.0
    volumeMounts:
    - name: data
      mountPath: /var/app/data
    securityContext:
      runAsUser: 1000  # App corre como usuario 1000
      runAsGroup: 1000
----

**Ejemplo 6: Verificar precondiciones**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-health-check
spec:
  initContainers:
  # Verifica que la API externa esté disponible
  - name: check-api
    image: curlimages/curl:latest
    command:
    - sh
    - -c
    - |
      echo "Checking external API health..."
      max_attempts=30
      attempt=0

      until [ $attempt -eq $max_attempts ]; do
        if curl -f https://api.example.com/health; then
          echo "API is healthy!"
          exit 0
        fi
        echo "API not ready, attempt $((attempt+1))/$max_attempts"
        sleep 10
        attempt=$((attempt+1))
      done

      echo "API failed health check after $max_attempts attempts"
      exit 1

  # Verifica conectividad de base de datos
  - name: check-database
    image: postgres:14-alpine
    command:
    - sh
    - -c
    - |
      echo "Checking database connectivity..."
      until pg_isready -h $DB_HOST -p $DB_PORT -U $DB_USER; do
        echo "Database not ready, waiting..."
        sleep 3
      done
      echo "Database is ready!"
    env:
    - name: DB_HOST
      value: "postgres.default.svc.cluster.local"
    - name: DB_PORT
      value: "5432"
    - name: DB_USER
      value: "myapp"
    - name: PGPASSWORD
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: password

  containers:
  - name: application
    image: myapp:1.0
    env:
    - name: DATABASE_URL
      value: "postgres://myapp@postgres.default.svc.cluster.local:5432/mydb"
----

**Ejemplo 7: Descargar certificados**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-certs
spec:
  volumes:
  - name: certs
    emptyDir: {}

  initContainers:
  # Descarga certificados desde Vault
  - name: fetch-certificates
    image: vault:1.12.0
    command:
    - sh
    - -c
    - |
      # Autentica con Vault
      vault login -method=kubernetes role=myapp

      # Descarga certificados
      vault read -field=certificate secret/certs/myapp > /certs/tls.crt
      vault read -field=key secret/certs/myapp > /certs/tls.key
      vault read -field=ca secret/certs/ca > /certs/ca.crt

      # Configura permisos
      chmod 600 /certs/tls.key
      chmod 644 /certs/tls.crt /certs/ca.crt

      echo "Certificates downloaded successfully"
    volumeMounts:
    - name: certs
      mountPath: /certs
    env:
    - name: VAULT_ADDR
      value: "https://vault.default.svc.cluster.local:8200"

  containers:
  - name: application
    image: myapp:1.0
    volumeMounts:
    - name: certs
      mountPath: /etc/ssl/certs
      readOnly: true
    env:
    - name: TLS_CERT_FILE
      value: "/etc/ssl/certs/tls.crt"
    - name: TLS_KEY_FILE
      value: "/etc/ssl/certs/tls.key"
    - name: CA_CERT_FILE
      value: "/etc/ssl/certs/ca.crt"
    ports:
    - containerPort: 8443
----

==== Orden de ejecución

Los init containers se ejecutan en un orden muy específico, que es crucial para entender su comportamiento.

**Flujo de ejecución:**

----
Pod creado
    │
    ▼
Scheduler asigna Pod a nodo
    │
    ▼
Kubelet descarga imágenes
    │
    ▼
┌───────────────────────────────┐
│ INIT CONTAINERS (secuencial)  │
│                               │
│  ┌────────────────────┐       │
│  │ Init Container 1   │       │
│  │ (debe completarse) │       │
│  └─────────┬──────────┘       │
│            │ ✓ Success        │
│            ▼                  │
│  ┌────────────────────┐       │
│  │ Init Container 2   │       │
│  │ (debe completarse) │       │
│  └─────────┬──────────┘       │
│            │ ✓ Success        │
│            ▼                  │
│  ┌────────────────────┐       │
│  │ Init Container N   │       │
│  │ (debe completarse) │       │
│  └─────────┬──────────┘       │
│            │ ✓ Success        │
└────────────┼──────────────────┘
             │
             ▼
┌───────────────────────────────┐
│ MAIN CONTAINERS (paralelo)    │
│                               │
│  ┌────────┐  ┌────────┐       │
│  │ Main 1 │  │ Main 2 │       │
│  └────────┘  └────────┘       │
│  (corren simultáneamente)      │
└───────────────────────────────┘
             │
             ▼
          Pod Running
----

**Reglas de ejecución:**

1. *Orden secuencial estricto*
   - Los init containers se ejecutan uno a la vez
   - El siguiente no inicia hasta que el anterior complete exitosamente
   - Si uno falla, los siguientes no se ejecutan

2. *Debe completarse exitosamente*
   - Cada init container debe terminar con exit code 0
   - Si falla (exit code != 0), Kubernetes reinicia el Pod

3. *Reintentos*
   - Si un init container falla, el Pod se reinicia
   - Todos los init containers se ejecutan de nuevo desde el principio
   - Sigue la `restartPolicy` del Pod

4. *Recursos compartidos*
   - Todos los init containers y main containers comparten volúmenes
   - Los cambios hechos por init containers persisten

**Ejemplo con logging de orden:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: init-order-demo
spec:
  initContainers:
  - name: init-1
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      echo "$(date): Init container 1 starting"
      sleep 5
      echo "$(date): Init container 1 completed"

  - name: init-2
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      echo "$(date): Init container 2 starting"
      sleep 5
      echo "$(date): Init container 2 completed"

  - name: init-3
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      echo "$(date): Init container 3 starting"
      sleep 5
      echo "$(date): Init container 3 completed"

  containers:
  - name: main-app
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      echo "$(date): Main container starting"
      echo "All init containers have completed!"
      sleep 3600

# Ver el orden de ejecución:
# kubectl logs init-order-demo -c init-1
# kubectl logs init-order-demo -c init-2
# kubectl logs init-order-demo -c init-3
# kubectl logs init-order-demo -c main-app
----

**Monitorear init containers:**

[source,bash]
----
# Ver status de init containers
kubectl describe pod myapp-pod

# Salida muestra:
# Init Containers:
#   init-myservice:
#     State:      Terminated
#       Reason:   Completed
#       Exit Code: 0
#   init-mydb:
#     State:      Running
#       Started:  ...

# Ver logs de init container específico
kubectl logs myapp-pod -c init-myservice

# Ver logs de todos los init containers
kubectl logs myapp-pod --all-containers=true --prefix=true

# Ver eventos para troubleshooting
kubectl get events --field-selector involvedObject.name=myapp-pod
----

**Ejemplo de manejo de fallos:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: init-failure-handling
spec:
  restartPolicy: Always  # Reinicia automáticamente en caso de fallo

  initContainers:
  # Este init container podría fallar
  - name: check-dependency
    image: busybox:1.35
    command:
    - sh
    - -c
    - |
      max_retries=10
      retry_interval=5

      for i in $(seq 1 $max_retries); do
        echo "Attempt $i/$max_retries to connect to service..."

        if nslookup my-service.default.svc.cluster.local; then
          echo "Service found!"
          exit 0
        fi

        if [ $i -eq $max_retries ]; then
          echo "Failed after $max_retries attempts"
          exit 1
        fi

        echo "Service not found, waiting $retry_interval seconds..."
        sleep $retry_interval
      done

  containers:
  - name: app
    image: myapp:1.0
----

**Best Practices para Init Containers:**

1. *Mantén init containers ligeros*
   - Usa imágenes pequeñas (busybox, alpine)
   - Solo realiza tareas necesarias

2. *Implementa timeouts*
   - No esperes indefinidamente
   - Falla rápido si algo está mal

3. *Logging detallado*
   - Log cada paso importante
   - Facilita debugging

4. *Idempotencia*
   - Los init containers deben poder ejecutarse múltiples veces
   - Deben producir el mismo resultado

5. *Minimiza dependencias*
   - Cuantos menos init containers, mejor
   - Agrupa tareas relacionadas si es posible

6. *Recursos apropiados*
   - Define requests/limits
   - Init containers pueden usar más recursos temporalmente

7. *Manejo de errores*
   - Exit codes apropiados (0 = éxito, != 0 = fallo)
   - Logs descriptivos de errores

=== Debugging y Troubleshooting

El debugging de Pods es una habilidad esencial para cualquier desarrollador o administrador de Kubernetes. Esta sección cubre las técnicas y herramientas más importantes.

==== Logs de contenedores

Los logs son la primera herramienta para diagnosticar problemas en Pods.

**Comandos básicos de logs:**

[source,bash]
----
# Ver logs de un Pod (si solo tiene un contenedor)
kubectl logs pod-name

# Ver logs de contenedor específico
kubectl logs pod-name -c container-name

# Seguir logs en tiempo real (similar a tail -f)
kubectl logs -f pod-name

# Ver logs de un contenedor anterior (si crasheó)
kubectl logs pod-name --previous

# Últimas N líneas
kubectl logs pod-name --tail=100

# Logs desde hace X tiempo
kubectl logs pod-name --since=10m
kubectl logs pod-name --since=1h
kubectl logs pod-name --since=2024-01-15T10:00:00Z

# Logs con timestamps
kubectl logs pod-name --timestamps

# Logs de todos los contenedores en el Pod
kubectl logs pod-name --all-containers=true

# Con prefijo del nombre del contenedor
kubectl logs pod-name --all-containers=true --prefix=true

# Combinar múltiples opciones
kubectl logs pod-name --tail=50 --timestamps --since=1h -f
----

**Ejemplo de logs con múltiples contenedores:**

[source,bash]
----
# Ver qué contenedores tiene el Pod
kubectl get pod myapp -o jsonpath='{.spec.containers[*].name}'

# Ver logs de cada contenedor
kubectl logs myapp -c webapp
kubectl logs myapp -c sidecar
kubectl logs myapp -c init-container --previous

# Ver logs de todos con prefijos
kubectl logs myapp --all-containers=true --prefix=true --tail=20
----

**Troubleshooting con logs:**

[source,bash]
----
# Caso 1: CrashLoopBackOff - ver por qué crashea
kubectl logs crashing-pod --previous

# Caso 2: Pod no inicia - ver init containers
kubectl describe pod stuck-pod
kubectl logs stuck-pod -c init-container-name

# Caso 3: Errores intermitentes - seguir en tiempo real
kubectl logs problematic-pod -f | grep ERROR

# Caso 4: Filtrar logs para búsqueda específica
kubectl logs myapp | grep "database connection"
kubectl logs myapp --since=30m | grep -i error

# Caso 5: Guardar logs para análisis
kubectl logs myapp --all-containers=true > pod-logs.txt
----

**Limitaciones de kubectl logs:**

* Solo muestra logs de stdout/stderr
* No persiste logs de Pods eliminados
* Limitado a logs del contenedor actual y anterior
* Para logs históricos, usa sistemas de agregación (ELK, Loki, etc.)

==== Ejecución de comandos en Pods

Ejecutar comandos dentro de contenedores es fundamental para debugging interactivo.

**kubectl exec:**

[source,bash]
----
# Ejecutar comando simple
kubectl exec pod-name -- ls -la /app

# Ver variables de entorno
kubectl exec pod-name -- env

# Ver procesos
kubectl exec pod-name -- ps aux

# Ver conectividad de red
kubectl exec pod-name -- ping google.com
kubectl exec pod-name -- nslookup mysql.default.svc.cluster.local
kubectl exec pod-name -- curl http://api-service:8080/health

# En contenedor específico
kubectl exec pod-name -c container-name -- command

# Shell interactivo (bash)
kubectl exec -it pod-name -- /bin/bash

# Shell interactivo (sh si no hay bash)
kubectl exec -it pod-name -- /bin/sh

# Shell en contenedor específico
kubectl exec -it pod-name -c sidecar -- /bin/bash

# Ejecutar script
kubectl exec pod-name -- /bin/bash -c "echo 'Script commands here'"
----

**Ejemplos prácticos de debugging:**

[source,bash]
----
# 1. Verificar archivos de configuración
kubectl exec myapp -- cat /etc/config/app.conf

# 2. Verificar permisos de archivos
kubectl exec myapp -- ls -la /var/data

# 3. Verificar espacio en disco
kubectl exec myapp -- df -h

# 4. Verificar memoria
kubectl exec myapp -- free -m

# 5. Ver logs de aplicación dentro del contenedor
kubectl exec myapp -- tail -f /var/log/app/application.log

# 6. Probar conectividad a base de datos
kubectl exec myapp -- nc -zv mysql.default.svc.cluster.local 3306

# 7. Verificar DNS
kubectl exec myapp -- nslookup kubernetes.default
kubectl exec myapp -- cat /etc/resolv.conf

# 8. Verificar procesos
kubectl exec myapp -- ps aux | grep java

# 9. Instalar herramientas de debugging (temporal)
kubectl exec -it myapp -- sh
  apk add curl  # En Alpine Linux
  apt-get update && apt-get install -y curl  # En Debian/Ubuntu
  yum install -y curl  # En RHEL/CentOS

# 10. Verificar montaje de volúmenes
kubectl exec myapp -- mount | grep /var/data
----

**Debugging interactivo avanzado:**

[source,bash]
----
# Sesión interactiva completa
kubectl exec -it myapp -- /bin/bash

# Dentro del contenedor:
> whoami
> pwd
> env | grep DATABASE
> cat /proc/1/environ | tr '\0' '\n'  # Variables del proceso principal
> netstat -tulpn  # Puertos en escucha
> ss -tulpn  # Alternativa moderna a netstat
> lsof -i :8080  # Ver qué usa el puerto 8080
> curl localhost:8080/health
> top  # Ver uso de recursos
> strace -p 1  # Trace del proceso principal (requiere privilegios)
----

**Limitaciones de kubectl exec:**

* Requiere que el contenedor esté corriendo
* No funciona en contenedores terminados
* Algunas imágenes mínimas no tienen shell (busybox, scratch)
* Puede requerir permisos adicionales según el securityContext

==== Port-forwarding

Port-forwarding permite acceder a puertos de Pods sin exponer Services.

**Sintaxis básica:**

[source,bash]
----
# Forward puerto del Pod a localhost
kubectl port-forward pod/pod-name 8080:80

# Especificar puerto local diferente
kubectl port-forward pod/nginx 8888:80

# Múltiples puertos
kubectl port-forward pod/myapp 8080:80 9090:9000

# Forward de Service
kubectl port-forward service/myapp 8080:80

# Forward de Deployment
kubectl port-forward deployment/myapp 8080:8080

# Escuchar en todas las interfaces (por defecto solo localhost)
kubectl port-forward --address 0.0.0.0 pod/myapp 8080:80

# Especificar namespace
kubectl port-forward -n production pod/myapp 8080:80

# Forward en background (requiere & al final)
kubectl port-forward pod/myapp 8080:80 > /dev/null 2>&1 &
----

**Casos de uso:**

**1. Acceder a aplicación web:**

[source,bash]
----
# Forward puerto de app web
kubectl port-forward pod/webapp 8080:80

# Abrir en navegador
open http://localhost:8080
----

**2. Acceder a base de datos:**

[source,bash]
----
# Forward puerto de PostgreSQL
kubectl port-forward pod/postgres-0 5432:5432

# Conectar con cliente local
psql -h localhost -p 5432 -U myuser -d mydb

# O con otra herramienta
pgcli postgresql://user:pass@localhost:5432/mydb
----

**3. Debugging de API:**

[source,bash]
----
# Forward API
kubectl port-forward pod/api-server 8080:8080

# Probar endpoints
curl http://localhost:8080/health
curl http://localhost:8080/api/v1/users

# Con Postman/Insomnia
# URL: http://localhost:8080
----

**4. Acceder a Prometheus/Grafana:**

[source,bash]
----
# Prometheus
kubectl port-forward -n monitoring pod/prometheus-0 9090:9090

# Grafana
kubectl port-forward -n monitoring service/grafana 3000:3000

# Acceder en navegador
open http://localhost:9090  # Prometheus
open http://localhost:3000  # Grafana
----

**5. Debugging de Kafka:**

[source,bash]
----
# Forward Kafka broker
kubectl port-forward pod/kafka-0 9092:9092

# Producir mensaje
echo "test message" | kafka-console-producer --broker-list localhost:9092 --topic test

# Consumir mensajes
kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning
----

**6. Forward múltiples Pods simultáneamente:**

[source,bash]
----
# Terminal 1
kubectl port-forward pod/pod-1 8080:80

# Terminal 2
kubectl port-forward pod/pod-2 8081:80

# Terminal 3
kubectl port-forward pod/pod-3 8082:80
----

**Automatización de port-forward:**

[source,bash]
----
# Script para mantener port-forward activo
#!/bin/bash

POD_NAME="myapp"
LOCAL_PORT=8080
REMOTE_PORT=80

while true; do
  echo "Starting port-forward..."
  kubectl port-forward pod/$POD_NAME $LOCAL_PORT:$REMOTE_PORT

  if [ $? -ne 0 ]; then
    echo "Port-forward failed, retrying in 5 seconds..."
    sleep 5
  fi
done
----

**Alternativa: kubectl proxy:**

[source,bash]
----
# Inicia proxy a API server
kubectl proxy --port=8001

# Acceder a Pods vía proxy API
# http://localhost:8001/api/v1/namespaces/default/pods/pod-name/proxy/

# Ejemplo: acceder a servicio en pod
curl http://localhost:8001/api/v1/namespaces/default/pods/myapp/proxy/health
----

==== Diagnóstico de problemas comunes

**Problema 1: Pod en estado Pending**

[source,bash]
----
# Ver detalles del Pod
kubectl describe pod pending-pod

# Buscar en eventos:
# - "Insufficient cpu" o "Insufficient memory"
# - "No nodes available"
# - "FailedScheduling"

# Soluciones:
# 1. Ver recursos disponibles en nodos
kubectl top nodes

# 2. Ver qué está consumiendo recursos
kubectl top pods --all-namespaces --sort-by=memory
kubectl top pods --all-namespaces --sort-by=cpu

# 3. Reducir requests del Pod o agregar nodos

# 4. Verificar taints y tolerations
kubectl describe nodes | grep -A 5 Taints

# 5. Ver por qué no se programa
kubectl get events --field-selector involvedObject.name=pending-pod
----

**Problema 2: ImagePullBackOff / ErrImagePull**

[source,bash]
----
# Ver detalles
kubectl describe pod image-pull-pod

# Eventos comunes:
# - "Failed to pull image"
# - "repository does not exist"
# - "unauthorized: authentication required"

# Verificaciones:
# 1. Nombre de imagen correcto
kubectl get pod image-pull-pod -o jsonpath='{.spec.containers[*].image}'

# 2. Registry accesible
# Probar pull manual en un nodo
docker pull nginx:1.21

# 3. Verificar imagePullSecrets
kubectl get pod image-pull-pod -o jsonpath='{.spec.imagePullSecrets}'

# 4. Ver si el secret existe
kubectl get secrets

# Soluciones:
# - Corregir nombre de imagen
# - Crear imagePullSecret si es registry privado
kubectl create secret docker-registry regcred \
  --docker-server=https://index.docker.io/v1/ \
  --docker-username=user \
  --docker-password=pass \
  --docker-email=email@example.com

# - Referenciar en Pod
spec:
  imagePullSecrets:
  - name: regcred
----

**Problema 3: CrashLoopBackOff**

[source,bash]
----
# Ver logs del crash
kubectl logs crash-pod --previous

# Ver razón de terminación
kubectl get pod crash-pod -o jsonpath='{.status.containerStatuses[0].lastState.terminated.reason}'

# Ver exit code
kubectl get pod crash-pod -o jsonpath='{.status.containerStatuses[0].lastState.terminated.exitCode}'

# Causas comunes:
# - Exit code 137: OOMKilled (Out of Memory)
# - Exit code 1: Error de aplicación
# - Exit code 0 con restartPolicy=Always: App termina cuando no debería

# Debugging:
# 1. Si es OOMKilled, aumentar memory limits
resources:
  limits:
    memory: "512Mi"  # Aumentar

# 2. Ver uso de memoria antes del kill
kubectl top pod crash-pod

# 3. Si es error de aplicación, revisar logs
kubectl logs crash-pod --previous | tail -50

# 4. Verificar health checks
kubectl describe pod crash-pod | grep -A 10 "Liveness\|Readiness"

# 5. Ejecutar localmente para debug
docker run -it image:tag sh
----

**Problema 4: Pod no recibe tráfico**

[source,bash]
----
# 1. Verificar que Pod está Ready
kubectl get pods

# Si no está Ready:
kubectl describe pod not-ready-pod

# 2. Verificar readiness probe
kubectl describe pod not-ready-pod | grep -A 10 Readiness

# 3. Probar readiness probe manualmente
kubectl exec not-ready-pod -- curl http://localhost:8080/ready

# 4. Verificar Service
kubectl get service myapp-service

# 5. Ver endpoints del Service
kubectl get endpoints myapp-service

# Si endpoints está vacío:
# - Verificar que labels del Pod coinciden con selector del Service
kubectl get pod not-ready-pod --show-labels
kubectl get service myapp-service -o jsonpath='{.spec.selector}'

# 6. Probar conectividad desde otro Pod
kubectl run test --image=busybox:1.35 --rm -it -- sh
  wget -O- http://myapp-service:80
----

**Problema 5: Problemas de red/DNS**

[source,bash]
----
# 1. Verificar DNS desde Pod
kubectl exec myapp -- nslookup kubernetes.default

# 2. Ver configuración DNS
kubectl exec myapp -- cat /etc/resolv.conf

# 3. Verificar CoreDNS está corriendo
kubectl get pods -n kube-system -l k8s-app=kube-dns

# 4. Ver logs de CoreDNS
kubectl logs -n kube-system -l k8s-app=kube-dns

# 5. Probar conectividad pod-to-pod
# Desde un Pod temporal
kubectl run test --image=busybox:1.35 --rm -it -- sh
  ping 10.244.1.5  # IP del Pod destino
  telnet myapp-service 80
  nc -zv myapp-service 80

# 6. Verificar Network Policies
kubectl get networkpolicies --all-namespaces

# 7. Verificar CNI plugin
kubectl get pods -n kube-system | grep calico
kubectl get pods -n kube-system | grep flannel
----

**Problema 6: Problemas de volúmenes**

[source,bash]
----
# 1. Verificar PVC está bound
kubectl get pvc

# 2. Ver detalles de PVC
kubectl describe pvc my-pvc

# 3. Ver PV asociado
kubectl get pv

# 4. Ver eventos de volumen
kubectl get events --field-selector reason=FailedMount

# 5. Verificar permisos de montaje
kubectl exec pod-with-volume -- ls -la /mount/path

# 6. Ver tipo de error
kubectl describe pod pod-with-volume | grep -A 20 "Events:"

# Errores comunes:
# - "Volume could not be attached"
# - "Multi-Attach error"
# - "Permission denied"

# 7. Verificar StorageClass
kubectl get storageclass
kubectl describe storageclass standard
----

**Problema 7: ConfigMap/Secret no encontrado**

[source,bash]
----
# Ver error
kubectl describe pod config-pod

# Error típico: "configmap 'my-config' not found"

# 1. Listar ConfigMaps
kubectl get configmaps

# 2. Verificar namespace
kubectl get configmaps --all-namespaces | grep my-config

# 3. Ver contenido
kubectl describe configmap my-config

# 4. Verificar nombre en Pod
kubectl get pod config-pod -o yaml | grep configMap

# 5. Crear si no existe
kubectl create configmap my-config --from-literal=key=value
----

**Herramientas de debugging avanzadas:**

**1. Ephemeral Containers (Kubernetes 1.23+):**

[source,bash]
----
# Agregar contenedor de debugging a Pod en ejecución
kubectl debug -it pod-name --image=busybox:1.35 --target=container-name

# Con imagen de debugging más completa
kubectl debug -it pod-name --image=nicolaka/netshoot

# Copiar Pod para debugging (no afecta original)
kubectl debug pod-name -it --copy-to=pod-name-debug --container=debug -- sh
----

**2. kubectl-debug plugin:**

[source,bash]
----
# Instalar via krew
kubectl krew install debug

# Debugear Pod
kubectl debug pod-name --agentless --port-forward
----

**3. Stern (logs de múltiples pods):**

[source,bash]
----
# Instalar stern
brew install stern  # macOS
# O desde releases: github.com/stern/stern

# Ver logs de todos los pods que coincidan
stern myapp

# Con selector de labels
stern --selector app=myapp

# Múltiples contenedores
stern myapp --all-namespaces --container=sidecar
----

**4. Kubetail:**

[source,bash]
----
# Ver logs de múltiples pods en paralelo
kubetail myapp

# Con namespace
kubetail myapp -n production

# Con selector
kubetail -l app=myapp
----

**Checklist de troubleshooting:**

[source,bash]
----
# 1. Estado del Pod
kubectl get pod problematic-pod -o wide

# 2. Eventos
kubectl get events --field-selector involvedObject.name=problematic-pod --sort-by='.lastTimestamp'

# 3. Descripción completa
kubectl describe pod problematic-pod

# 4. Logs
kubectl logs problematic-pod --all-containers=true --previous

# 5. YAML actual
kubectl get pod problematic-pod -o yaml

# 6. Recursos del nodo
kubectl describe node node-name

# 7. Estado del cluster
kubectl get nodes
kubectl get pods --all-namespaces

# 8. Logs de componentes del sistema
kubectl logs -n kube-system kube-apiserver-master
kubectl logs -n kube-system kube-controller-manager-master
kubectl logs -n kube-system kube-scheduler-master

# 9. Verificar RBAC si hay problemas de permisos
kubectl auth can-i --list --as=system:serviceaccount:default:myapp

# 10. Métricas (si metrics-server está instalado)
kubectl top pod problematic-pod
kubectl top node
----

**Best Practices para Debugging:**

1. *Logging estructurado*
   - Usa JSON en logs para facilitar parsing
   - Incluye IDs de request/transaction
   - Niveles de log apropiados (DEBUG, INFO, WARN, ERROR)

2. *Health checks bien implementados*
   - Readiness: verifica dependencias
   - Liveness: verifica estado de la app
   - No compartas la misma ruta para ambos

3. *Recursos definidos correctamente*
   - Requests basadas en uso real
   - Limits con margen para picos
   - Monitorear uso real con `kubectl top`

4. *Labels y annotations*
   - Labels consistentes para selección
   - Annotations con metadata útil (version, git commit, etc.)

5. *Monitoreo proactivo*
   - Usa Prometheus/Grafana
   - Alertas en métricas clave
   - Logs centralizados (ELK, Loki)

6. *Debugging en desarrollo*
   - Usa entornos locales (minikube, kind)
   - Prueba en ambiente similar a producción
   - Usa herramientas de debugging remoto cuando sea necesario

== Módulo 3: Controllers y Workloads

=== ReplicaSets

ReplicaSets son controladores que aseguran que un número específico de réplicas de un Pod se ejecutan en todo momento. Son la base de los Deployments.

**Nota**: En la mayoría de los casos, deberías usar Deployments en lugar de ReplicaSets directamente, ya que los Deployments proporcionan actualizaciones declarativas de Pods.

==== Función y propósito

**¿Qué es un ReplicaSet?**

Un ReplicaSet es un controlador que:

* Mantiene un conjunto estable de Pods en ejecución
* Asegura que un número especificado de réplicas del Pod existe en todo momento
* Crea/elimina Pods según sea necesario para mantener el número deseado
* Reemplaza Pods que fallan, son eliminados o terminados

**Diagrama de funcionamiento:**

----
┌──────────────────────────────────────┐
│       ReplicaSet Controller          │
│  (Observa y mantiene estado)         │
├──────────────────────────────────────┤
│                                      │
│  Deseado: 3 réplicas                 │
│  Actual:  2 Pods corriendo           │
│  Acción:  Crear 1 Pod más            │
│                                      │
└──────────────────────────────────────┘
             │
             ├──────────────────────┐
             │                      │
         ┌───▼────┐  ┌───┐  ┌──────▼──┐
         │ Pod 1  │  │Pod│  │ Pod 3   │
         │Running │  │ 2 │  │Creating │
         └────────┘  └───┘  └─────────┘
----

==== Definición de ReplicaSets

**Sintaxis básica:**

[source,yaml]
----
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
  labels:
    app: nginx
spec:
  replicas: 3  # Número de Pods deseados
  selector:
    matchLabels:
      app: nginx  # Debe coincidir con labels del template
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
----

**Campos importantes:**

[cols="1,3", options="header"]
|===
|Campo
|Descripción

|replicas
|Número de Pods que deben estar ejecutándose

|selector
|Cómo identificar Pods que pertenecen a este ReplicaSet

|template
|Especificación del Pod a crear

|minReadySeconds
|Segundos que un Pod debe estar listo antes de contarse como disponible

|progressDeadlineSeconds
|Tiempo máximo para que ReplicaSet haga progreso
|===

**Ejemplo completo con más opciones:**

[source,yaml]
----
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: web-rs
  namespace: production
  labels:
    app: web
    tier: frontend
spec:
  replicas: 5
  minReadySeconds: 10  # Espera 10s después de Ready
  progressDeadlineSeconds: 600  # Timeout de 10 minutos

  selector:
    matchLabels:
      app: web
      tier: frontend
    matchExpressions:
    - key: version
      operator: In
      values:
      - v1
      - v1.1

  template:
    metadata:
      labels:
        app: web
        tier: frontend
        version: v1.1
    spec:
      containers:
      - name: web
        image: web:1.1
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: "250m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
----

**Comandos básicos:**

[source,bash]
----
# Crear ReplicaSet
kubectl apply -f replicaset.yaml

# Listar ReplicaSets
kubectl get replicaset
kubectl get rs

# Ver detalles
kubectl describe rs nginx-rs

# Ver Pods creados por ReplicaSet
kubectl get pods -l app=nginx

# Ver YAML completo
kubectl get rs nginx-rs -o yaml

# Eliminar ReplicaSet (también elimina Pods)
kubectl delete rs nginx-rs

# Eliminar ReplicaSet pero mantener Pods
kubectl delete rs nginx-rs --cascade=orphan
----

==== Escalado manual

**Cambiar el número de réplicas:**

[source,bash]
----
# Escalar usando kubectl scale
kubectl scale rs nginx-rs --replicas=5

# Escalar a cero (detiene todos los Pods)
kubectl scale rs nginx-rs --replicas=0

# Escalar a 2
kubectl scale rs nginx-rs --replicas=2

# Escalar basado en archivo YAML
kubectl scale -f replicaset.yaml --replicas=10
----

**Editar ReplicaSet directamente:**

[source,bash]
----
# Editar en línea
kubectl edit rs nginx-rs

# Buscar el campo replicas y cambiar el número
# spec:
#   replicas: 5   # Cambiar a 3

# Guardar y salir del editor
# El ReplicaSet ajustará automáticamente el número de Pods
----

**Patch para cambiar réplicas:**

[source,bash]
----
# Patch específico
kubectl patch rs nginx-rs -p '{"spec":{"replicas":10}}'

# Patch desde JSON Patch
kubectl patch rs nginx-rs --type='json' -p='[{"op": "replace", "path": "/spec/replicas", "value": 7}]'
----

**Monitorear escalado:**

[source,bash]
----
# Ver Pods mientras se escalan
kubectl get pods -l app=nginx -w

# Ver eventos del ReplicaSet
kubectl describe rs nginx-rs

# Ver métricas si metrics-server está instalado
kubectl top pods -l app=nginx
----

**Ejemplo práctico de escalado:**

[source,bash]
----
# Crear ReplicaSet con 2 réplicas
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: app-rs
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: myapp:1.0
EOF

# Ver Pods creados
kubectl get pods -l app=myapp

# Escalar a 5
kubectl scale rs app-rs --replicas=5

# Ver cómo se crean los Pods
kubectl get pods -l app=myapp -w

# Escalar a 1
kubectl scale rs app-rs --replicas=1

# Ver cómo se eliminan los Pods
kubectl get pods -l app=myapp
----

==== Self-healing

El self-healing es la capacidad de ReplicaSet de recuperarse automáticamente cuando Pods fallan.

**Cómo funciona:**

----
Monitoreo continuo
    │
    ▼
┌──────────────────────────────┐
│ ReplicaSet Controller        │
│ Cada 5-10 segundos:          │
│ 1. Cuenta Pods actuales      │
│ 2. Compara con replicas      │
│ 3. Toma acciones correctivas │
└──────────────────────────────┘
    │
    ├─ Pod falla → Crear nuevo Pod
    ├─ Pod eliminado → Crear nuevo Pod
    ├─ Pod extra → Eliminar Pod
    └─ Pod no listo → Mantener monitoreo
----

**Escenarios de self-healing:**

**1. Pod crashea:**

[source,bash]
----
# Crear ReplicaSet
kubectl apply -f rs.yaml

# Ver Pods
kubectl get pods

# Eliminar manualmente un Pod
kubectl delete pod pod-name

# ReplicaSet crea automáticamente uno nuevo
kubectl get pods

# Ver eventos
kubectl describe rs my-rs
----

**2. Nodo falla:**

[source,bash]
----
# Cuando un nodo falla y sus Pods se marcan como terminados,
# el ReplicaSet crea los Pods en nodos disponibles

# Ver Pods en nodo que falló
kubectl get pods -o wide

# Después de eviction/timeout:
# - Pods se marcan como Terminating/Terminated
# - ReplicaSet crea nuevos Pods en otros nodos
----

**3. Monitoreo de self-healing:**

[source,bash]
----
# Ver eventos de ReplicaSet
kubectl describe rs my-rs

# Salida muestra:
# Events:
#   Type    Reason           Message
#   ----    ------           -------
#   Normal  SuccessfulCreate Created pod: pod-1
#   Normal  SuccessfulDelete Deleted pod: pod-2
#   Normal  SuccessfulCreate Created pod: pod-3

# Ver logs si es necesario
kubectl logs -n kube-system -l component=replicaset-controller
----

**Ejemplo práctico:**

[source,bash]
----
# 1. Crear ReplicaSet con 3 réplicas
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: resilient-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: resilient
  template:
    metadata:
      labels:
        app: resilient
    spec:
      containers:
      - name: app
        image: myapp:1.0
        command: ["sleep", "3600"]
EOF

# 2. Ver Pods creados
kubectl get pods -l app=resilient

# 3. Eliminar un Pod (simular fallo)
kubectl delete pod resilient-app-xxxxx

# 4. Observar cómo ReplicaSet crea uno nuevo
# (Puede tomar 1-2 segundos)
kubectl get pods -l app=resilient

# 5. Ver que hay 3 Pods nuevamente
# Nota: El nuevo Pod tendrá un nombre diferente
----

**Limitaciones del self-healing:**

* Solo recupera el número de réplicas, no realiza backups
* No puede recuperar de fallos de aplicación inherentes
* Los Pods recién creados son instancias nuevas
* Datos locales en el Pod se pierden

**Best Practices:**

1. *Usa Deployments en lugar de ReplicaSets directamente*
   - Los Deployments proporcionan actualizaciones declarativas
   - Mayor flexibilidad para cambios

2. *Define recursos adecuados*
   - CPU/Memory requests para scheduling
   - Limits para evitar agotamiento

3. *Implementa health checks*
   - Liveness probes
   - Readiness probes
   - Los Pods sin respuesta se reemplazan

4. *Usa selectors específicos*
   - Evita capturar Pods no deseados
   - Mantén selectores simples

5. *Monitorea eventos*
   - Revisa frecuentemente eventos del ReplicaSet
   - Alerta en patrones de fallo

6. *Combina con autoscaling*
   - Usa HPA (Horizontal Pod Autoscaler)
   - Escala automáticamente según métricas

=== Deployments

Los Deployments son la forma recomendada de ejecutar Pods stateless en Kubernetes. Proporcionan actualizaciones declarativas de Pods y ReplicaSets, con manejo automático de versiones e historial.

**¿Cuándo usar Deployments?**

* Aplicaciones web stateless
* APIs REST
* Microservicios
* Cualquier aplicación que puede tener múltiples réplicas idénticas

**¿Cuándo NO usar Deployments?**

* Aplicaciones con estado (usa StatefulSets)
* Necesitas controlador personalizado
* Un único Pod (usa Pod directamente)

==== Estrategias de despliegue

**Rolling Update (por defecto)**

En una Rolling Update, los Pods se reemplazan gradualmente. Los Pods nuevos se crean mientras los antiguos se terminan.

**Ventajas:**

* Sin tiempo de inactividad
* Permite detectar problemas con la nueva versión
* Fácil de revertir

**Diagrama:**

----
Versión 1: ▌▌▌▌▌ (5 Pods)
Actualizar a Versión 2...
Paso 1:    ▌▌▌▌ ▎
Paso 2:    ▌▌▌ ▎▎
Paso 3:    ▌▌ ▎▎▎
Paso 4:    ▌ ▎▎▎▎
Paso 5:     ▎▎▎▎▎ (5 Pods de versión 2)
----

**Configuración:**

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deployment
spec:
  replicas: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1          # Máximo Pods extras durante actualización
      maxUnavailable: 0    # Máximo Pods que pueden estar no disponibles
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: web
        image: web:v2
        ports:
        - containerPort: 8080
----

**Parámetros importantes:**

[cols="1,3", options="header"]
|===
|Parámetro
|Descripción

|maxSurge
|Número máximo de Pods extras permitidos durante actualización. Puede ser número o porcentaje (ej: "25%")

|maxUnavailable
|Número máximo de Pods que pueden estar no disponibles. Puede ser número o porcentaje

|minReadySeconds
|Segundos que un Pod debe estar listo antes de considerarlo disponible
|===

**Ejemplo de Rolling Update segura:**

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: production-app
spec:
  replicas: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2          # 2 Pods extras
      maxUnavailable: 1    # 1 Pod puede estar no disponible

  minReadySeconds: 30      # Espera 30s antes de reemplazar

  selector:
    matchLabels:
      app: production

  template:
    metadata:
      labels:
        app: production
        version: v2
    spec:
      containers:
      - name: app
        image: app:v2
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
----

**Recreate**

En una estrategia Recreate, todos los Pods antiguos se terminan antes de crear los nuevos.

**Ventajas:**

* Garantiza que no hay dos versiones corriendo simultáneamente
* Simple de entender
* Uso de recursos predecible

**Desventajas:**

* Tiempo de inactividad durante la actualización
* No ideal para aplicaciones críticas

**Diagrama:**

----
Versión 1: ▌▌▌▌▌
Terminar todos los Pods...
           (esperando)
Crear nuevos Pods...
Versión 2: ▎▎▎▎▎
----

**Configuración:**

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: batch-job
spec:
  replicas: 3
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: batch
  template:
    metadata:
      labels:
        app: batch
    spec:
      containers:
      - name: batch
        image: batch-processor:v2
----

**Cuándo usar Recreate:**

* Aplicaciones que no pueden correr múltiples versiones
* Migraciones de base de datos
* Cambios incompatibles en estado compartido
* Entornos de desarrollo/testing

==== Rollback de versiones

Kubernetes mantiene un historial de revisiones de Deployments, permitiendo rollback rápido.

**Ver historial de despliegues:**

[source,bash]
----
# Ver revisiones
kubectl rollout history deployment/web-deployment

# Salida:
# REVISION  CHANGE-CAUSE
# 1         <none>
# 2         kubectl apply --filename=web.yaml
# 3         kubectl set image deployment/web web=web:v3

# Ver detalles de una revisión específica
kubectl rollout history deployment/web-deployment --revision=2
----

**Realizar rollback:**

[source,bash]
----
# Rollback a la revisión anterior
kubectl rollout undo deployment/web-deployment

# Rollback a una revisión específica
kubectl rollout undo deployment/web-deployment --to-revision=1

# Ver cambios durante rollback
kubectl rollout history deployment/web-deployment

# Ver estado del rollback
kubectl rollout status deployment/web-deployment
----

**Pausar y reanudar despliegues:**

[source,bash]
----
# Pausar un despliegue en progreso
kubectl rollout pause deployment/web-deployment

# Ver estado actual
kubectl get deployment web-deployment

# Hacer cambios mientras está pausado
kubectl set image deployment/web-deployment web=web:v4 --record

# Reanudar el despliegue
kubectl rollout resume deployment/web-deployment

# Ver estado
kubectl rollout status deployment/web-deployment
----

**Ejemplo práctico de actualización con rollback:**

[source,bash]
----
# 1. Crear Deployment original
kubectl apply -f deployment-v1.yaml

# 2. Ver revisión actual
kubectl rollout history deployment/web

# 3. Actualizar imagen
kubectl set image deployment/web web=web:v2 --record

# 4. Monitorear el despliegue
kubectl rollout status deployment/web -w

# 5. Si hay problemas, hacer rollback
kubectl rollout undo deployment/web

# 6. Verificar que se revirtió
kubectl rollout status deployment/web
----

==== Gestión del historial

**Limitar el historial de revisiones:**

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  revisionHistoryLimit: 10  # Mantener últimas 10 revisiones
  # Valor por defecto es 10
  # Establecer en 0 para deshabilitar rollbacks

  # ... resto de configuración
----

**Ver cambios entre revisiones:**

[source,bash]
----
# Ver qué cambió en una revisión
kubectl rollout history deployment/web --revision=2

# Comparar con YAML actual
kubectl get deployment web -o yaml > current.yaml
kubectl get deployment web --revision=1 -o yaml > revision1.yaml
diff revision1.yaml current.yaml

# Ver eventos de despliegue
kubectl describe deployment web | tail -20
----

==== Pause y resume de deployments

**Pausar actualizaciones:**

[source,bash]
----
# Útil para aplicar múltiples cambios a la vez
kubectl rollout pause deployment/web

# Cambio 1
kubectl set image deployment/web web=web:v3

# Cambio 2
kubectl set env deployment/web ENV=production

# Cambio 3
kubectl patch deployment web -p '{"spec":{"minReadySeconds":30}}'

# Ahora reanudar (todos los cambios se aplican a la vez)
kubectl rollout resume deployment/web

# Monitorear
kubectl rollout status deployment/web
----

**Ejemplo de pausa estratégica:**

[source,bash]
----
# 1. Pausar despliegue
kubectl rollout pause deployment/web

# 2. Actualizar la imagen
kubectl set image deployment/web \
  web=web:v4 \
  --record

# 3. Escalar Pods
kubectl scale deployment/web --replicas=10

# 4. Cambiar recurso limits
kubectl set resources deployment/web \
  --limits=cpu=500m,memory=512Mi \
  --requests=cpu=250m,memory=256Mi

# 5. Reanudar
kubectl rollout resume deployment/web

# Todos los cambios se aplicarán con la estrategia RollingUpdate
----

**Cancelar una actualización pausada:**

[source,bash]
----
# Ver que está pausado
kubectl rollout status deployment/web

# Para cancelar cambios sin aplicar
kubectl rollout undo deployment/web

# Ver que se revirtió a la versión anterior
kubectl rollout status deployment/web
----

**Monitoreo durante despliegues:**

[source,bash]
----
# Ver estado en tiempo real
kubectl rollout status deployment/web -w

# Ver réplicas durante despliegue
kubectl get deployment web --watch

# Ver Pods siendo reemplazados
kubectl get pods -l app=web -w

# Ver eventos
kubectl get events --field-selector involvedObject.name=web

# Ver imagen actual
kubectl get deployment web -o jsonpath='{.spec.template.spec.containers[0].image}'
----

**Ejemplo completo: Canary Deployment**

[source,bash]
----
# 1. Deployment original con 10 réplicas
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: app:v1
EOF

# 2. Pausar despliegue
kubectl rollout pause deployment/app

# 3. Actualizar imagen (crea 1 Pod con v2)
kubectl set image deployment/app app=app:v2

# 4. Dejar corriendo v1 y v2 juntas por monitoreo
kubectl rollout status deployment/app

# 5. Si todo OK, reanudar
kubectl rollout resume deployment/app

# 6. Monitorear actualización completa
kubectl rollout status deployment/app -w
----

**Best Practices para Deployments:**

1. *Usa RollingUpdate por defecto*
   - Minimiza tiempo de inactividad
   - Permite detección de problemas

2. *Configura health checks apropiados*
   - Readiness probe: indica si puede recibir tráfico
   - Liveness probe: indica si está vivo

3. *Define minReadySeconds*
   - Espera suficiente tiempo para estabilización
   - Típicamente 10-30 segundos

4. *Usa record para cambios*
   - `--record` guarda el comando que causó cambio
   - Facilita auditoría

5. *Monitorea despliegues*
   - Usa `rollout status` durante actualizaciones
   - Configura alertas en eventos

6. *Planifica rollbacks*
   - Mantén revisiones útiles
   - Prueba rollbacks en staging

7. *Usa labels y annotations*
   - Versión de la aplicación
   - Git commit SHA
   - Change cause

=== StatefulSets

**¿Qué son los StatefulSets?**

StatefulSets es un controlador de Kubernetes diseñado para gestionar aplicaciones con estado (stateful). A diferencia de los Deployments que son ideales para aplicaciones sin estado (stateless), los StatefulSets proporcionan:

- *Identidad estable de Pods*: Cada Pod tiene un nombre y hostname persistente
- *Almacenamiento persistente*: Cada Pod puede tener su propio volumen dedicado
- *Actualización ordenada*: Los Pods se actualizan de forma ordenada
- *Escalado ordenado*: Los Pods se escalan y se reducen ordenadamente
- *Garantías de orden de inicio*: Los Pods se inician en orden y uno a uno

**Tabla comparativa: Deployments vs StatefulSets**

|===
| Característica | Deployment | StatefulSet

| Identidad Pod
| Efímera (web-abc123)
| Estable (mysql-0, mysql-1, mysql-2)

| Nombre de host
| No persistente
| Persistente (pod-name.service-name.ns)

| Almacenamiento
| Compartido o efímero
| PVC por Pod

| Escalado
| Paralelo, rápido
| Secuencial

| Actualización
| Rápida, paralela
| Lenta, secuencial

| Red
| Balanceada por Service
| Headless Service con DNS de Pod

| Orden de inicio
| Sin garantía
| Garantizado
|===

**Ejemplo: Diferencia en nomenclatura**

Deployment con 3 réplicas:
----
web-xyz789     web-abc123     web-def456
(efímero)      (efímero)      (efímero)
----

StatefulSet con 3 réplicas:
----
mysql-0  →  mysql-0.mysql.default.svc.cluster.local
mysql-1  →  mysql-1.mysql.default.svc.cluster.local
mysql-2  →  mysql-2.mysql.default.svc.cluster.local
(estable)
----

==== Identidad Estable de Pods

Los Pods en un StatefulSet tienen identidades estables y predecibles:

**Nombre y Ordinal**

Cada Pod en un StatefulSet tiene un nombre formado por: `<StatefulSet-name>-<ordinal>`

[source,bash]
----
# StatefulSet: mysql
# Replicas: 3
# Resultado:
mysql-0   (ordinal 0)
mysql-1   (ordinal 1)
mysql-2   (ordinal 2)
----

**Hostname Estable**

El hostname dentro del Pod es el mismo que su nombre:

[source,bash]
----
# En el Pod mysql-0:
hostname
# Salida: mysql-0

# En el Pod mysql-1:
hostname
# Salida: mysql-1
----

**DNS Estable**

Cada Pod tiene un DNS predecible. Con un Headless Service:

[source,bash]
----
# Format: <pod-name>.<service-name>.<namespace>.svc.cluster.local
mysql-0.mysql.default.svc.cluster.local
mysql-1.mysql.default.svc.cluster.local
mysql-2.mysql.default.svc.cluster.local
----

**Casos de uso para identidad estable:**

- *Bases de datos con replicación*: MySQL Replication identifica nodos por hostname
- *Sistemas distribuidos*: Etcd, Zookeeper usan identidades estables
- *Clustering*: Cassandra, Elasticsearch necesitan direcciones predecibles
- *Configuración Master-Slave*: La replicación requiere conocer los miembros

==== Almacenamiento Persistente con VolumeClaimTemplates

StatefulSets usan `volumeClaimTemplates` para crear un PVC por cada Pod:

**Definición básica:**

[source,yaml]
----
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql              # Headless Service (requerido)
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 10Gi
----

**Cómo funciona volumeClaimTemplates:**

[source]
----
StatefulSet con 3 réplicas
        ↓
Crea 3 PVCs:
- mysql-data-0 (10Gi)
- mysql-data-1 (10Gi)
- mysql-data-2 (10Gi)
        ↓
Cada Pod obtiene su PVC dedicado:
- mysql-0 → mysql-data-0
- mysql-1 → mysql-data-1
- mysql-2 → mysql-data-2
----

**Durabilidad y persistencia:**

Si un Pod se elimina, el PVC persiste. Cuando se recrea el Pod, se remontan los datos:

[source,bash]
----
# 1. Eliminar Pod mysql-0
kubectl delete pod mysql-0

# 2. StatefulSet recrea mysql-0
# 3. El nuevo mysql-0 se monta a mysql-data-0 (datos intactos)
# 4. mysql-0 recupera su estado anterior
----

==== Actualización Ordenada

Los StatefulSets actualizan Pods uno a uno, en orden inverso (del más alto ordinal al más bajo):

**Estrategia RollingUpdate:**

[source,yaml]
----
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: app
spec:
  serviceName: app
  replicas: 3
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0  # Actualizar todos los Pods (0 = comenzar desde 0)
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name: app
        image: app:v1
----

**Ejemplo de actualización:**

[source,bash]
----
# Inicial: app-0, app-1, app-2 (todos en v1)

# Actualizar imagen a v2
kubectl set image statefulset/app app=app:v2

# Orden de actualización (inverso):
# 1. app-2 → actualizar y esperar a Ready
# 2. app-1 → actualizar y esperar a Ready
# 3. app-0 → actualizar y esperar a Ready
----

**Actualización Canary con partition:**

`partition` controla cuál ordinal comienza la actualización:

[source,yaml]
----
# updateStrategy con partition: 2
# Solo actualizar app-2, mantener app-0 y app-1 en v1
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    partition: 2
----

[source,bash]
----
# Cambiar imagen a v2
kubectl set image statefulset/app app=app:v2 --record

# Con partition: 2, solo app-2 se actualiza
# Estado:
# app-0: v1 (sin cambios)
# app-1: v1 (sin cambios)
# app-2: v2 (actualizado)

# Monitorear app-2 en producción...

# Cuando esté listo, reducir partition:
kubectl patch statefulset app -p '{"spec":{"updateStrategy":{"rollingUpdate":{"partition":0}}}}'

# Ahora se actualizan app-1 y app-0
----

**Estrategia OnDelete:**

[source,yaml]
----
updateStrategy:
  type: OnDelete
----

Con OnDelete, no se actualiza automáticamente. Se actualiza solo cuando se elimina el Pod manualmente:

[source,bash]
----
# El cambio de imagen no actualiza Pods automáticamente
kubectl set image statefulset/mysql mysql=mysql:8.0

# Debe eliminar manualmente para disparar actualización:
kubectl delete pod mysql-0
# Se recrea con imagen nueva mysql:8.0
----

==== Headless Services

StatefulSets requieren un Headless Service para funcionar correctamente:

**¿Qué es un Headless Service?**

Un Headless Service es un Service sin ClusterIP (ClusterIP: None). En lugar de balancear carga, proporciona DNS para cada Pod:

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  clusterIP: None              # Headless - sin balanceo
  selector:
    app: mysql
  ports:
  - port: 3306
    targetPort: 3306
----

**Resolución DNS en Headless Service:**

[source,bash]
----
# Query: mysql.default.svc.cluster.local
# Respuesta: Todos los Pods
# 10.0.0.1 (mysql-0)
# 10.0.0.2 (mysql-1)
# 10.0.0.3 (mysql-2)

# Query: mysql-0.mysql.default.svc.cluster.local
# Respuesta: Solo mysql-0
# 10.0.0.1
----

**StatefulSet con Headless Service:**

[source,yaml]
----
---
# Headless Service
apiVersion: v1
kind: Service
metadata:
  name: postgresql
spec:
  clusterIP: None
  selector:
    app: postgresql
  ports:
  - name: postgres
    port: 5432
    targetPort: 5432
---
# StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgresql
spec:
  serviceName: postgresql
  replicas: 3
  selector:
    matchLabels:
      app: postgresql
  template:
    metadata:
      labels:
        app: postgresql
    spec:
      containers:
      - name: postgresql
        image: postgresql:13
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 20Gi
----

==== Ejemplo Completo: MySQL con Replicación

**Diagrama de arquitectura:**

[source]
----
┌─────────────────────────────────────────────┐
│     MySQL Replication Cluster               │
├─────────────────────────────────────────────┤
│  mysql-0       mysql-1       mysql-2       │
│  (Master)      (Slave)       (Slave)       │
│    [10G]         [10G]         [10G]       │
│     PVC-0        PVC-1         PVC-2       │
└─────────────────────────────────────────────┘
          ↑           ↑           ↑
   mysql.default.svc.cluster.local (Headless)
----

**Configuración completa:**

[source,bash]
----
kubectl apply -f - <<EOF
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-config
data:
  master.cnf: |
    # Master configuration
    [mysqld]
    binlog_format = ROW
    log-bin = mysql-bin
  slave.cnf: |
    # Slave configuration
    [mysqld]
    relay-log = mysql-relay-bin
    relay-log-index = mysql-relay-bin.index
---
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  clusterIP: None
  selector:
    app: mysql
  ports:
  - name: mysql
    port: 3306
    targetPort: 3306
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - -c
        - |
          set -ex
          # Generar MySQL server ID basado en ordinal
          ordinal=\${HOSTNAME##*-}
          echo "Server ID: \$((100 + ordinal))" > /mnt/conf.d/server_id.cnf

          # Copiar configuración master o slave
          if [[ \$ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
      - name: clone-mysql
        image: xtrabackup:2.4
        command:
        - bash
        - -c
        - |
          set -ex

          # No clonar en el primer Pod
          [[ \$(hostname) == "mysql-0" ]] && exit 0

          # Clonaje del Pod anterior
          while true; do
            mysql-xtrabackup --backup \\
              --host=mysql-\$((ordinal-1)).mysql \\
              --user=root \\
              --password=\${MYSQL_ROOT_PASSWORD} \\
              --stream=xbstream \\
              --dir=/tmp
            if [[ -f "/tmp/xtrabackup_info" ]]; then
              break
            fi
            echo "Esperando Pod anterior..."
            sleep 5
          done
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      volumes:
      - name: config-map
        configMap:
          name: mysql-config
      - name: conf
        emptyDir: {}
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: default
      resources:
        requests:
          storage: 10Gi
EOF
----

**Verificar replicación:**

[source,bash]
----
# Ver estado del master (mysql-0)
kubectl exec mysql-0 -- mysql -uroot -p\${MYSQL_ROOT_PASSWORD} -e \
  "SHOW MASTER STATUS\G"

# Ver estado de slaves
kubectl exec mysql-1 -- mysql -uroot -p\${MYSQL_ROOT_PASSWORD} -e \
  "SHOW SLAVE STATUS\G"

# Probar replicación: insertar en master
kubectl exec mysql-0 -- mysql -uroot -p\${MYSQL_ROOT_PASSWORD} -e \
  "CREATE DATABASE test; CREATE TABLE test.data (id INT PRIMARY KEY AUTO_INCREMENT, val VARCHAR(100));"

# Verificar en slave
kubectl exec mysql-1 -- mysql -uroot -p\${MYSQL_ROOT_PASSWORD} -e \
  "SELECT * FROM test.data;"
----

==== Escalado de StatefulSets

El escalado es ordenado y determinístico:

**Escalado hacia arriba:**

[source,bash]
----
# Aumentar de 3 a 5 réplicas
# Nuevos Pods se crean en orden:
kubectl scale statefulset mysql --replicas=5

# Orden de creación:
# 1. mysql-3 (creado y esperado a Ready)
# 2. mysql-4 (creado y esperado a Ready)
----

**Reducción de escala:**

[source,bash]
----
# Reducir de 5 a 2 réplicas
kubectl scale statefulset mysql --replicas=2

# Orden de eliminación (inverso):
# 1. mysql-4 (eliminado primero)
# 2. mysql-3 (eliminado segundo)
# mysql-0, mysql-1 permanecen

# IMPORTANTE: Los PVCs NO se eliminan automáticamente
# Debe eliminar manualmente si no los necesita:
kubectl delete pvc data-mysql-4 data-mysql-3
----

==== Manejo de Ordenamiento

StatefulSets tiene control fino sobre el ordenamiento mediante `podManagementPolicy`:

**RollingUpdate (defecto):**

[source,yaml]
----
spec:
  podManagementPolicy: Parallel  # Se crean/actualizan en paralelo
  # Sin esperar a que esté Ready
----

**Ordered (secuencial):**

[source,yaml]
----
spec:
  podManagementPolicy: Ordered  # Espera a que esté Ready antes de crear siguiente
----

[source,bash]
----
# Parallel: mysql-0, mysql-1, mysql-2 se crean juntos
# Rápido pero sin garantías de orden

# Ordered: mysql-0 espera a Ready → mysql-1 espera a Ready → mysql-2
# Lento pero garantizado
----

==== Casos de Uso Comunes

**1. Bases de datos con persistencia:**
- MySQL con replicación
- PostgreSQL con clustering
- MongoDB con sharding

**2. Sistemas de mensajería:**
- Kafka con temas y particiones
- RabbitMQ con clustering
- Redis Sentinel

**3. Almacenamiento distribuido:**
- Cassandra
- Elasticsearch
- Ceph

**4. Aplicaciones que requieren identidad estable:**
- Git servers con identidad
- Aplicaciones que cachean por hostname
- Sistemas que necesitan persistencia de estado

==== Best Practices para StatefulSets

1. *Usa Headless Services siempre*
   - Requerido para DNS de Pod individual
   - Evita balanceo incorrecto

2. *Planifica almacenamiento*
   - Usa StorageClasses apropiadas
   - Dimensiona PVCs correctamente
   - Monitorea uso de disco

3. *Ordena según aplicación*
   - Usa Ordered para replicación master-slave
   - Usa Parallel para clústeres distribuidos
   - Ajusta según requisitos de inicialización

4. *Maneja eliminación de Pods*
   - Ten cuidado al escalar hacia abajo
   - Verifica que no haya datos en Pods siendo eliminados
   - Usa `StatefulSet.Spec.VolumeClaimPolicy` para control automático

5. *Monitorea replicación*
   - En bases de datos, verifica lag de replicación
   - Configura alertas para Pods no listos
   - Prueba failover regularmente

6. *Backup y recovery*
   - Implementa backups automáticos
   - Prueba restauración periódicamente
   - Mantén snapshots de volúmenes

7. *Configuración de readiness/liveness*
   - Define probes que reflejen estado real
   - Readiness: ¿listo para tráfico? (replicación sincronizada)
   - Liveness: ¿proceso corriendo? (no mata por lentitud)

8. *Versionado de imagen*
   - Usa etiquetas específicas (mysql:5.7.30, no mysql:5.7)
   - Facilita rollbacks
   - Compatible con partition updates

==== Troubleshooting Común

**Problema: Pod no avanza de Pending**

[source,bash]
----
# Verificar eventos
kubectl describe statefulset mysql
kubectl describe pod mysql-0

# Causas comunes:
# - PVC no se puede crear (storage class no existe)
# - No hay nodos con suficiente espacio
# - Init container falla
----

**Problema: Pod queda en CrashLoopBackOff**

[source,bash]
----
# Ver logs
kubectl logs mysql-0
kubectl logs mysql-0 -c clone-mysql

# Verificar comandos en init containers
kubectl describe pod mysql-0

# Causas:
# - Falla en init container (clonaje no completo)
# - Configuración incorrecta de MySQL
# - Permisos de volumen
----

**Problema: Escalado lento**

[source,bash]
----
# Aumentar paralelismo:
kubectl patch statefulset mysql -p \
  '{"spec":{"podManagementPolicy":"Parallel"}}'

# CUIDADO: Puede romper garantías de orden
# Solo para aplicaciones que lo permiten
----

**Problema: PVCs quedan huérfanos**

[source,bash]
----
# Ver PVCs huérfanos
kubectl get pvc | grep -v "Bound"

# Opción 1: Vincular manualmente a nuevo Pod
kubectl patch pvc data-mysql-0 -p '{...}'

# Opción 2: Eliminar si están rotos
kubectl delete pvc data-mysql-3
----

=== DaemonSets

**¿Qué son los DaemonSets?**

Un DaemonSet es un controlador de Kubernetes que asegura que una copia de un Pod se ejecute en cada nodo del cluster. Diferente a los Deployments y StatefulSets, los DaemonSets no tiene réplicas configurables - siempre hay exactamente un Pod por nodo (salvo excepciones).

**Casos de uso principales:**

- *Recolección de logs*: Fluent, Filebeat, Logstash en cada nodo
- *Monitoreo*: Prometheus Node Exporter, Telegraf en todos los nodos
- *Almacenamiento distribuido*: Ceph, Gluster con agentes en cada nodo
- *Networking*: CNI plugins, kube-proxy ejecutándose en cada nodo
- *Seguridad*: Falco, SELinux agents para auditoría
- *Mantenimiento*: Actualización de paquetes, limpieza de registros
- *Sincronización*: NTP clients, sincronización de estado

**Ejemplo: Diferencia entre Deployment y DaemonSet**

Con Deployment (3 réplicas en cluster de 10 nodos):
```
Nodo1: [Pod]
Nodo2: [Pod]
Nodo3: [Pod]
Nodo4: -
Nodo5: -
...
Nodo10: -
```

Con DaemonSet (mismo cluster):
```
Nodo1: [Pod]
Nodo2: [Pod]
Nodo3: [Pod]
Nodo4: [Pod]
Nodo5: [Pod]
...
Nodo10: [Pod]
```

==== Definición Básica de DaemonSets

**YAML básico:**

[source,yaml]
----
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      containers:
      - name: node-exporter
        image: prom/node-exporter:latest
        ports:
        - containerPort: 9100
----

**Comparación: Deployment vs DaemonSet vs StatefulSet**

|===
| Aspecto | Deployment | DaemonSet | StatefulSet

| Réplicas
| Configurable
| 1 por nodo
| Configurable

| Pods por nodo
| Varía (0 o más)
| Exactamente 1
| Varía según réplicas

| Identidad
| Efímera
| Efímera
| Estable

| Caso de uso
| Apps stateless
| Agentes en nodos
| Apps stateful

| Ordenamiento
| Sin garantía
| Por nodo
| Secuencial
|===

==== Selección de Nodos en DaemonSets

Los DaemonSets pueden ejecutarse en subconjuntos de nodos mediante selección:

**nodeSelector:**

[source,yaml]
----
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: gpu-monitor
spec:
  selector:
    matchLabels:
      app: gpu-monitor
  template:
    metadata:
      labels:
        app: gpu-monitor
    spec:
      nodeSelector:
        nvidia.com/gpu: "true"  # Solo en nodos con GPUs
      containers:
      - name: monitor
        image: gpu-monitor:latest
----

**Verificar labels en nodos:**

[source,bash]
----
# Ver todos los labels de nodos
kubectl get nodes --show-labels

# Etiquetar un nodo
kubectl label nodes node-1 nvidia.com/gpu=true

# Verificar que DaemonSet se crea en nodo específico
kubectl get pods -o wide
----

**Affinidad de nodos (más avanzado):**

[source,yaml]
----
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: storage-agent
spec:
  selector:
    matchLabels:
      app: storage-agent
  template:
    metadata:
      labels:
        app: storage-agent
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: storage-type
                operator: In
                values:
                - fast-ssd
                - fast-nvme
      containers:
      - name: agent
        image: storage-agent:latest
----

**Tolerations (tolerar taints de nodos):**

[source,yaml]
----
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: system-monitor
spec:
  selector:
    matchLabels:
      app: system-monitor
  template:
    metadata:
      labels:
        app: system-monitor
    spec:
      tolerations:
      # Tolerar nodos master (normalmente tienen taint NoSchedule)
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      containers:
      - name: monitor
        image: system-monitor:latest
----

==== Actualización de DaemonSets

Los DaemonSets soportan dos estrategias de actualización:

**RollingUpdate (por defecto):**

[source,yaml]
----
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: filebeat
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1  # Máximo 1 Pod no disponible durante actualización
  selector:
    matchLabels:
      app: filebeat
  template:
    metadata:
      labels:
        app: filebeat
    spec:
      containers:
      - name: filebeat
        image: docker.elastic.co/beats/filebeat:7.10.0
----

**OnDelete:**

[source,yaml]
----
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: critical-agent
spec:
  updateStrategy:
    type: OnDelete  # Solo actualiza cuando Pod se elimina manualmente
  selector:
    matchLabels:
      app: critical-agent
  template:
    metadata:
      labels:
        app: critical-agent
    spec:
      containers:
      - name: agent
        image: critical-agent:latest
----

**Ejemplo de actualización:**

[source,bash]
----
# Cambiar imagen (con RollingUpdate)
kubectl set image daemonset/filebeat filebeat=docker.elastic.co/beats/filebeat:7.11.0

# Ver progreso de actualización
kubectl rollout status daemonset/filebeat

# Historial de rollout
kubectl rollout history daemonset/filebeat

# Rollback a versión anterior
kubectl rollout undo daemonset/filebeat

# Ver pods siendo actualizados
kubectl get pods -l app=filebeat --sort-by=.metadata.creationTimestamp
----

==== Ejemplo: Node Exporter para Monitoreo

**Setup completo de Prometheus Node Exporter:**

[source,yaml]
----
---
apiVersion: v1
kind: Service
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  type: ClusterIP
  ports:
  - port: 9100
    targetPort: 9100
    name: http
  selector:
    app: node-exporter
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      - effect: NoSchedule
        key: node-role.kubernetes.io/control-plane
      hostNetwork: true           # Usa red del host
      hostPID: true               # Acceso a procesos del host
      hostIPC: true               # Acceso a IPC del host
      containers:
      - name: node-exporter
        image: prom/node-exporter:v1.2.2
        args:
          - "--path.procfs=/host/proc"
          - "--path.sysfs=/host/sys"
          - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($|/)"
        ports:
        - containerPort: 9100
          name: http
        resources:
          limits:
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 128Mi
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - name: rootfs
          mountPath: /rootfs
          readOnly: true
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: rootfs
        hostPath:
          path: /
----

**Verificar node exporter en todos los nodos:**

[source,bash]
----
# Ver DaemonSet status
kubectl describe daemonset -n monitoring node-exporter

# Ver pods en todos los nodos
kubectl get pods -n monitoring -o wide -l app=node-exporter

# Conectar a metrics de un nodo específico
kubectl port-forward -n monitoring daemonset/node-exporter 9100:9100

# En otra terminal:
curl http://localhost:9100/metrics | grep node_cpu_seconds_total
----

==== Ejemplo: Recolección de Logs con Fluent

**Fluent bit como DaemonSet:**

[source,yaml]
----
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: logging
data:
  fluent-bit.conf: |
    [SERVICE]
        Daemon Off
        Flush 5
        Log_Level info

    [INPUT]
        Name tail
        Tag docker.*
        Path /var/log/containers/*.log
        Parser docker
        Mem_Buf_Limit 5MB
        Skip_Long_Lines On

    [FILTER]
        Name kubernetes
        Match docker.*
        Kube_URL https://kubernetes.default.svc:443
        Kube_CA_File /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File /var/run/secrets/kubernetes.io/serviceaccount/token
        Keep_Log On

    [OUTPUT]
        Name stackdriver
        Match *
        google_service_credentials /var/secrets/google/key.json
        resource k8s_container
        k8s_cluster_name my-cluster
        k8s_cluster_location us-central1
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: logging
spec:
  selector:
    matchLabels:
      app: fluent-bit
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: fluent-bit
    spec:
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        tolerationSeconds: 5
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        tolerationSeconds: 5
      serviceAccountName: fluent-bit
      containers:
      - name: fluent-bit
        image: fluent/fluent-bit:1.8.0
        volumeMounts:
        - name: config
          mountPath: /fluent-bit/etc/
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 256Mi
      volumes:
      - name: config
        configMap:
          name: fluent-bit-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluent-bit
  namespace: logging
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluent-bit
rules:
- apiGroups: [""]
  resources:
  - namespaces
  - pods
  - pods/logs
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluent-bit
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fluent-bit
subjects:
- kind: ServiceAccount
  name: fluent-bit
  namespace: logging
----

==== Ciclo de Vida y Comportamiento

**Creación de DaemonSet:**

[source]
----
1. Se crea DaemonSet en API
2. Controlador itera sobre todos los nodos
3. Para cada nodo (sin exclusiones):
   - Crea un Pod con el template
4. Pod se asigna directamente al nodo
----

**Cuando se agrega un nodo nuevo:**

[source,bash]
----
# 1. Nuevo nodo se une al cluster
kubectl get nodes
# NAME      STATUS   ROLES   ...
# node-1    Ready    <none>
# node-2    Ready    <none>
# node-3    Ready    <none>  (nuevo)

# 2. DaemonSet detecta automáticamente
# 3. Crea un Pod en node-3 en segundos

kubectl get pods -o wide
# NAME                    READY   STATUS    NODE
# node-exporter-abc12     1/1     Running   node-1
# node-exporter-def45     1/1     Running   node-2
# node-exporter-ghi78     1/1     Running   node-3 (nuevo)
----

**Cuando se remueve un nodo:**

[source,bash]
----
# 1. Drenar nodo (graceful)
kubectl drain node-3 --ignore-daemonsets

# 2. DaemonSet elimina Pod del nodo
# 3. Nodo se marca como unschedulable

# 4. Cuando se vuelve a agregar
kubectl uncordon node-3

# 5. DaemonSet detecta y recrea Pod automáticamente
----

==== Acceso a Host desde DaemonSets

DaemonSets puede acceder a recursos del nodo host:

**hostNetwork:**

[source,yaml]
----
spec:
  template:
    spec:
      hostNetwork: true  # Usa el namespace de red del host
      containers:
      - name: app
        image: app:latest
----

[source,bash]
----
# Con hostNetwork: true
# Pod accede a puertos en 0.0.0.0 del host
# Puede ver todos los procesos de red del nodo
netstat -tlnp
----

**hostPID:**

[source,yaml]
----
spec:
  template:
    spec:
      hostPID: true  # Acceso a PID del host
      containers:
      - name: monitor
        image: monitor:latest
----

[source,bash]
----
# Con hostPID: true
# Puede ver todos los procesos del nodo
ps aux | grep -v container
----

**hostIPC:**

[source,yaml]
----
spec:
  template:
    spec:
      hostIPC: true  # Acceso a IPC del host
      containers:
      - name: agent
        image: agent:latest
----

**Volúmenes hostPath:**

[source,yaml]
----
spec:
  template:
    spec:
      containers:
      - name: app
        volumeMounts:
        - name: sys
          mountPath: /host/sys
      volumes:
      - name: sys
        hostPath:
          path: /sys
          type: Directory
----

==== Best Practices para DaemonSets

1. *Usa tolerations para nodos especiales*
   - Master/control-plane siempre tienen taints
   - Configura tolerations explícitamente
   - Documenta por qué necesitas ejecutar en cada nodo

2. *Define recursos adecuadamente*
   - DaemonSets ejecutan en cada nodo
   - El overhead se multiplica por número de nodos
   - Establece requests y limits apropiados

3. *Implementa health checks*
   - Readiness: ¿listo para recibir tráfico?
   - Liveness: ¿el proceso está vivo?
   - Importante para agentes críticos

4. *Actualiza cuidadosamente*
   - maxUnavailable controla velocidad
   - Comienza con maxUnavailable: 1
   - Monitorea impacto antes de aumentar

5. *Documenta el caso de uso*
   - ¿Por qué se necesita en cada nodo?
   - ¿Qué datos accede del host?
   - ¿Qué permisos necesita?

6. *Considera performance*
   - No crees DaemonSets innecesarios
   - Usa nodeSelector para limitar si es posible
   - Monitorea uso de recursos

7. *Seguridad en hostPath*
   - Limita acceso de volúmenes
   - Usa readOnly cuando sea posible
   - Ten cuidado con volúmenes de root

8. *Planifica tamaño de cluster*
   - Recurso: N pods × (requests de cada pod)
   - Ejemplo: 100 nodos × 256Mi = 25Gi de memoria
   - Escala según necesidad de nodos

==== Troubleshooting Comunes

**Problema: Pod no se crea en algunos nodos**

[source,bash]
----
# Verificar taints en nodos
kubectl describe node node-1 | grep Taints

# Si hay taints, agregar tolerations en DaemonSet
kubectl patch daemonset my-daemon --type merge -p \
  '{"spec":{"template":{"spec":{"tolerations":[{"key":"mykey","operator":"Equal","value":"myvalue","effect":"NoSchedule"}]}}}}'
----

**Problema: DaemonSet actualiza muy lentamente**

[source,bash]
----
# Verificar maxUnavailable
kubectl get daemonset my-daemon -o jsonpath='{.spec.updateStrategy}'

# Aumentar maxUnavailable para actualizar más rápido
# (CUIDADO: puede afectar servicio)
kubectl patch daemonset my-daemon -p \
  '{"spec":{"updateStrategy":{"rollingUpdate":{"maxUnavailable":2}}}}'
----

**Problema: Pod crashea en todos los nodos**

[source,bash]
----
# Ver logs del DaemonSet
kubectl logs -l app=my-daemon --tail=50 -f

# Ver eventos de los pods
kubectl describe pod -l app=my-daemon

# Rollback a versión anterior
kubectl rollout undo daemonset/my-daemon
----

**Problema: DaemonSet usa demasiados recursos**

[source,bash]
----
# Limitar a nodos específicos
kubectl patch daemonset my-daemon -p \
  '{"spec":{"template":{"spec":{"nodeSelector":{"role":"compute"}}}}}'

# Etiquetar nodos apropiados primero
kubectl label node node-1 role=compute
----

=== Jobs y CronJobs

**¿Qué son los Jobs?**

Un Job en Kubernetes es un controlador que crea Pods para ejecutar tareas batch (no continuas). A diferencia de otros controladores:

- *Objetivo de finalización*: El Job se completa cuando sus Pods terminan exitosamente
- *No reinicia indefinidamente*: Los Pods se ejecutan una vez y terminan
- *Garantiza completitud*: Reinicia Pods si fallan (hasta completarse)
- *Paralelismo configurable*: Puede ejecutar múltiples Pods en paralelo

**Casos de uso principales:**

- *Procesamiento batch*: Procesar archivos CSV, logs, datos
- *Cálculos complejos*: Rendering, análisis científicos
- *Exportación/Import de datos*: Backup, restore de bases de datos
- *Trabajos programados*: Jobs disparados por eventos externos
- *Limpiezas periódicas*: Compresión, archivado de datos
- *Notificaciones*: Envío de emails, alerts

==== Diferencia: Deployment vs Job

|===
| Característica | Deployment | Job

| Propósito
| Servir tráfico continuo
| Ejecutar tarea hasta completarse

| Pods
| Reinician indefinidamente
| Terminan cuando la tarea finaliza

| Réplicas
| Mantiene # configurable
| Crea # según paralelismo

| Éxito
| Pod corriendo
| Completions alcanzadas

| Típica duración
| Horas/días
| Minutos/horas
|===

==== Definición Básica de Jobs

**YAML básico:**

[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: process-files
spec:
  completions: 1        # Número de Pods que deben completarse
  parallelism: 1        # Número de Pods en paralelo
  backoffLimit: 3       # Reintentos antes de fallar
  template:
    spec:
      containers:
      - name: process
        image: python:3.9
        command: ["python", "process.py"]
      restartPolicy: Never  # Never o OnFailure (no Always en Jobs)
----

**Estados de un Job:**

[source,bash]
----
# Estados posibles:
# 1. Active: Ejecutándose
# 2. Succeeded: Completado exitosamente
# 3. Failed: Falló definitivamente

# Ver estado
kubectl describe job process-files

# Ver pods del job
kubectl get pods -l job-name=process-files
----

==== Paralelismo y Completions

**Diferentes patrones de ejecución:**

**Patrón 1: Tarea simple (1 Pod que completa)**

[source,yaml]
----
spec:
  completions: 1
  parallelism: 1
  # Se crea 1 Pod
  # Cuando termina exitosamente: Job completado
----

**Patrón 2: Completar múltiples veces (1 Pod a la vez)**

[source,yaml]
----
spec:
  completions: 5       # Necesita 5 terminaciones exitosas
  parallelism: 1       # Pero solo 1 a la vez
  # Crea Pod 1 → espera a completarse → crea Pod 2 → ...
  # Orden secuencial, toma 5× el tiempo
----

**Patrón 3: Trabajo paralelo (múltiples Pods juntos)**

[source,yaml]
----
spec:
  completions: 10
  parallelism: 5       # 5 Pods en paralelo
  # Crea 5 Pods → esperan a completar → crea siguiente batch
  # Reducción: 2 batches en lugar de 10 secuenciales
----

**Patrón 4: Trabajo con work queue (sin completions)**

[source,yaml]
----
spec:
  completions: null    # Puede omitirse
  parallelism: 3
  # Crea 3 Pods
  # Cuando 1 termina exitosamente: Job completado
  # Otros Pods se eliminan
  # Patrón: trabajo disponible en queue, toma 1 Pod
----

**Ejemplo práctico: Procesar 100 archivos en paralelo**

[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: process-images
spec:
  completions: 100       # Necesita procesar 100 imágenes
  parallelism: 10        # 10 workers en paralelo
  backoffLimit: 2        # Reintentar 2 veces si falla
  template:
    spec:
      containers:
      - name: worker
        image: image-processor:latest
        env:
        - name: TOTAL_JOBS
          value: "100"
        - name: PARALLELISM
          value: "10"
      restartPolicy: OnFailure
----

**Lógica en el contenedor:**

[source,bash]
----
#!/bin/bash
# El contenedor de worker debe:
# 1. Determinar qué porción procesar
# 2. Procesar la porción asignada
# 3. Reportar éxito/fallo

# Opción: cada Pod procesa 1 archivo
# Opción: usar Job index (BATCH_JOB_SEQUENCE_NUM)

# Job índices (feature alpha/beta):
export JOB_INDEX=$((RANDOM % PARALLELISM))
echo "Processing file $JOB_INDEX of $TOTAL_JOBS"
process_file.py --index=$JOB_INDEX
----

==== Configuración de Reintentos

**backoffLimit: Número máximo de reintentos**

[source,yaml]
----
spec:
  backoffLimit: 3
  # Primera ejecución
  # Fallo → reintento 1
  # Fallo → reintento 2
  # Fallo → reintento 3
  # Fallo → Job fallido
----

**activeDeadlineSeconds: Timeout total del Job**

[source,yaml]
----
spec:
  activeDeadlineSeconds: 3600  # 1 hora máximo
  # Si el Job no completa en 1 hora: fallo
  # Incluso si hay reintentos disponibles
----

**Ejemplo: Reintentos con backoff**

[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: unreliable-task
spec:
  backoffLimit: 4
  activeDeadlineSeconds: 600  # 10 minutos máximo
  template:
    spec:
      containers:
      - name: task
        image: unreliable-service:latest
        # El contenedor falla aleatoriamente
      restartPolicy: OnFailure
      # OnFailure: reintenta en el mismo Pod
      # Never: crea nuevo Pod para cada intento
----

==== Casos de Uso: Jobs Complejos

**Job: Procesamiento ETL (Extract, Transform, Load)**

[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: etl-pipeline
spec:
  completions: 5      # 5 tablas a procesar
  parallelism: 2      # 2 tablas en paralelo
  backoffLimit: 3
  template:
    metadata:
      labels:
        job: etl
    spec:
      containers:
      - name: etl-worker
        image: etl-worker:1.0
        env:
        - name: DATABASE_HOST
          valueFrom:
            configMapKeyRef:
              name: db-config
              key: host
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: password
        volumeMounts:
        - name: data
          mountPath: /data
      volumes:
      - name: data
        emptyDir: {}
      restartPolicy: OnFailure
      # Asegurar suficiente tiempo
      terminationGracePeriodSeconds: 30
EOF

# Monitorear progreso
kubectl describe job etl-pipeline

# Ver pods en paralelo
kubectl get pods -l job-name=etl-pipeline --watch
----

**Job: Análisis de Datos Distribuido**

[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: data-analysis
spec:
  parallelism: 4
  completions: 4
  template:
    spec:
      containers:
      - name: analyzer
        image: data-analyzer:latest
        args:
          - "--input=/data"
          - "--output=/results"
        resources:
          requests:
            cpu: 2
            memory: 4Gi
          limits:
            cpu: 4
            memory: 8Gi
        volumeMounts:
        - name: input-data
          mountPath: /data
          readOnly: true
        - name: results
          mountPath: /results
      volumes:
      - name: input-data
        persistentVolumeClaim:
          claimName: analysis-data
      - name: results
        persistentVolumeClaim:
          claimName: analysis-results
      restartPolicy: Never
----

==== Limpieza y Gestión de Jobs

**Política de limpieza automática:**

[source,yaml]
----
spec:
  ttlSecondsAfterFinished: 3600
  # Job completado hace más de 1 hora se elimina automáticamente
  # Útil para ahorrar espacio en etcd
----

**Comandos de gestión:**

[source,bash]
----
# Ver Jobs completados
kubectl get jobs

# Ver Job en detalle
kubectl describe job process-files

# Ver logs de todos los Pods del Job
kubectl logs -l job-name=process-files --all-containers=true

# Eliminar Job (mantiene Pods)
kubectl delete job process-files

# Eliminar Job y sus Pods
kubectl delete job process-files --cascade=foreground

# Ver Jobs en namespace específico
kubectl get jobs -n production
----

==== CronJobs: Tareas Programadas

**¿Qué es un CronJob?**

Un CronJob es un controlador que crea Jobs automáticamente en horarios específicos. Funciona como `cron` en sistemas Unix.

**Sintaxis de CronJob (cron expression):**

[source,bash]
----
# Formato: minuto hora día_mes mes día_semana
# *    *    *    *    *
# |    |    |    |    |
# |    |    |    |    +--- Día de la semana (0-6, 0=domingo)
# |    |    |    +------- Mes (1-12)
# |    |    +----------- Día del mes (1-31)
# |    +--------------- Hora (0-23)
# +------------------- Minuto (0-59)

Ejemplos:
0 2 * * *       # Cada día a las 2:00 AM
0 */4 * * *     # Cada 4 horas
0 9 * * 1-5     # Lunes a viernes a las 9:00 AM
0 0 1 * *       # Primer día del mes a medianoche
*/15 * * * *    # Cada 15 minutos
----

**YAML básico:**

[source,yaml]
----
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-daily
spec:
  schedule: "0 2 * * *"      # 2:00 AM cada día
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: backup-tool:latest
            command: ["backup.sh"]
          restartPolicy: OnFailure
----

**CronJob con timezone (beta feature):**

[source,yaml]
----
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-daily
spec:
  timezone: "America/New_York"
  schedule: "0 2 * * *"
  # 2:00 AM en New York, no UTC
  jobTemplate:
    # ...
----

==== Ejemplos Prácticos de CronJobs

**CronJob 1: Backup diario de base de datos**

[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mysql-backup
spec:
  schedule: "0 2 * * *"        # 2:00 AM cada día
  successfulJobsHistoryLimit: 3  # Mantener últimos 3 backups
  failedJobsHistoryLimit: 1      # Mantener último fallo
  jobTemplate:
    spec:
      backoffLimit: 2
      template:
        spec:
          serviceAccountName: backup-service
          containers:
          - name: backup
            image: mysql:8.0
            env:
            - name: MYSQL_HOST
              value: mysql-primary.database.svc.cluster.local
            - name: MYSQL_USER
              value: backup
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysql-backup-credentials
                  key: password
            - name: BACKUP_PATH
              value: /backups
            command:
            - /bin/bash
            - -c
            - |
              set -e
              TIMESTAMP=\$(date +%Y%m%d_%H%M%S)
              BACKUP_FILE=\$BACKUP_PATH/mysql_\$TIMESTAMP.sql

              echo "Starting backup at \$(date)"
              mysqldump -h \$MYSQL_HOST -u \$MYSQL_USER -p\$MYSQL_PASSWORD \\
                --all-databases --single-transaction > \$BACKUP_FILE

              echo "Backup completed: \$BACKUP_FILE"
              echo "Size: \$(du -h \$BACKUP_FILE)"

              # Mantener últimos 7 días
              find \$BACKUP_PATH -name "mysql_*.sql" -mtime +7 -delete
            volumeMounts:
            - name: backups
              mountPath: /backups
            resources:
              requests:
                cpu: 500m
                memory: 512Mi
              limits:
                cpu: 1
                memory: 1Gi
          volumes:
          - name: backups
            persistentVolumeClaim:
              claimName: backup-storage
          restartPolicy: OnFailure
EOF

# Ver CronJobs
kubectl get cronjobs

# Ver próxima ejecución
kubectl describe cronjob mysql-backup

# Ver histórico de Jobs creados
kubectl get jobs -l cronjob=mysql-backup
----

**CronJob 2: Limpieza de logs y caché**

[source,yaml]
----
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cleanup-job
spec:
  schedule: "0 3 * * 0"        # 3:00 AM todos los domingos
  concurrencyPolicy: Forbid    # No ejecutar si hay una corriendo
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cleanup
            image: busybox:latest
            command:
            - /bin/sh
            - -c
            - |
              echo "Starting cleanup at $(date)"

              # Limpiar logs viejos
              find /var/log -name "*.log" -mtime +30 -delete

              # Comprimir logs de la semana pasada
              find /var/log -name "*.log" -mtime +7 -mtime -30 -exec gzip {} \;

              # Estadísticas
              du -sh /var/log

              echo "Cleanup completed"
            volumeMounts:
            - name: logs
              mountPath: /var/log
          volumes:
          - name: logs
            hostPath:
              path: /var/log
          restartPolicy: OnFailure
----

**CronJob 3: Sincronización de datos cada hora**

[source,yaml]
----
apiVersion: batch/v1
kind: CronJob
metadata:
  name: data-sync
spec:
  schedule: "0 * * * *"         # Cada hora
  concurrencyPolicy: Replace    # Si no terminó, crear nueva ejecución
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      activeDeadlineSeconds: 1800  # 30 minutos máximo
      backoffLimit: 2
      template:
        spec:
          containers:
          - name: sync
            image: data-sync-service:latest
            env:
            - name: SOURCE_DB
              valueFrom:
                configMapKeyRef:
                  name: sync-config
                  key: source_db
            - name: TARGET_DB
              valueFrom:
                configMapKeyRef:
                  name: sync-config
                  key: target_db
            - name: API_KEY
              valueFrom:
                secretKeyRef:
                  name: sync-credentials
                  key: api_key
          restartPolicy: OnFailure
----

==== concurrencyPolicy: Control de Ejecuciones Simultáneas

CronJob tiene 3 políticas de concurrencia:

**Allow (por defecto):**

[source,yaml]
----
spec:
  concurrencyPolicy: Allow
  # Permite múltiples Jobs ejecutando simultáneamente
  # Si el cron ejecuta antes de que termine el anterior: 2 Jobs
----

**Forbid:**

[source,yaml]
----
spec:
  concurrencyPolicy: Forbid
  schedule: "0 * * * *"
  # Si hay Job ejecutando: NO crea uno nuevo
  # Espera a siguiente ventana de tiempo
  # Útil para backup: no múltiples backups juntos
----

**Replace:**

[source,yaml]
----
spec:
  concurrencyPolicy: Replace
  # Si hay Job ejecutando: cancela y crea uno nuevo
  # Útil para sincronización: que sea frecuente y reciente
----

==== Planificación de CronJobs

**Consideraciones al agendar:**

[source]
----
1. Ventana de tiempo:
   ¿Cuánto tarda la tarea normalmente?
   ¿Cuánto es el máximo esperado?
   Agendarla con suficiente margen

2. Distribución de carga:
   No agendar todas a la misma hora
   Ejemplo: 0 2 * * * (backup), 0 3 * * * (sync)
   Evita picos de carga

3. Timezone:
   UTC por defecto
   Usar timezone en v1.25+ para horarios locales

4. Retry:
   backoffLimit: reintentos si falla
   activeDeadlineSeconds: timeout total

5. Historico:
   successfulJobsHistoryLimit: cuantos successful mantener
   failedJobsHistoryLimit: cuantos failed mantener
   Importante para auditoría y debugging
----

==== Best Practices para Jobs y CronJobs

1. *Define restartPolicy apropiada*
   - Never: Nuevo Pod para cada intento
   - OnFailure: Reintenta en el mismo Pod
   - Never es más seguro para tareas stateful

2. *Configura resourceRequest adecuadamente*
   - Jobs son predecibles, dimensiona bien
   - Evita Out-of-Memory kills
   - Monitorea uso real vs estimado

3. *Implementa mecanismo de bloqueo*
   - CronJob puede dispararse múltiples veces
   - Usa locks en aplicación (base de datos, archivos)
   - O usa concurrencyPolicy: Forbid

4. *Usa ttlSecondsAfterFinished*
   - Limpia Jobs completados automáticamente
   - Ahorra espacio en etcd
   - Típicamente: 7 días = 604800 segundos

5. *Monitorea ejecución*
   - Configura alertas para Jobs fallidos
   - Verifica que CronJobs se ejecuten en horarios esperados
   - Revisa logs regularmente

6. *Documenta la propósito del Job*
   - Qué tarea realiza
   - Cuánto tiempo toma
   - Qué hacer si falla

7. *Usa ConfigMaps y Secrets*
   - Configuración en ConfigMaps
   - Credenciales en Secrets
   - Facilita actualización sin cambiar imagen

8. *Prueba jobs manualmente primero*
   - Crea Job (no CronJob) para probar
   - Valida en staging
   - Luego automatiza con CronJob

==== Troubleshooting Común

**Problema: CronJob no se ejecuta**

[source,bash]
----
# Verificar programación
kubectl get cronjob cleanup
kubectl describe cronjob cleanup

# Ver eventos
kubectl get events | grep cleanup

# Causas comunes:
# - Sintaxis de schedule incorrecta
# - ServiceAccount sin permisos
# - Cluster sin suficientes recursos
----

**Problema: Job se ejecuta múltiples veces**

[source,bash]
----
# Ver Jobs del CronJob
kubectl get jobs -l cronjob=my-cronjob

# Si hay múltiples simultáneos:
# Cambiar concurrencyPolicy a Forbid o Replace

kubectl patch cronjob my-cronjob -p '{"spec":{"concurrencyPolicy":"Forbid"}}'
----

**Problema: Job tarda mucho o timeout**

[source,bash]
----
# Ver logs del Pod
kubectl logs -l job-name=my-job

# Verificar resourceRequest vs uso real
kubectl top pod -l job-name=my-job

# Aumentar activeDeadlineSeconds
kubectl patch job my-job -p '{"spec":{"activeDeadlineSeconds":7200}}'
----

**Problema: Job entra en backoff loop**

[source,bash]
----
# Ver razón de fallo
kubectl describe pod -l job-name=my-job | grep "Last State"

# Ver logs completos
kubectl logs -l job-name=my-job --previous

# Aumentar backoffLimit si es transitorio
# O reducir si hay error permanente
----

== Módulo 4: Servicios y Redes

=== Networking en Kubernetes

**Modelo de red de Kubernetes**

Kubernetes implementa un modelo de red plano y agnóstico. Las características fundamentales son:

**Principios del modelo de red:**

1. *Todos los Pods pueden comunicarse entre sí*
   - Sin NAT (Network Address Translation)
   - Dentro del mismo cluster
   - Independientemente del nodo donde corran

2. *Todos los nodos pueden comunicarse con todos los Pods*
   - Sin NAT requerido
   - Comunicación directa

3. *La dirección IP de un Pod es su propia dirección*
   - No hay aliasing
   - Transparencia en la comunicación

**Diagrama del modelo de red:**

[source]
----
┌─────────────────────────────────────────────────┐
│         Cluster Kubernetes                       │
├─────────────────────────────────────────────────┤
│  ┌──────────────────┐      ┌──────────────────┐ │
│  │  Nodo 1          │      │  Nodo 2          │ │
│  ├──────────────────┤      ├──────────────────┤ │
│  │ Pod A (10.0.0.1) │      │ Pod C (10.0.0.3) │ │
│  │ Pod B (10.0.0.2) │      │ Pod D (10.0.0.4) │ │
│  │                  │      │                  │ │
│  │ eth0: 192.168.1.1│      │ eth0: 192.168.1.2│ │
│  └──────────────────┘      └──────────────────┘ │
│         │                          │             │
│         └──────────────────────────┘             │
│     (Red overlay o L3: 10.0.0.0/24)             │
│     (Red nodos: 192.168.1.0/24)                 │
└─────────────────────────────────────────────────┘

Pod A → Pod C (sin NAT):
10.0.0.1 → 10.0.0.3 ✓
(no se reescribe IP de origen)
----

**Espacios de direcciones:**

- *Pod CIDR*: Red de Pods (típicamente 10.0.0.0/8 o 10.0.0.0/16)
- *Service CIDR*: Red de Services (típicamente 10.96.0.0/12)
- *Node CIDR*: Red de nodos (depende de cloud provider)

==== CNI: Container Network Interface

**¿Qué es CNI?**

CNI es un estándar que define cómo deben conectarse los contenedores en Kubernetes. Kubernetes delega el networking a plugins CNI.

**Responsabilidades del plugin CNI:**

1. *Crear interfaz de red en el contenedor*
   - Asignar dirección IP del Pod
   - Configurar rutas

2. *Conectar contenedor a la red del cluster*
   - Red overlay o L3 routing

3. *Mantener conectividad entre Pods*

4. *Implementar Network Policies (algunos plugins)*

**Plugins CNI populares:**

|===
| Plugin | Tipo | Características | Casos de uso

| Flannel
| Overlay
| Simple, ligero
| Desarrollo, clusters pequeños

| Calico
| L3 Routing
| Alto rendimiento, network policies
| Producción, clusters grandes

| Weave
| Overlay
| Balanceo automático
| Networking flexible

| Cilium
| eBPF
| Alto rendimiento, seguridad avanzada
| Producción crítica

| AWS VPC CNI
| Nativo AWS
| Integración AWS
| Clusters EKS

| Kubenet
| (legacy)
| Simple, limitado
| Solo GCP GKE

| OVN
| Open vSwitch
| L2/L3, avanzado
| Red empresarial
|===

**Instalación de CNI:**

[source,bash]
----
# Verificar CNI instalado
kubectl get nodes
# STATUS debe ser "Ready" (indica CNI funciona)

# Buscar pods de CNI
kubectl get pods -n kube-system -o wide

# Ejemplo: Flannel
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

# Ejemplo: Calico
kubectl apply -f https://docs.projectcalico.org/manifests/tigera-operator.yaml

# Verificar
kubectl get daemonset -n kube-system
kubectl get pods -n calico-system
----

==== Comunicación Pod-to-Pod

**Dentro del mismo nodo:**

[source]
----
┌─────────────────────────────┐
│  Nodo 1                     │
├─────────────────────────────┤
│  ┌──────────┐  ┌──────────┐ │
│  │ Pod A    │  │ Pod B    │ │
│  │10.0.0.1  │  │10.0.0.2  │ │
│  └──────────┘  └──────────┘ │
│       │              │       │
│       └──────┬───────┘       │
│              │               │
│        virtual bridge       │
│        (docker0, cni0)      │
│              │               │
│         eth0 │               │
│       192.168.1.1            │
└─────────────────────────────┘

Comunicación directa a través del bridge virtual
----

**Entre diferentes nodos:**

[source]
----
┌──────────────────────┐       ┌──────────────────────┐
│  Nodo 1              │       │  Nodo 2              │
├──────────────────────┤       ├──────────────────────┤
│  Pod A (10.0.0.1)    │       │  Pod C (10.0.0.3)    │
└──────────────────────┘       └──────────────────────┘
         │                              ▲
      10.0.0.1                       10.0.0.3
         │                              │
         └──────────────────────────────┘
              (overlay network)
         o
      (encapsulation si es necesario)

Dependiendo del plugin CNI:
- Flannel: VXLAN encapsulation
- Calico: BGP routing (sin encapsulation)
- Weave: Encryption optional
----

**Ejemplo práctico: Comunicación entre Pods**

[source,bash]
----
# Terminal 1: crear Pod servidor
kubectl run server --image=nginx:latest --port=80

# Esperar a que esté listo
kubectl wait --for=condition=ready pod/server --timeout=300s

# Terminal 2: crear Pod cliente
kubectl run -it client --image=busybox --rm --restart=Never -- sh

# En el cliente:
# Obtener IP del servidor
SERVER_IP=$(kubectl get pod server -o jsonpath='{.status.podIP}')
echo $SERVER_IP

# Probar conectividad
wget http://$SERVER_IP
# Debería funcionar sin ninguna configuración especial
----

==== DNS en Kubernetes

**Servicio DNS en el cluster:**

El DNS en Kubernetes es proporcionado por CoreDNS (o kube-dns en versiones antiguas).

**Pods DNS:**

[source,bash]
----
# Ver DNS en el cluster
kubectl get pods -n kube-system | grep dns

# CoreDNS típicamente corre en kube-system
# kubectl get svc -n kube-system
# kube-dns (10.96.0.10) es el DNS del cluster
----

**Resolución de nombres:**

[source]
----
Formato: <pod-name>.<namespace>.pod.cluster.local

Ejemplos:
- mypod.default.pod.cluster.local → 10.0.0.5
- mysql-0.default.pod.cluster.local → 10.0.0.10

Service DNS:
- <service-name>.<namespace>.svc.cluster.local
- web.production.svc.cluster.local → 10.96.1.5

Corto (mismo namespace):
- web → web.default.svc.cluster.local
- mysql → mysql.default.svc.cluster.local
----

**Configuración DNS de un Pod:**

[source,bash]
----
# En cualquier Pod, /etc/resolv.conf tiene:
cat /etc/resolv.conf
# nameserver 10.96.0.10  (DNS del cluster)
# search default.svc.cluster.local svc.cluster.local cluster.local
# options ndots:5
----

**Politica de resolución DNS:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  dnsPolicy: ClusterFirst        # Por defecto: cluster primero
  dnsConfig:
    nameservers:
      - 8.8.8.8
    searches:
      - my-domain.com
  containers:
  - name: app
    image: app:latest
----

Opciones de `dnsPolicy`:

- *ClusterFirst* (por defecto): DNS del cluster, fallback a DNS del nodo
- *ClusterFirstWithHostNet*: DNS del cluster incluso con hostNetwork: true
- *Default*: DNS del nodo
- *None*: usa dnsConfig

**Resolver nombres desde un Pod:**

[source,bash]
----
# Crear un Pod con herramientas
kubectl run debug --image=nicolaka/netshoot -it --rm --restart=Never -- bash

# Resolver un Service
nslookup kubernetes.default.svc.cluster.local

# Resolver otro Pod
nslookup mysql-0.mysql.default.pod.cluster.local

# Ver configuración DNS
cat /etc/resolv.conf

# Diagnosticar DNS
dig @10.96.0.10 web.default.svc.cluster.local

# Probar conectividad de red
ping 10.96.1.5
curl http://web.default.svc.cluster.local
----

==== Flujo de paquetes en Kubernetes

**Ejemplo: Pod A (nodo 1) → Pod B (nodo 2)**

[source]
----
1. Pod A genera paquete:
   Origen: 10.0.0.1 (Pod A)
   Destino: 10.0.0.2 (Pod B)

2. Sale de Pod A a través de virtual interface

3. Llega al plugin CNI que:
   - Si está en el mismo nodo: entrega directamente
   - Si está en otro nodo:
     a) Flannel: encapsula en VXLAN, envía a nodo 2
     b) Calico: usa BGP routing hacia nodo 2

4. En nodo 2, CNI:
   - Desencapsula (si fue encapsulado)
   - Entrega a Pod B

5. Pod B recibe paquete con IP original intacta
   Origen: 10.0.0.1
   Destino: 10.0.0.2
----

==== Troubleshooting de Conectividad

**Problema: Pod no puede resolver nombres**

[source,bash]
----
# Verificar CoreDNS está running
kubectl get pods -n kube-system -l k8s-app=kube-dns

# Ver logs de CoreDNS
kubectl logs -n kube-system -l k8s-app=kube-dns

# Verificar configuración DNS de Pod
kubectl exec my-pod -- cat /etc/resolv.conf

# Probar resolución
kubectl exec my-pod -- nslookup kubernetes.default
----

**Problema: Pod no puede alcanzar otro Pod**

[source,bash]
----
# Verificar IP del Pod destino
kubectl get pod target-pod -o wide

# Desde Pod origen:
kubectl exec source-pod -- ping <target-ip>

# Si falla, verificar CNI:
# 1. ¿Está instalado el plugin CNI?
kubectl get daemonset -n kube-system

# 2. ¿El CNI pod está running?
kubectl get pods -n kube-system -l app=flannel

# 3. Ver logs del CNI
kubectl logs -n kube-system <cni-pod-name>

# 4. Verificar rutas en el nodo
kubectl debug node/node-name -it --image=ubuntu
# En el nodo:
ip route  # Debe haber ruta a Pod CIDR
----

**Problema: Service no es accesible desde Pod**

[source,bash]
----
# Verificar Service existe
kubectl get svc my-service

# Verificar Service tiene Endpoints
kubectl get endpoints my-service

# Desde Pod, probar conectividad
kubectl exec my-pod -- nslookup my-service

# Ver logs de CoreDNS
kubectl logs -n kube-system -l k8s-app=kube-dns | grep my-service
----

==== Best Practices de Networking

1. *Entiende el modelo de red de tu CNI*
   - Overlay vs routing directo
   - Implicaciones de performance
   - Encapsulation overhead

2. *Usa DNS en lugar de IPs*
   - IPs de Pods son efímeras
   - DNS proporciona descubrimiento automático
   - Usa Service DNS (FQDN corto)

3. *Configura dnsPolicy apropiadamente*
   - ClusterFirst es el defecto usual
   - ClusterFirstWithHostNet para hostNetwork pods
   - None solo si necesitas control total

4. *Monitorea conectividad de red*
   - Prueba resolución DNS regularmente
   - Verifica latencia entre nodos
   - Monitorea pérdida de paquetes

5. *Planifica capacidad de red*
   - Pods CIDR: suficientemente grande para crecer
   - Service CIDR: separada de Pod CIDR
   - Node CIDR: no debe superponerse

6. *Usa herramientas de debug*
   - kubectl exec para diagnosticar
   - Imágenes con netshoot (nicolaka/netshoot)
   - tcpdump para análisis de paquetes

7. *Mantén seguridad en mente*
   - Implementa Network Policies desde el inicio
   - No confíes en aislamiento solo por namespace
   - Criptografía entre nodos si es sensible

8. *Optimiza para latencia*
   - Preferentemente coloca Pods relacionados en mismo nodo
   - Considera node affinity para aplicaciones latency-sensitive
   - Monitorea jitter de red

=== Services

**¿Qué es un Service?**

Un Service en Kubernetes es una abstracción que expone un conjunto de Pods como un servicio de red. Proporciona:

- *Dirección IP estable*: incluso cuando Pods cambian
- *Nombre DNS estable*: para descubrimiento de servicios
- *Load balancing*: distribuye tráfico entre Pods
- *Acceso consistente*: sin necesidad de conocer IPs de Pods

**¿Por qué necesitamos Services?**

Los Pods son efímeros. Sus IPs cambian cuando se recrean. Un Service proporciona una dirección estable para acceder a Pods.

[source]
----
Sin Service:
Pod A (10.0.0.1) muere
↓
Nuevo Pod A (10.0.0.5)
↑
Cliente debe conocer nueva IP

Con Service:
Pod A (10.0.0.1) → web.default.svc.cluster.local (10.96.1.5)
Pod B (10.0.0.2) →
Pod C (10.0.0.3) →
↓
Pod A muere
↓
Nuevo Pod A (10.0.0.5) → web.default.svc.cluster.local (10.96.1.5)
↑
Cliente sigue usando mismo nombre
----

==== Tipos de Services

**ClusterIP (por defecto)**

Service que solo es accesible dentro del cluster.

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: ClusterIP
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80          # Puerto del Service
    targetPort: 8080  # Puerto del Pod
----

[source,bash]
----
# Crear Service
kubectl apply -f service.yaml

# Ver Service
kubectl get svc web
# NAME   TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)
# web    ClusterIP   10.96.1.5    <none>        80/TCP

# Acceder desde dentro del cluster
kubectl exec -it client-pod -- wget http://web

# Acceder por IP
kubectl exec -it client-pod -- wget http://10.96.1.5:80
----

**NodePort**

Service expuesto en cada nodo en un puerto estático.

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: NodePort
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80          # Puerto en el Service
    targetPort: 8080  # Puerto en el Pod
    nodePort: 30080   # Puerto en cada nodo (30000-32767)
----

[source,bash]
----
# Kubernetes asigna puerto automáticamente si no especificas
# Rango: 30000-32767

# Ver servicio
kubectl get svc web
# NAME   TYPE       CLUSTER-IP   EXTERNAL-IP   PORT(S)
# web    NodePort   10.96.1.5    <none>        80:30080/TCP

# Acceder desde fuera del cluster
curl http://node-ip:30080
curl http://192.168.1.1:30080

# O usar cualquier nodo
curl http://192.168.1.2:30080
curl http://192.168.1.3:30080
----

**Caso de uso NodePort:**

- Desarrollo/testing
- Exposición temporal
- Aplicaciones que no requieren load balancer

**LoadBalancer**

Service expuesto externamente usando un cloud load balancer.

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: LoadBalancer
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
----

[source,bash]
----
# Ver servicio
kubectl get svc web
# NAME   TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)
# web    LoadBalancer   10.96.1.5    203.0.113.100   80:30080/TCP

# La dirección EXTERNAL-IP es asignada por el cloud provider

# Acceder desde fuera
curl http://203.0.113.100
----

**Configuración avanzada de LoadBalancer:**

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: LoadBalancer
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 8080
  loadBalancerSourceRanges:
  - 203.0.113.0/24  # Solo desde este CIDR
  loadBalancerIP: 203.0.113.100  # IP específica (si soporta el provider)
  externalTrafficPolicy: Local  # No SNAT local traffic
----

**ExternalName**

Service que redirige a un nombre externo (CNAME).

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: external-db
spec:
  type: ExternalName
  externalName: db.example.com
  ports:
  - port: 5432
----

[source,bash]
----
# Desde un Pod del cluster:
kubectl exec -it client-pod -- psql -h external-db -U user

# Se resuelve como:
# external-db.default.svc.cluster.local → db.example.com
----

==== Tabla comparativa de tipos de Services

|===
| Tipo | Acceso | Rango de puertos | Casos de uso

| ClusterIP
| Solo intra-cluster
| Cualquiera
| Comunicación interna, databases, caches

| NodePort
| Extra-cluster (nodo:puerto)
| 30000-32767
| Desarrollo, testing, sin load balancer

| LoadBalancer
| Extra-cluster (IP pública)
| Cualquiera
| Producción, acceso público

| ExternalName
| DNS externo
| N/A
| Integración con servicios externos
|===

==== Selección de Pods mediante Labels

Los Services usan selectors (basados en labels) para identificar qué Pods están detrás.

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  selector:
    app: web           # Selecciona Pods con label app=web
    version: v1        # Y label version=v1
  ports:
  - port: 80
    targetPort: 8080
---
# Pods que coinciden:
apiVersion: v1
kind: Pod
metadata:
  name: web-pod-1
  labels:
    app: web          # ✓ Seleccionado
    version: v1       # ✓ Seleccionado
spec:
  containers:
  - name: app
    image: web:v1
---
# Este Pod NO es seleccionado:
apiVersion: v1
kind: Pod
metadata:
  name: web-pod-2
  labels:
    app: web          # ✓
    version: v2       # ✗ (necesita v1)
spec:
  containers:
  - name: app
    image: web:v2
----

[source,bash]
----
# Ver qué Pods están detrás del Service
kubectl get endpoints web

# Ver detallado
kubectl describe svc web

# Ver Pods seleccionados
kubectl get pods -l app=web,version=v1
----

==== Endpoints

Los Endpoints son los Pods reales detrás de un Service.

[source,bash]
----
# Ver Endpoints
kubectl get endpoints web

# ENDPOINTS           AGE
# 10.0.0.10:8080,10.0.0.11:8080   3m

# Ver detallado
kubectl describe endpoints web

# Formato YAML
kubectl get endpoints web -o yaml
----

Cuando Kubernetes detecta cambios en Pods que coinciden con los selectores, actualiza automáticamente los Endpoints.

[source]
----
1. Pod A creado con label app=web
   ↓
2. Service selector busca label app=web
   ↓
3. Pod A IP añadida a Endpoints
   ↓
4. Pod A muere
   ↓
5. IP removida de Endpoints automáticamente
----

==== Ejemplo: Service con múltiples Pods

[source,bash]
----
kubectl apply -f - <<EOF
---
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: app
        image: nginx:latest
        ports:
        - containerPort: 8080
EOF

# Ver Service
kubectl get svc web

# Ver Endpoints (3 Pods)
kubectl get endpoints web
# ENDPOINTS                                                 AGE
# 10.0.0.10:8080,10.0.0.11:8080,10.0.0.12:8080          2m

# Verificar balanceo de carga
kubectl run -it client --image=busybox --rm --restart=Never -- \
  wget -O- http://web

# Cada petición va a un Pod diferente (round-robin)
----

==== Session Affinity

Por defecto, el tráfico se distribuye sin estado entre Pods. Session Affinity permite "pegar" un cliente a un Pod.

**ClientIP affinity:**

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  selector:
    app: web
  sessionAffinity: ClientIP    # "pega" cliente a Pod
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800    # 3 horas
  ports:
  - port: 80
    targetPort: 8080
----

[source,bash]
----
# Mismo cliente siempre va al mismo Pod
# Útil para aplicaciones que almacenan estado en memoria

kubectl run -it client --image=busybox --rm --restart=Never -- sh

# Dentro del cliente:
# Primera petición → Pod A
wget -O- http://web

# Segunda petición → Pod A (mismo, no Pod B)
wget -O- http://web

# Tercera petición → Pod A (mismo)
wget -O- http://web
----

**Casos de uso para Session Affinity:**

- Aplicaciones con sesiones en memoria
- Caches locales
- Conexiones con estado prolongado

**NOTA:** Mejor que usar session affinity es:
- Almacenar sesiones en cache distribuido (Redis)
- Usar cookies para tracking
- Hacer la aplicación stateless

==== Multi-port Services

Un Service puede exponer múltiples puertos:

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  selector:
    app: web
  ports:
  - name: http
    port: 80
    targetPort: 8080
    protocol: TCP
  - name: https
    port: 443
    targetPort: 8443
    protocol: TCP
  - name: metrics
    port: 9090
    targetPort: 9090
    protocol: TCP
----

[source,bash]
----
# Acceder a diferentes puertos
kubectl exec client -- wget http://web:80    # HTTP
kubectl exec client -- wget https://web:443  # HTTPS
kubectl exec client -- wget http://web:9090  # Metrics
----

==== Headless Services

Un Service sin ClusterIP que proporciona DNS directo a Pods.

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  clusterIP: None              # Headless
  selector:
    app: mysql
  ports:
  - port: 3306
    targetPort: 3306
----

[source,bash]
----
# Con Headless Service (como StatefulSet):
nslookup mysql.default.svc.cluster.local
# ANSWER SECTION:
# mysql.default.svc.cluster.local. 30 IN A 10.0.0.10
# mysql.default.svc.cluster.local. 30 IN A 10.0.0.11
# mysql.default.svc.cluster.local. 30 IN A 10.0.0.12

# Resolución de Pod individual:
nslookup mysql-0.mysql.default.svc.cluster.local
# ANSWER SECTION:
# mysql-0.mysql.default.svc.cluster.local. 30 IN A 10.0.0.10
----

==== External IPs

Exponer un Service en IPs específicas de los nodos:

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  selector:
    app: web
  externalIPs:
  - 192.168.1.10
  - 192.168.1.11
  ports:
  - port: 80
    targetPort: 8080
----

[source,bash]
----
# Accesible en las IPs especificadas
curl http://192.168.1.10:80
curl http://192.168.1.11:80
----

==== Best Practices para Services

1. *Usa nombres descriptivos*
   - web, api, database, cache
   - Facilita discovery

2. *Elige el tipo correcto*
   - ClusterIP para comunicación interna
   - LoadBalancer para acceso externo
   - NodePort solo para testing

3. *Especifica puertos con nombres*
   - Facilita debugging y monitoreo
   - Mejor para multi-port services

4. *Usa selectores consistentes*
   - Labels deben coincidir con Pods
   - Mantén convención de naming

5. *Monitorea Endpoints*
   - Verifica que Pods estén "seleccionados"
   - Alertas si Endpoints vacío

6. *Cuidado con Session Affinity*
   - Puede causar desbalanceo
   - Mejor ser stateless

7. *Documenta propósito del Service*
   - Qué aplicaciones sirve
   - Qué puerto usan

8. *Usa Network Policies con Services*
   - Controla qué puede acceder
   - Defense in depth

==== Troubleshooting de Services

**Problema: Endpoints vacío**

[source,bash]
----
# Ver Service
kubectl describe svc web

# Ver Endpoints
kubectl get endpoints web
# Debería mostrar IPs de Pods, si está vacío:

# 1. Ver Pods disponibles
kubectl get pods -o wide

# 2. Ver qué labels tienen los Pods
kubectl get pods --show-labels

# 3. Verificar selector del Service
kubectl get svc web -o yaml
# Mirar spec.selector

# 4. Verificar Pods con ese selector
kubectl get pods -l app=web

# Causa común: labels no coinciden
----

**Problema: Service no responde**

[source,bash]
----
# 1. Verificar Service existe y tiene ClusterIP
kubectl get svc web

# 2. Verificar endpoints no vacíos
kubectl get endpoints web

# 3. Probar conectividad a Pod directamente
kubectl exec client -- ping <pod-ip>

# 4. Verificar kube-proxy
kubectl get daemonset -n kube-system kube-proxy
kubectl logs -n kube-system -l component=kube-proxy

# 5. Ver iptables/ipvs en nodo
kubectl debug node/node-name -it --image=ubuntu
# En el nodo:
sudo iptables -L -n | grep <service-ip>
sudo ipvsadm -L  # si usa IPVS
----

**Problema: LoadBalancer sin EXTERNAL-IP**

[source,bash]
----
# Verificar Service
kubectl get svc web

# Ver eventos
kubectl describe svc web

# Causas:
# - Cloud provider no soporta LoadBalancer
# - Cuota de IPs agotada
# - Provider no configurado

# En metal desnudo, usar metallb
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/main/config/manifests/metallb-native.yaml
----

=== Ingress

**¿Qué es Ingress?**

Ingress es una API de Kubernetes que expone rutas HTTP/HTTPS desde fuera del cluster a Services dentro del cluster. Proporciona:

- *Routing de tráfico*: dirigir tráfico a diferentes Services basado en hostname/ruta
- *TLS/SSL termination*: manejo de certificados
- *Load balancing*: distribución de carga HTTP(S)
- *Virtual hosting*: múltiples dominios en un solo Ingress

**Diagrama: Ingress vs LoadBalancer Service**

[source]
----
┌────────────────────────────────────────────────┐
│         LoadBalancer Service                    │
├────────────────────────────────────────────────┤
│  Cliente HTTP → LB IP:80 → Pods                │
│  127.0.0.1:80 (ej)                             │
│  (nivel 4, transporte)                         │
└────────────────────────────────────────────────┘

┌────────────────────────────────────────────────┐
│              Ingress                            │
├────────────────────────────────────────────────┤
│  Cliente HTTP → LB IP:80 → Ingress Controller   │
│  ↓                                              │
│  Parsea Host/Path HTTP                         │
│  ↓                                              │
│  web.example.com/api → service-api             │
│  web.example.com/v2 → service-v2               │
│  api.example.com → service-api-internal        │
│  (nivel 7, aplicación)                         │
└────────────────────────────────────────────────┘

LoadBalancer: L4 (transporte)
Ingress: L7 (aplicación)
----

**¿Cuándo usar Ingress?**

- Múltiples dominios/subdomios
- Basado en paths de URLs
- Compartir un LoadBalancer entre múltiples Services
- TLS/SSL termination centralizado
- Rate limiting, autenticación

==== YAML básico de Ingress

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
spec:
  rules:
  - host: web.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web
            port:
              number: 80
----

[source,bash]
----
# Crear Ingress
kubectl apply -f ingress.yaml

# Ver Ingress
kubectl get ingress

# Ver detallado
kubectl describe ingress web-ingress

# Actualizar /etc/hosts para testing local
echo "127.0.0.1 web.example.com" >> /etc/hosts

# Acceder
curl http://web.example.com
----

==== Ingress Controllers

Un Ingress Controller es un controlador que implementa Ingress.Kubernetes NO incluye uno por defecto. Debes instalar uno.

**Controladores populares:**

|===
| Controlador | Tipo | Características | Casos de uso

| NGINX
| Reverse proxy
| Simple, rápido, estable
| La mayoría de casos

| Traefik
| Reverse proxy
| Moderno, auto-discovery
| Microservicios, dinámico

| HAProxy
| Load balancer
| Alto rendimiento
| Producción crítica

| Istio Ingress Gateway
| Service mesh
| Routing avanzado, observabilidad
| Architecturas service mesh

| AWS ALB
| Nativo AWS
| Integración AWS
| Clusters EKS

| GCP Cloud Load Balancer
| Nativo GCP
| Integración GCP
| Clusters GKE
|===

==== NGINX Ingress Controller

**Instalación de NGINX Ingress Controller:**

[source,bash]
----
# Opción 1: usando Helm (recomendado)
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm install nginx ingress-nginx/ingress-nginx \
  --namespace ingress-nginx \
  --create-namespace

# Opción 2: usando manifiestos
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.0/deploy/static/provider/cloud/deploy.yaml

# Verificar instalación
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx

# Obtener IP del LoadBalancer
kubectl get svc -n ingress-nginx ingress-nginx-controller
# NAME                              TYPE           CLUSTER-IP     EXTERNAL-IP
# ingress-nginx-controller          LoadBalancer   10.96.1.5      203.0.113.100
----

**Ejemplo: Ingress NGINX con host y path**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp
spec:
  ingressClassName: nginx
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app
            port:
              number: 80
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api
            port:
              number: 8080
      - path: /v2
        pathType: Prefix
        backend:
          service:
            name: api-v2
            port:
              number: 8080
----

==== Path-based Routing

Enrutar basado en la ruta de la URL:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web
spec:
  ingressClassName: nginx
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: website
            port:
              number: 80
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8080
      - path: /admin
        pathType: Prefix
        backend:
          service:
            name: admin-panel
            port:
              number: 3000
      - path: /api/v2
        pathType: Prefix
        backend:
          service:
            name: api-v2
            port:
              number: 8080
----

**Tipos de pathType:**

- *Prefix*: `/api` coincide `/api`, `/api/v1`, `/api/users`
- *Exact*: `/api` coincide solo `/api` (no `/api/v1`)
- *ImplementationSpecific*: depende del controlador

[source]
----
Requests a example.com:
GET /                   → website service
GET /api                → api-service
GET /api/v1            → api-service (coincide /api)
GET /admin             → admin-panel
GET /admin/users       → admin-panel
GET /api/v2            → api-v2 (coincide /api/v2 exacto)
----

==== Host-based Routing

Enrutar basado en el hostname:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-tenant
spec:
  ingressClassName: nginx
  rules:
  - host: customer-a.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: customer-a-app
            port:
              number: 80
  - host: customer-b.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: customer-b-app
            port:
              number: 80
  - host: "*.example.com"  # Wildcard
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: default-app
            port:
              number: 80
----

[source]
----
Requests:
customer-a.example.com → customer-a-app service
customer-b.example.com → customer-b-app service
customer-c.example.com → default-app service
subdomain.example.com → default-app service
----

==== TLS/SSL Termination

Exponer HTTPS a través de Ingress:

**Crear certificado TLS:**

[source,bash]
----
# Opción 1: Crear con OpenSSL
openssl req -x509 -newkey rsa:4096 -keyout tls.key -out tls.crt -days 365 -nodes

# Opción 2: Usar certificado existente
# Tienes tls.crt y tls.key

# Crear Secret
kubectl create secret tls web-tls --cert=tls.crt --key=tls.key
----

**Ingress con TLS:**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: secure-app
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - app.example.com
    secretName: web-tls         # Secret con cert/key
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app
            port:
              number: 80
----

[source,bash]
----
# Acceder con HTTPS
curl https://app.example.com

# Verificar certificado
openssl s_client -connect app.example.com:443
----

**Múltiples certificados:**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-tls
spec:
  tls:
  - hosts:
    - app.example.com
    secretName: app-tls
  - hosts:
    - api.example.com
    secretName: api-tls
  - hosts:
    - "*.internal.example.com"
    secretName: wildcard-tls
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app
            port:
              number: 80
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api
            port:
              number: 8080
----

==== Cert-Manager: Certificados automáticos

Cert-Manager genera y renueva certificados automáticamente usando Let's Encrypt.

**Instalación:**

[source,bash]
----
# Instalar cert-manager
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.10.0/cert-manager.yaml

# Verificar
kubectl get pods -n cert-manager
----

**ClusterIssuer para Let's Encrypt:**

[source,yaml]
----
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@example.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx
----

**Ingress con auto-certificado:**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"  # Ref al issuer
spec:
  tls:
  - hosts:
    - app.example.com
    secretName: app-tls-auto     # Se crea automáticamente
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app
            port:
              number: 80
----

[source,bash]
----
# Cert-manager crea automáticamente:
# 1. Certificate resource
# 2. Valida con Let's Encrypt
# 3. Crea Secret con certificado
# 4. Renueva automáticamente (60 días antes de expirar)

# Ver Certificates
kubectl get certificate

# Ver Secret creado
kubectl get secret app-tls-auto -o yaml
----

==== Ejemplo completo: Multi-tenant Ingress

[source,bash]
----
kubectl apply -f - <<EOF
---
# Namespace para tenants
apiVersion: v1
kind: Namespace
metadata:
  name: tenants
---
# Services para cada tenant
apiVersion: v1
kind: Service
metadata:
  name: tenant-a-app
  namespace: tenants
spec:
  selector:
    tenant: a
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: tenant-b-app
  namespace: tenants
spec:
  selector:
    tenant: b
  ports:
  - port: 80
    targetPort: 8080
---
# Deployments para cada tenant
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tenant-a
  namespace: tenants
spec:
  replicas: 2
  selector:
    matchLabels:
      tenant: a
  template:
    metadata:
      labels:
        tenant: a
    spec:
      containers:
      - name: app
        image: nginx:latest
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tenant-b
  namespace: tenants
spec:
  replicas: 2
  selector:
    matchLabels:
      tenant: b
  template:
    metadata:
      labels:
        tenant: b
    spec:
      containers:
      - name: app
        image: nginx:latest
---
# Ingress multi-tenant
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-tenant
  namespace: tenants
spec:
  ingressClassName: nginx
  rules:
  - host: a.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: tenant-a-app
            port:
              number: 80
  - host: b.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: tenant-b-app
            port:
              number: 80
EOF

# Verificar
kubectl get ingress -n tenants
kubectl get svc -n tenants
kubectl get pods -n tenants
----

==== Ingress Annotations

Las anotaciones permiten configuración avanzada del controlador:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: advanced
  annotations:
    # NGINX-específicas
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/limit-rps: "10"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "*"

    # Cert-manager
    cert-manager.io/cluster-issuer: "letsencrypt-prod"

    # Auth
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required'
spec:
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api
            port:
              number: 8080
----

==== Traefik: Ingress Controller moderno

**Instalación de Traefik:**

[source,bash]
----
# Con Helm
helm repo add traefik https://traefik.github.io/charts
helm install traefik traefik/traefik \
  --namespace traefik \
  --create-namespace

# Verificar
kubectl get pods -n traefik
kubectl get svc -n traefik
----

**Ingress con Traefik:**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app
  annotations:
    traefik.ingress.kubernetes.io/router.tls: "true"
spec:
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app
            port:
              number: 80
----

Traefik también soporta CRDs nativos (IngressRoute) para configuración más avanzada.

==== Best Practices para Ingress

1. *Elige el controlador adecuado*
   - NGINX para la mayoría de casos
   - Traefik si necesitas auto-discovery
   - Cloud-native si usas EKS/GKE/AKS

2. *Usa tls automático*
   - Implementa cert-manager
   - Let's Encrypt para dominios públicos

3. *Organiza Ingress resources*
   - Uno por aplicación
   - Agrupa por namespace

4. *Implementa rate limiting*
   - Previene abuso
   - Protege backends

5. *Usa reescritura de URLs*
   - rewrite-target para servicios internos
   - Mantiene URLs limpias para usuarios

6. *Monitorea Ingress Controller*
   - Logs de HTTP
   - Métricas (Prometheus)
   - Alertas para 5xx errors

7. *Documenta reglas de routing*
   - Qué lleva a dónde
   - Quién puede acceder

8. *Valida configuración*
   - Verifica Ingress antes de desplegar
   - Prueba routing antes en staging

==== Troubleshooting de Ingress

**Problema: Ingress sin IP externa**

[source,bash]
----
# Ver estado
kubectl get ingress

# Ver detallado
kubectl describe ingress myapp

# Verificar que el controlador está corriendo
kubectl get pods -n ingress-nginx

# Si hay error, ver eventos
kubectl get events -n ingress-nginx
----

**Problema: 404 Not Found**

[source,bash]
----
# Verificar que Service existe
kubectl get svc

# Verificar que Service tiene endpoints
kubectl get endpoints

# Probar acceso directo a Pod
kubectl exec client -- wget http://service-ip

# Ver logs del Ingress Controller
kubectl logs -n ingress-nginx <controller-pod>

# Verificar path correcto en Ingress
kubectl get ingress -o yaml
----

**Problema: HTTPS certificado inválido**

[source,bash]
----
# Verificar Secret existe
kubectl get secret

# Ver contenido del Secret
kubectl get secret tls-secret -o yaml

# Verificar certificado
kubectl get secret tls-secret -o jsonpath='{.data.tls\.crt}' | base64 -d | openssl x509 -noout -text

# Si usa cert-manager, ver Certificate
kubectl get certificate

# Ver eventos del Certificate
kubectl describe certificate myapp
----

=== Network Policies

**¿Qué es una Network Policy?**

Una Network Policy es una especificación de cómo grupos de Pods pueden comunicarse entre sí y otros endpoints en la red. Proporciona:

- *Aislamiento de red*: restricción de tráfico entre Pods
- *Seguridad*: principio de menor privilegio en red
- *Segmentación*: separación lógica de aplicaciones
- *Control granular*: reglas por Pod, namespace, puerto

**¿Por qué Network Policies?**

Por defecto, en Kubernetes todos los Pods pueden comunicarse con todos los demás Pods. Network Policies restringen esta comunicación.

[source]
----
Sin Network Policy:
┌─────────────────────────────────────┐
│  Namespace: production              │
├─────────────────────────────────────┤
│  web ↔ api ↔ database ↔ monitoring │
│     Cualquier Pod habla con cualquier otro
└─────────────────────────────────────┘

Con Network Policy:
┌─────────────────────────────────────┐
│  Namespace: production              │
├─────────────────────────────────────┤
│  web → api → database               │
│              ↑                       │
│        (monitoring no puede acceder) │
└─────────────────────────────────────┘
----

==== Requisitos

Network Policies se requiere que el CNI lo soporte:
- Calico: ✓ soporta
- Cilium: ✓ soporta
- Flannel: ✗ NO soporta
- Weave: ✓ soporta

[source,bash]
----
# Verificar si tu CNI soporta Network Policies
kubectl get daemonset -n kube-system
# Si ves calico-node, cilium-agent, weave: OK
# Si ves flannel-ds: Network Policies no funcionarán
----

==== YAML básico de Network Policy

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
spec:
  podSelector: {}  # Aplica a todos los Pods
  policyTypes:
  - Ingress
  - Egress
  ingress: []      # Ningún ingress permitido
  egress: []       # Ningún egress permitido
----

[source,bash]
----
# Aplicar Network Policy
kubectl apply -f netpol.yaml

# Ver Network Policies
kubectl get networkpolicy

# Ver detallado
kubectl describe networkpolicy deny-all
----

==== Selección de Pods

Network Policies usan pod selectors basados en labels:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-web
spec:
  podSelector:
    matchLabels:
      app: web      # Aplica a Pods con label app=web
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: client  # Permite tráfico desde Pods con label app=client
    ports:
    - protocol: TCP
      port: 80
----

==== Reglas de Ingress

Ingress controla tráfico entrante a un Pod.

**Estructura básica:**

[source,yaml]
----
spec:
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: client
    - namespaceSelector:
        matchLabels:
          name: frontend
    - ipBlock:
        cidr: 203.0.113.0/24
    ports:
    - protocol: TCP
      port: 80
----

Esto significa: "Permite ingress si proviene de (Pod con label app=client) O (namespace frontend) O (CIDR 203.0.113.0/24) EN puertos TCP 80".

**Ejemplo 1: Permitir desde Pods específicos**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-api
spec:
  podSelector:
    matchLabels:
      app: database
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: api     # Solo Pods con label app=api
    ports:
    - protocol: TCP
      port: 5432      # Solo puerto 5432 (PostgreSQL)
----

**Ejemplo 2: Permitir desde Namespace específico**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-namespace
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend  # Solo desde namespace "frontend"
    ports:
    - protocol: TCP
      port: 8080
----

**Ejemplo 3: Permitir desde CIDR específico**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-cidr
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - ipBlock:
        cidr: 10.0.0.0/8  # Permitir desde esta red
        except:
        - 10.0.0.5/32     # Excepto esta IP
    ports:
    - protocol: TCP
      port: 443
----

==== Reglas de Egress

Egress controla tráfico saliente de un Pod.

**Estructura:**

[source,yaml]
----
spec:
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: api
    ports:
    - protocol: TCP
      port: 8080
----

**Ejemplo: Restricción de egress**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: restrict-egress
spec:
  podSelector:
    matchLabels:
      app: web
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: api     # Solo puede enviar a api
    ports:
    - protocol: TCP
      port: 8080
  - to:
    - podSelector:
        matchLabels:
          app: database  # Y a database
    ports:
    - protocol: TCP
      port: 5432
  # DNS es especial (necesario excepto si desactivas)
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53
----

==== Politicas de aislamiento por defecto

**Deny All Ingress:**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  # ingress: [] (vacío = rechaza todo)
----

Después de aplicar esta policy, NADA puede entrar a ningún Pod a menos que haya otra policy que permita.

**Deny All Egress:**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-egress
spec:
  podSelector: {}
  policyTypes:
  - Egress
  # egress: [] (vacío = rechaza todo)
----

Después de aplicar, ningún Pod puede salir a menos que haya otra policy.

**Deny All (Ingress y Egress):**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
----

==== Casos de uso comunes

**Caso 1: Arquitectura de 3 capas (web, api, database)**

[source,yaml]
----
---
# 1. Permitir tráfico a web (desde internet)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-to-web
spec:
  podSelector:
    matchLabels:
      tier: web
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector: {}  # Desde cualquier namespace
    ports:
    - protocol: TCP
      port: 80
    - protocol: TCP
      port: 443
---
# 2. Web puede hablar con api
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-web-to-api
spec:
  podSelector:
    matchLabels:
      tier: api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: web
    ports:
    - protocol: TCP
      port: 8080
---
# 3. API puede hablar con database
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-api-to-db
spec:
  podSelector:
    matchLabels:
      tier: database
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: api
    ports:
    - protocol: TCP
      port: 5432
---
# 4. Denegar todo lo demás (default deny)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  # (ingress y egress vacías = rechaza todo excepto lo permitido arriba)
----

**Caso 2: Aislamiento por tenant (multi-tenant)**

[source,yaml]
----
---
# Tenant A puede hablar internamente
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tenant-a-internal
spec:
  podSelector:
    matchLabels:
      tenant: a
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tenant: a
  egress:
  - to:
    - podSelector:
        matchLabels:
          tenant: a
  - to:
    - namespaceSelector: {}  # DNS
    ports:
    - protocol: UDP
      port: 53
---
# Tenant B aislado
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tenant-b-internal
spec:
  podSelector:
    matchLabels:
      tenant: b
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tenant: b
  egress:
  - to:
    - podSelector:
        matchLabels:
          tenant: b
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53
----

==== Segmentación por Namespace

Network Policies pueden aislar namespaces completos:

[source,bash]
----
# Etiquetar namespace
kubectl label namespace frontend name=frontend
kubectl label namespace backend name=backend
kubectl label namespace database name=database
----

[source,yaml]
----
# Backend solo puede acceder a database
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-to-db
  namespace: backend
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    ports:
    - protocol: TCP
      port: 5432
  - to:
    - namespaceSelector: {}  # DNS
    ports:
    - protocol: UDP
      port: 53
---
# Frontend solo puede acceder a backend
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-to-backend
  namespace: frontend
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: backend
    ports:
    - protocol: TCP
      port: 8080
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53
----

==== Manejo de DNS

DNS es especial. Si bloqueas egress, necesitas permitir DNS (puerto 53 UDP):

[source,yaml]
----
spec:
  egress:
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53  # DNS query
    - protocol: TCP
      port: 53  # DNS query (TCP)
  - to:
    - podSelector:
        matchLabels:
          app: api
    ports:
    - protocol: TCP
      port: 8080
----

Sin esto, los Pods no pueden resolver nombres de otros Services.

==== Herramientas para Network Policies

**Tester de NetworkPolicy:**

[source,bash]
----
# Ver si tráfico está permitido
kubectl run -it --image=nicolaka/netshoot debug --rm --restart=Never -- bash

# Dentro del Pod:
# Probar conectividad
wget http://api:8080
curl http://database:5432

# Ver IPs de otros Pods
nslookup api.default.svc.cluster.local
----

**Visualización:**

[source,bash]
----
# Ver Network Policies
kubectl get networkpolicy
kubectl get networkpolicy -A

# Ver detalles
kubectl describe networkpolicy <name>

# Ver en YAML
kubectl get networkpolicy -o yaml
----

==== Best Practices para Network Policies

1. *Empieza permisivo, termina restrictivo*
   - Primero permite lo necesario
   - Luego añade deny-all
   - No es al revés

2. *Usa labels consistentemente*
   - Etiquetar todos los Pods apropiadamente
   - Labels para tier, app, tenant, env

3. *Agrupa reglas por lógica*
   - Una policy por relación (web→api, api→db)
   - Facilita mantenimiento

4. *Permite DNS explícitamente*
   - Si usas egress policy, permite port 53
   - Sino, los Pods no pueden resolver nombres

5. *Monitorea tráfico bloqueado*
   - Activar logging en el CNI
   - Alertas para tráfico rechazado

6. *Documenta políticas*
   - Qué permite cada policy
   - Por qué es necesaria

7. *Prueba antes de producción*
   - Aplica en staging primero
   - Verifica que no rompe aplicaciones

8. *Usa namespaces para separación*
   - Cada tenant en namespace distinto
   - Facilita aislamiento

==== Troubleshooting de Network Policies

**Problema: Tráfico bloqueado inesperadamente**

[source,bash]
----
# Ver Network Policies aplicadas
kubectl get networkpolicy

# Ver detalles de una policy
kubectl describe networkpolicy <name>

# Verificar labels de Pods
kubectl get pods --show-labels

# Revisar si labels coinciden con selectors
# Causa común: labels no coinciden

# Probar tráfico directo
kubectl exec <pod> -- curl http://<destino>

# Ver logs del CNI
kubectl logs -n kube-system -l k8s-app=calico-node
----

**Problema: DNS no funciona**

[source,bash]
----
# Verificar egress allow DNS
kubectl describe networkpolicy <name>

# Debe haber:
# to: namespaceSelector: {}
# port: 53 UDP

# Agregar si falta:
# spec:
#   egress:
#   - to:
#     - namespaceSelector: {}
#     ports:
#     - protocol: UDP
#       port: 53
----

**Problema: Ingress/Egress rechaza legítimamente**

[source,bash]
----
# Ver tráfico actual
kubectl exec <pod> -- netstat -tlnp

# Identificar qué necesita comunicarse con qué
# Luego crear policies permitiendo eso

# Mejor: aplicar deny-all luego allow específico
# Que al revés (deny específico)
----

== Módulo 5: Almacenamiento

=== Volúmenes

**¿Qué es un volumen?**

Un volumen en Kubernetes es una forma de almacenamiento que persiste durante la vida de un Pod. Los volúmenes permiten:

- *Compartir datos entre contenedores* en el mismo Pod
- *Acceder a datos del nodo host*
- *Almacenar datos de forma temporal*
- *Inyectar configuración en Pods* (ConfigMaps, Secrets)

**Ciclo de vida:**

- Cuando se crea un Pod se crean sus volúmenes
- Cuando se elimina el Pod se eliminan los volúmenes (excepto PVs)
- Los datos dentro persisten mientras el Pod exista

**Diferencia: Volúmenes vs PersistentVolumes**

|===
| Aspecto | Volumen | PersistentVolume

| Ciclo de vida
| Pod (efímero)
| Cluster (persistente)

| Alcance
| Un Pod
| Todo el cluster

| Provisioning
| Manual o automático
| Manual o dinámico

| Casos de uso
| Temporal, cache, config
| Bases de datos, backups

| Persistencia
| Se elimina con Pod
| Persiste después del Pod
|===

==== Tipos de Volúmenes Efímeros

**emptyDir: Almacenamiento temporal**

Un volumen vacío que se crea cuando se crea el Pod y se elimina cuando se elimina.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: multi-container
spec:
  containers:
  - name: writer
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      while true; do
        echo "$(date): hello from writer" >> /data/log.txt
        sleep 5
      done
    volumeMounts:
    - name: data
      mountPath: /data
  - name: reader
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      while true; do
        cat /data/log.txt
        sleep 10
      done
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    emptyDir: {}
----

**Casos de uso emptyDir:**

- *Cache entre contenedores*: un contenedor escribe, otro lee
- *Almacenamiento temporal*: archivos que se descartan con Pod
- *Git-sync sidecar*: descargar código en volumen compartido
- *Log processor*: un contenedor escribe logs, otro los procesa

**emptyDir con límite de tamaño:**

[source,yaml]
----
volumes:
- name: cache
  emptyDir:
    sizeLimit: 1Gi  # Máximo 1GB, luego el Pod se evicta
----

**emptyDir con tipo de almacenamiento:**

[source,yaml]
----
volumes:
- name: cache
  emptyDir:
    medium: Memory  # Usar RAM en lugar de disco
    sizeLimit: 512Mi
----

Usar `medium: Memory` es útil para caches rápidos pero consume memoria del nodo.

==== hostPath: Acceder al nodo host

Un volumen que monta un archivo o directorio del nodo host.

**ADVERTENCIA:** `hostPath` es potencialmente inseguro. Evitarlo en clusters multi-tenant.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: host-access
spec:
  containers:
  - name: app
    image: busybox
    volumeMounts:
    - name: host-logs
      mountPath: /host-logs
  volumes:
  - name: host-logs
    hostPath:
      path: /var/log      # Ruta en el nodo
      type: Directory     # Debe ser directorio
----

**Tipos de hostPath:**

|===
| Tipo | Descripción | Verificación

| (vacío)
| No verificar
| Sin validación

| DirectoryOrCreate
| Directorio, crear si no existe
| Crea si falta

| Directory
| Debe existir directorio
| Falla si no existe

| FileOrCreate
| Archivo, crear si no existe
| Crea si falta

| File
| Debe existir archivo
| Falla si no existe

| Socket
| Socket UNIX existente
| Falla si no es socket

| CharDevice
| Dispositivo char existente
| Falla si no es char device

| BlockDevice
| Dispositivo block existente
| Falla si no es block device
|===

**Ejemplo: Monitoreo con acceso a host**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: system-monitor
spec:
  containers:
  - name: monitor
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      while true; do
        echo "=== CPU Info ==="
        cat /host-proc/cpuinfo | head -5
        echo "=== Memory Info ==="
        cat /host-proc/meminfo | head -3
        sleep 10
      done
    volumeMounts:
    - name: proc
      mountPath: /host-proc
    - name: sys
      mountPath: /host-sys
  volumes:
  - name: proc
    hostPath:
      path: /proc
      type: Directory
  - name: sys
    hostPath:
      path: /sys
      type: Directory
----

==== configMap y secret como volúmenes

ConfigMaps y Secrets pueden montarse como volúmenes en Pods.

**ConfigMap como volumen:**

[source,bash]
----
# Crear ConfigMap con archivos
kubectl create configmap app-config \
  --from-literal=database.host=localhost \
  --from-literal=database.port=5432
----

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: app:latest
    volumeMounts:
    - name: config
      mountPath: /etc/config
  volumes:
  - name: config
    configMap:
      name: app-config
      items:
      - key: database.host
        path: db-host.txt      # Renombar clave a archivo
      - key: database.port
        path: db-port.txt
      defaultMode: 0644       # Permisos (octal)
----

**Secret como volumen:**

[source,bash]
----
# Crear Secret
kubectl create secret generic db-credentials \
  --from-literal=username=admin \
  --from-literal=password=secret123
----

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: app:latest
    volumeMounts:
    - name: secrets
      mountPath: /etc/secrets
      readOnly: true        # Recomendado para secrets
  volumes:
  - name: secrets
    secret:
      secretName: db-credentials
      defaultMode: 0400     # Solo lectura (permisos)
----

**Acceder desde el contenedor:**

[source,bash]
----
# Con volumen montado, los archivos están disponibles:
ls /etc/config
# database.host
# database.port

cat /etc/config/database.host
# localhost

cat /etc/config/database.port
# 5432
----

==== Montaje de volúmenes

**Estructura completa de volumeMounts:**

[source,yaml]
----
spec:
  containers:
  - name: app
    volumeMounts:
    - name: data           # Nombre del volumen (debe coincidir)
      mountPath: /app/data # Ruta en el contenedor
      subPath: subfolder   # Opcional: subcarpeta del volumen
      readOnly: false      # Lectura/escritura (defecto)
----

**Ejemplo con subPath:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: multi-mount
spec:
  containers:
  - name: app
    image: app:latest
    volumeMounts:
    - name: shared
      mountPath: /app/logs
      subPath: logs        # Monta solo la subcarpeta logs/
    - name: shared
      mountPath: /app/data
      subPath: data        # Monta solo la subcarpeta data/
  volumes:
  - name: shared
    emptyDir: {}
----

Sin `subPath`, ambas volumeMounts sobreescribirían. Con `subPath`, montan diferentes carpetas.

==== Ejemplo completo: Multi-container con volúmenes

[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: log-processor
spec:
  containers:
  # Contenedor que escribe logs
  - name: producer
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      counter=0
      while true; do
        counter=$((counter+1))
        echo "[\$(date '+%Y-%m-%d %H:%M:%S')] Event \$counter" >> /logs/events.log
        sleep 2
      done
    volumeMounts:
    - name: logs
      mountPath: /logs

  # Contenedor que procesa los logs
  - name: processor
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      sleep 3  # Esperar a que exista el archivo
      while true; do
        wc -l /logs/events.log
        sleep 5
      done
    volumeMounts:
    - name: logs
      mountPath: /logs
      readOnly: true

  volumes:
  - name: logs
    emptyDir:
      sizeLimit: 100Mi
EOF

# Ver logs
kubectl logs log-processor -c producer
kubectl logs log-processor -c processor
----

==== Best Practices para Volúmenes

1. *Usa emptyDir para datos temporales*
   - Cache, buffers, workspace
   - Se elimina con Pod (OK)

2. *Evita hostPath en producción*
   - Seguridad: acceso sin restricciones
   - Portabilidad: vinculado a nodo específico
   - Usa PersistentVolumes si necesitas persistencia

3. *Monta Secrets como readOnly*
   - Protege credenciales
   - Previene escritura accidental

4. *Usa subPath para múltiples montes*
   - Claridad: qué va dónde
   - Flexibilidad: un volumen, múltiples usos

5. *Configura sizeLimit en emptyDir*
   - Previene agotamiento de disco
   - Evicta Pod automáticamente si lo excede

6. *Documenta propósito del volumen*
   - Qué datos almacena
   - Tiempo de vida esperado

7. *Usa medium: Memory para caches críticos*
   - Alto rendimiento
   - Requiere RAM disponible

8. *Valida permisos en volúmenes*
   - defaultMode para permisos
   - readOnly para datos inmutables

==== Troubleshooting de Volúmenes

**Problema: Pod no monta volumen**

[source,bash]
----
# Ver descripción del Pod
kubectl describe pod my-pod

# Buscar errores en MountVolume
# Causas comunes:
# - ConfigMap o Secret no existe
# - mountPath ya existe (en algunas imágenes)
# - Permisos incorrectos

# Ver logs del Pod
kubectl logs my-pod
----

**Problema: Archivo en ConfigMap no aparece en volumen**

[source,bash]
----
# Verificar que ConfigMap existe
kubectl get configmap

# Ver contenido de ConfigMap
kubectl get configmap app-config -o yaml

# Verificar volumeMounts en Pod
kubectl get pod my-pod -o yaml | grep -A 10 volumeMounts

# Las claves del ConfigMap se convierten en archivos
# Clave "db.host" → archivo "db.host"
----

**Problema: Volumen lleno (sizeLimit excedido)**

[source,bash]
----
# Ver estado del Pod
kubectl describe pod my-pod

# Debería mostrar evento de evicción
# "Pod Evicted: emptyDir exceeds sizeLimit"

# Soluciones:
# 1. Aumentar sizeLimit
# 2. Reducir tamaño de datos
# 3. Usar different storage (PersistentVolume)
----

=== Persistent Volumes (PV)

**¿Qué es un PersistentVolume?**

Un PersistentVolume (PV) es una abstracción de almacenamiento a nivel de cluster. Representa un pedazo de almacenamiento en el cluster que ha sido aprovisionado por un administrador o por un provisioner dinámico.

**Características:**

- *Persistencia*: Los datos persisten después de que el Pod se elimina
- *Independencia*: Existen independientemente de los Pods
- *Recursos de cluster*: Como CPUs y memoria
- *Ciclo de vida*: Administrado por el cluster

**Diagrama: Ciclo de vida PV/PVC**

[source]
----
┌──────────┐
│ Admin    │ Provisiona almacenamiento
└─────┬────┘
      │
      ▼
┌──────────────────────────────┐
│  PersistentVolume (PV)       │ ← Storage físico (NFS, iSCSI, etc)
├──────────────────────────────┤ Estado: Available
│ 10GB, NFS, ReadWriteOnce     │
└─────┬────────────────────────┘
      │ Pod solicita almacenamiento
      ▼
┌──────────────────────────────┐
│ PersistentVolumeClaim (PVC)  │ Estado: Bound
├──────────────────────────────┤
│ 5GB, ReadWriteOnce           │ ← Bind a PV matching
└──────────────────────────────┘
      │
      ▼
┌──────────────────────────────┐
│ Pod usa volumen              │
├──────────────────────────────┤
│ volumeClaimName: my-pvc      │
└──────────────────────────────┘
----

==== Ciclo de vida de PersistentVolumes

**Estados de un PV:**

1. *Provisioning*: Creación del almacenamiento
   - Manual: Admin crea PV
   - Dinámico: Provisioner automático (via StorageClass)

2. *Binding*: PVC se vincula a PV
   - PVC solicita almacenamiento
   - Control loop encuentra PV matching
   - PVC y PV se vinculan mutuamente

3. *Using*: Pod usa el PVC
   - Pod se monta al PVC
   - Pod accede al almacenamiento

4. *Releasing*: PVC se elimina
   - Usuario elimina PVC
   - PV se desvincula (pero no desaparece)

5. *Reclaiming*: Reciclaje del PV
   - Según ReclaimPolicy
   - Delete, Retain, o Recycle

**YAML básico de PersistentVolume:**

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs-10g
spec:
  capacity:
    storage: 10Gi        # Tamaño total
  accessModes:
  - ReadWriteOnce        # Modo de acceso
  persistentVolumeReclaimPolicy: Retain  # Qué hacer al deletear PVC
  storageClassName: standard             # Clase de storage
  nfs:                   # Detalles del backend
    server: 192.168.1.100
    path: /exports/pv
----

==== Modos de Acceso (Access Modes)

Los modos de acceso especifican cómo se puede montar el volumen:

|===
| Modo | Descripción | Soporte

| ReadWriteOnce (RWO)
| Lectura/escritura por un solo nodo
| La mayoría de backends

| ReadOnlyMany (ROX)
| Solo lectura por múltiples nodos
| NFS, iSCSI, algunos cloud

| ReadWriteMany (RWX)
| Lectura/escritura por múltiples nodos
| NFS, algunos cloud

| ReadWriteOncePod (RWOP)
| Lectura/escritura por un solo Pod
| Algunos backends (v1.22+)
|===

**Implicaciones prácticas:**

- *RWO*: Base de datos (un servidor), ideal para bloque storage
- *ROX*: Distribución de código (múltiples readers)
- *RWX*: Datos compartidos, NFS típicamente
- *RWOP*: Más restrictivo que RWO, una pod exactamente

[source,bash]
----
# PV puede soportar múltiples modos
accessModes:
- ReadWriteOnce
- ReadOnlyMany

# PVC debe usar modos que PV soporta
# Si PV soporta RWO y ROX, PVC puede pedir RWO
----

==== Políticas de Reclaim (ReclaimPolicy)

Define qué ocurre con el almacenamiento cuando se elimina el PVC:

|===
| Política | Comportamiento | Uso

| Retain
| Mantener datos, PV no reutilizable
| Importante: manual cleanup

| Delete
| Eliminar almacenamiento automáticamente
| Cloud volumes, desarrollo

| Recycle (deprecated)
| Limpiar y reutilizar
| Legado, no usar
|===

**Ejemplo: Retain**

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-important-data
spec:
  capacity:
    storage: 100Gi
  persistentVolumeReclaimPolicy: Retain
  accessModes:
  - ReadWriteOnce
  # Los datos persisten después de eliminar PVC
  # Manual cleanup requerido
----

**Ejemplo: Delete**

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-elastic-block
spec:
  capacity:
    storage: 50Gi
  persistentVolumeReclaimPolicy: Delete
  accessModes:
  - ReadWriteOnce
  # Volumen se elimina automáticamente con PVC
  # Útil para cloud (EBS, GCE PD)
----

==== Storage Classes

Una StorageClass automatiza el aprovisionamiento de PersistentVolumes.

**¿Por qué Storage Classes?**

Sin Storage Classes: Admin debe crear PVs manualmente (tedioso)

Con Storage Classes: Sistema crea PVs automáticamente (dinámico)

**YAML básico:**

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs  # Driver que crea almacenamiento
parameters:
  type: gp3                          # Tipo de volumen
  iops: "3000"
  throughput: "125"
allowVolumeExpansion: true           # Permite crecer volumen
reclaimPolicy: Delete                # ReclaimPolicy por defecto
volumeBindingMode: WaitForFirstConsumer
----

**Provisioners populares:**

|===
| Provisioner | Cloud provider | Tipo

| kubernetes.io/aws-ebs
| AWS
| EBS volumes

| kubernetes.io/gce-pd
| Google Cloud
| Persistent Disks

| kubernetes.io/azure-disk
| Azure
| Managed Disks

| kubernetes.io/cinder
| OpenStack
| Cinder volumes

| nfs.io/nfs
| (cualquiera)
| NFS share

| hostpath.csi.k8s.io
| (local)
| Host path (testing)

| longhorn.io/longhorn
| (cualquiera)
| Distributed storage
|===

**Ejemplo: NFS StorageClass**

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage
provisioner: nfs.io/nfs
parameters:
  server: 192.168.1.100
  path: /exports
allowVolumeExpansion: true
----

**Ejemplo: Local Storage**

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-fast
provisioner: kubernetes.io/no-provisioner  # Manual PV creation
allowVolumeExpansion: false
volumeBindingMode: WaitForFirstConsumer    # Espera scheduler
----

==== PV Backend: Diferentes tipos de almacenamiento

**NFS (Network File System):**

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteMany      # NFS soporta múltiples accesos
  - ReadOnlyMany
  nfs:
    server: nfs-server.example.com
    path: /shared/data
----

**iSCSI (SCSI over network):**

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-iscsi
spec:
  capacity:
    storage: 100Gi
  accessModes:
  - ReadWriteOnce
  iscsi:
    targetPortal: iscsi.example.com:3260
    iqn: iqn.2019-12.com.example:storage
    lun: 0
----

**Local Storage (nodo específico):**

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-local
spec:
  capacity:
    storage: 50Gi
  accessModes:
  - ReadWriteOnce
  local:
    path: /mnt/fast-ssd
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node-1  # Vinculado a nodo específico
----

==== Verificación de PersistentVolumes

[source,bash]
----
# Ver todos los PVs
kubectl get pv

# Ver PV específico
kubectl get pv pv-name -o yaml

# Ver información detallada
kubectl describe pv pv-name

# Ver Storage Classes
kubectl get storageclass
----

==== Best Practices para PersistentVolumes

1. *Plan capacity adecuadamente*
   - Sobrestima ligeramente
   - Crecimiento futuro

2. *Usa appropriate access modes*
   - RWO para bases de datos
   - RWX para compartidos
   - ROX para solo lectura

3. *Selecciona reclaim policy cuidadosamente*
   - Retain para datos críticos
   - Delete para desarrollo
   - Documenta la decisión

4. *Usa Storage Classes dinámicas*
   - Automatiza provisioning
   - Consistencia en configuración

5. *Monitorea espacio disponible*
   - Alertas cuando se acerca límite
   - Plan para expansión

6. *Backup y recovery*
   - Snapshots si el backend lo soporta
   - Prueba restauración

7. *Documenta backend storage*
   - Qué tipo de almacenamiento
   - Performance características
   - Política de backup

8. *Valida access modes requeridos*
   - No todos los backends soportan todos los modos
   - Verifica antes de producción

**YAML básico de PVC:**

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: standard  # Referenciar StorageClass
  resources:
    requests:
      storage: 5Gi            # Solicitar 5GB
----

**Binding automático:**

Una vez creado PVC, Kubernetes busca un PV que cumpla:
- Suficiente capacidad (PVC pide 5Gi, PV debe tener al menos 5Gi)
- Access modes compatibles
- StorageClassName coincida

Si encuentra, vincula automáticamente.

**Estados de un PVC:**

- *Pending*: Esperando PV disponible
- *Bound*: Vinculado a un PV
- *Lost*: PV fue eliminado

**Ejemplo: Pod usando PVC**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: myapp:latest
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: my-pvc    # Referenciar PVC
----

==== Expansión de Volúmenes

Si configuraste `allowVolumeExpansion: true` en StorageClass, puedes expandir un PVC:

[source,bash]
----
# Expandir PVC de 5Gi a 10Gi
kubectl patch pvc my-pvc -p '{"spec":{"resources":{"requests":{"storage":"10Gi"}}}}'

# Ver estado
kubectl describe pvc my-pvc
----

**Limitaciones:**

- Solo expansión (no reducción)
- Requiere `allowVolumeExpansion: true` en StorageClass
- El backend debe soportar expansión

==== Snapshots de Volúmenes

Crear snapshots (capturas) de volúmenes para backup/restore.

**Requisitos:**

- StorageClass con snapshot support
- VolumeSnapshotClass configurada

**Crear snapshot:**

[source,yaml]
----
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: snapshot-pvc1
spec:
  volumeSnapshotClassName: csi-snapshotter
  source:
    persistentVolumeClaimName: my-pvc
----

**Restaurar desde snapshot:**

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: restored-pvc
spec:
  storageClassName: standard
  accessModes:
  - ReadWriteOnce
  dataSource:
    name: snapshot-pvc1      # Snapshots como source
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  resources:
    requests:
      storage: 5Gi
----

==== Best Practices para PVC

1. *Solicita capacity apropiada*
   - Ni muy poco (evicción)
   - Ni muy mucho (desperdicio)

2. *Usa StorageClasses explícitamente*
   - Claridad
   - Control de comportamiento

3. *Monitorea uso de espacio*
   - Alertas cuando se acerca límite
   - Plan para expansión

4. *Documenta propósito*
   - Qué aplicación usa PVC
   - Datos críticos o no

5. *Implementa backups*
   - Snapshots si backend soporta
   - Exportación de datos

=== Dynamic Provisioning

**¿Qué es Dynamic Provisioning?**

Dynamic Provisioning crea PersistentVolumes automáticamente cuando se crea un PVC, en lugar de requerir creación manual por admin.

**Flujo:**

1. Admin crea StorageClass con provisioner
2. User crea PVC que referencia StorageClass
3. Control plane detecta PVC sin PV matching
4. Provisioner crea almacenamiento automáticamente
5. Control plane crea PV que representa el almacenamiento
6. PVC se vincula a PV

**Ventajas:**

- Automatización
- Escalabilidad
- Consistencia
- Elástico (crear/eliminar según demanda)

**Requisitos:**

- StorageClass con provisioner válido
- Provisioner debe estar instalado/configurado

==== Configurando Dynamic Provisioning

**StorageClass con aprovisionamiento dinámico:**

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: aws-ebs-gp3
provisioner: ebs.csi.aws.com  # Driver AWS EBS
allowVolumeExpansion: true
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
  encrypted: "true"
  kms_key_id: arn:aws:kms:...
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
----

**volumeBindingMode opciones:**

- *Immediate*: Bind apenas se crea PVC (defecto)
- *WaitForFirstConsumer*: Espera hasta que Pod consume

WaitForFirstConsumer es más inteligente para considerar localidad de nodo.

**Parámetros específicos del provisioner:**

Cada provisioner tiene sus propios parámetros:

[source,yaml]
----
# AWS EBS
parameters:
  type: gp3
  iops: "3000"

# Google Cloud PD
parameters:
  type: pd-ssd
  replication-type: regional-pd

# Azure Disk
parameters:
  storageaccounttype: Premium_LRS
  kind: Managed
----

**PVC automáticamente aprovisionada:**

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-data
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: aws-ebs-gp3   # Referencia StorageClass
  resources:
    requests:
      storage: 100Gi
----

Automáticamente:
1. Provisioner crea volumen EBS de 100GB
2. Control plane crea PV para ese volumen
3. PVC se vincula al PV

**Verificar provisioning:**

[source,bash]
----
# Ver PVC
kubectl get pvc app-data

# Ver PV creado automáticamente
kubectl get pv
# Debería haber nuevo PV vinculado a PVC

# Ver detalles
kubectl describe pvc app-data
----

==== Provisioners comunes

**AWS EBS:**

```bash
helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
helm install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver -n kube-system
```

**Google Cloud PD:**

```bash
gcloud container clusters update CLUSTER_NAME --enable-disk-csi-driver
```

**Azure Disk:**

```bash
helm repo add azuredisk-csi-driver https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/charts
helm install azuredisk-csi-driver azuredisk-csi-driver/azuredisk-csi-driver -n kube-system
```

**NFS (cualquier cloud):**

```bash
helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
helm install nfs-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
  --set nfs.server=192.168.1.100 \
  --set nfs.path=/exports
```

==== Best Practices para Dynamic Provisioning

1. *Define StorageClasses claras*
   - fast (SSD)
   - standard (HDD)
   - archive (slow + cheap)

2. *Usa WaitForFirstConsumer cuando sea posible*
   - Mejor localidad
   - Evita binding prematuro

3. *Documenta provisioners instalados*
   - Qué opciones soportan
   - Parámetros disponibles

4. *Configura reclaimPolicy apropiada*
   - Delete para desarrollo
   - Retain para producción

5. *Monitorea provisioner health*
   - Ver logs de provisioner
   - Alertas si no crea volúmenes

=== StatefulSets con Almacenamiento

**Volume Claim Templates:**

StatefulSets usa volumeClaimTemplates para crear un PVC por cada Pod.

[source,yaml]
----
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-db
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 50Gi
----

**Resultado:**

- mysql-db-0 → PVC: data-mysql-db-0 (50Gi)
- mysql-db-1 → PVC: data-mysql-db-1 (50Gi)
- mysql-db-2 → PVC: data-mysql-db-2 (50Gi)

**Ordenamiento de almacenamiento:**

StatefulSet espera a que cada Pod esté ready antes de crear el siguiente:

1. mysql-db-0 creado
2. PVC data-mysql-db-0 creado y bound
3. mysql-db-0 reaches Ready
4. mysql-db-1 creado
5. PVC data-mysql-db-1 creado y bound
6. mysql-db-1 reaches Ready
7. ... (mysql-db-2)

**Persistencia en aplicaciones stateful:**

Los datos persisten incluso si el Pod se elimina:

```bash
# Pod muere
kubectl delete pod mysql-db-0

# StatefulSet lo recrea
# Nuevo Pod mysql-db-0 se monta a PVC data-mysql-db-0
# Datos intactos
```

**Eliminación de StatefulSet:**

```bash
# Eliminar StatefulSet pero mantener Pods
kubectl delete statefulset mysql-db --cascade=orphan

# Eliminar StatefulSet y Pods (pero no PVCs)
kubectl delete statefulset mysql-db

# PVCs persisten (datos no se pierden)
kubectl get pvc
# data-mysql-db-0
# data-mysql-db-1
# data-mysql-db-2
```

**Ejemplo completo: PostgreSQL con StatefulSet**

[source,bash]
----
kubectl apply -f - <<EOF
---
apiVersion: v1
kind: Service
metadata:
  name: postgresql
spec:
  clusterIP: None
  selector:
    app: postgresql
  ports:
  - port: 5432
    targetPort: 5432
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgresql
spec:
  serviceName: postgresql
  replicas: 3
  selector:
    matchLabels:
      app: postgresql
  template:
    metadata:
      labels:
        app: postgresql
    spec:
      containers:
      - name: postgresql
        image: postgres:14
        env:
        - name: POSTGRES_PASSWORD
          value: password
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
          subPath: postgres
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: standard
      resources:
        requests:
          storage: 20Gi
EOF

# Ver StatefulSet
kubectl get statefulset

# Ver Pods con almacenamiento
kubectl get pods -o wide

# Ver PVCs creados
kubectl get pvc

# Conectar a base de datos
kubectl run postgres-client --image=postgres:14 -it --rm -- psql -h postgresql-0.postgresql -U postgres
----

==== Best Practices para StatefulSets + Almacenamiento

1. *Planifica correctamente volumeClaimTemplates*
   - Tamaño
   - StorageClass
   - Access modes

2. *Usa Headless Services*
   - Necesario para identidad estable
   - Permite acceso directo a Pods

3. *Implementa health checks*
   - readiness: base de datos funcional
   - liveness: proceso corriendo

4. *Backup automático*
   - Snapshots periódicos
   - Exportación de datos

5. *Documentación clara*
   - Qué datos almacena
   - Política de retención

== Módulo 6: Configuración y Secrets

=== ConfigMaps

**¿Qué es un ConfigMap?**

Un ConfigMap es un objeto que almacena datos de configuración no confidencial en pares clave-valor. Permite:

- *Separación de configuración de aplicación* (12-factor app)
- *Inyectar configuración en Pods* sin modificar imagen
- *Compartir configuración entre Pods*
- *Actualizar configuración sin redesplegar*

**Casos de uso:**

- Archivos de configuración (nginx.conf, app.properties)
- Parámetros de aplicación (conexión DB, URLs)
- Valores por ambiente (dev, staging, prod)
- Scripts de inicialización

**NO usar ConfigMaps para:**

- Datos sensibles (credenciales, tokens) → usar Secrets
- Datos grandes (>1MB) → usar persistencia
- Datos binarios → considerar PersistentVolumes

==== Creación de ConfigMaps

**Opción 1: Literales**

[source,bash]
----
kubectl create configmap app-config \
  --from-literal=database.host=localhost \
  --from-literal=database.port=5432 \
  --from-literal=app.name=myapp \
  --from-literal=app.version=1.0
----

**Opción 2: Desde archivo**

[source,bash]
----
# Crear archivo
echo "database.host=localhost" > config.properties
echo "database.port=5432" >> config.properties

# Crear ConfigMap
kubectl create configmap app-config --from-file=config.properties

# Desde directorio (todas los archivos)
kubectl create configmap app-config --from-file=./config/
----

**Opción 3: YAML declarativo**

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: default
data:
  database.host: localhost
  database.port: "5432"
  app.name: myapp
  app.version: "1.0"
  nginx.conf: |
    server {
      listen 80;
      server_name _;
      location / {
        proxy_pass http://backend:8080;
      }
    }
----

**Opción 4: Mezcla literal + archivo**

[source,bash]
----
kubectl create configmap app-config \
  --from-literal=env=production \
  --from-file=config.yml
----

==== Consumo en Pods

**Opción 1: Variables de entorno (individual)**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: myapp:latest
    env:
    - name: DB_HOST
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: database.host
    - name: DB_PORT
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: database.port
    - name: APP_NAME
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: app.name
----

**Opción 2: Variables de entorno (bulk)**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: myapp:latest
    envFrom:
    - configMapRef:
        name: app-config
----

Inyecta TODAS las claves como variables de entorno:
- `database.host` → `DATABASE_HOST` (automático conversion)
- `database.port` → `DATABASE_PORT`

**Opción 3: Archivos en volumen**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:latest
    volumeMounts:
    - name: config
      mountPath: /etc/nginx/conf.d
  volumes:
  - name: config
    configMap:
      name: app-config
      items:
      - key: nginx.conf
        path: default.conf  # Renombrar
----

Los archivos están disponibles en `/etc/nginx/conf.d/default.conf`.

**Opción 4: Command-line arguments**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: myapp:latest
    args:
    - --database-host=$(DB_HOST)
    - --database-port=$(DB_PORT)
    - --app-name=$(APP_NAME)
    env:
    - name: DB_HOST
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: database.host
    - name: DB_PORT
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: database.port
    - name: APP_NAME
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: app.name
----

==== Actualización de ConfigMaps

**Problema: Cambios no se propagan automáticamente**

Si cambias ConfigMap, Pods existentes NO se actualizan automáticamente:

[source,bash]
----
# Actualizar ConfigMap
kubectl edit configmap app-config
# O
kubectl patch configmap app-config -p '{"data":{"app.name":"newname"}}'

# Pods existentes NO ven cambios (siguen inyectando datos viejos)
# Solo Pods nuevos ven nuevos valores
----

**Soluciones:**

1. *Reiniciar Pods* (fuerza reinyección):
```bash
kubectl rollout restart deployment/app
```

2. *Usar volumes* (más dinámico):
   - ConfigMaps montados como volumen se actualizan automáticamente
   - Cambios se propagan en segundos

3. *Reloader* (herramienta):
   - Monitorea cambios en ConfigMaps
   - Reinicia Pods automáticamente

==== Ejemplo completo: Multi-file ConfigMap

[source,bash]
----
# Crear archivos de configuración
mkdir config
cat > config/app.properties <<EOF
server.port=8080
app.name=myapp
database.url=jdbc:mysql://db:3306/mydb
EOF

cat > config/logging.properties <<EOF
log.level=INFO
log.format=json
EOF

# Crear ConfigMap
kubectl create configmap app-config \
  --from-file=config/app.properties \
  --from-file=config/logging.properties

# Ver ConfigMap
kubectl get configmap app-config -o yaml

# Usar en Pod
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: myapp:latest
    volumeMounts:
    - name: config
      mountPath: /etc/config
  volumes:
  - name: config
    configMap:
      name: app-config
EOF

# Acceder desde Pod
kubectl exec -it app -- cat /etc/config/app.properties
----

==== Best Practices para ConfigMaps

1. *Separación por ambiente*
   - configmap-dev, configmap-prod
   - Diferentes valores por entorno

2. *Versionado de ConfigMaps*
   - configmap-v1, configmap-v2
   - Fácil rollback

3. *Documentación clara*
   - Propósito de cada clave
   - Formato esperado

4. *Limitar tamaño*
   - Máximo 1MB por ConfigMap
   - Múltiples ConfigMaps si necesitas más

5. *Usar volúmenes para archivos grandes*
   - Más eficiente que envFrom
   - Actualizaciones dinámicas

6. *No guardes secretos*
   - Nunca contraseñas en ConfigMaps
   - Usa Secrets para datos sensibles

7. *Válida valores*
   - Verifica formato (JSON, YAML)
   - Testing antes de producción

==== Secrets

**¿Qué es un Secret?**

Un Secret es similar a ConfigMap pero diseñado para datos sensibles (credenciales, tokens, llaves). Ofrece:

- *Encriptación en reposo* (opcional)
- *Control de acceso* (RBAC)
- *Protección en transmisión* (etcd encryption)
- *Montaje seguro* (permisos restringidos por defecto)

**Tipos de Secrets:**

|===
| Tipo | Propósito | Contenido

| Opaque
| Datos genéricos
| Cualquier base64

| kubernetes.io/service-account-token
| Token de Service Account
| Token JWT automático

| kubernetes.io/dockercfg
| Credenciales Docker (legacy)
| Archivo .dockercfg

| kubernetes.io/dockerconfigjson
| Credenciales Docker
| config.json (docker login)

| kubernetes.io/basic-auth
| Credenciales básicas
| username, password

| kubernetes.io/ssh-auth
| SSH keys
| ssh-privatekey

| kubernetes.io/tls
| Certificados TLS
| tls.crt, tls.key

| bootstrap.kubernetes.io/token
| Bootstrap token
| token, usado en kubeadm
|===

==== Creación de Secrets

**Opción 1: Literal**

[source,bash]
----
kubectl create secret generic db-credentials \
  --from-literal=username=admin \
  --from-literal=password=secretpassword
----

**Opción 2: Desde archivo**

[source,bash]
----
echo -n "admin" > username.txt
echo -n "secretpassword" > password.txt

kubectl create secret generic db-credentials \
  --from-file=username=username.txt \
  --from-file=password=password.txt
----

**Opción 3: YAML declarativo (base64)**

[source,bash]
----
# Codificar en base64
echo -n "admin" | base64       # YWRtaW4=
echo -n "secretpass" | base64  # c2VjcmV0cGFzcw==

# Crear Secret YAML
cat > secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: db-credentials
type: Opaque
data:
  username: YWRtaW4=
  password: c2VjcmV0cGFzcw==
EOF

kubectl apply -f secret.yaml
----

**Opción 4: Secret TLS**

[source,bash]
----
# Crear certificado (o usar existente)
openssl req -x509 -newkey rsa:4096 \
  -keyout tls.key -out tls.crt \
  -days 365 -nodes

# Crear Secret TLS
kubectl create secret tls tls-secret \
  --cert=tls.crt \
  --key=tls.key
----

**Opción 5: Docker Registry Secret**

[source,bash]
----
kubectl create secret docker-registry regcred \
  --docker-server=docker.io \
  --docker-username=myuser \
  --docker-password=mypassword \
  --docker-email=myuser@example.com
----

==== Codificación base64

**IMPORTANTE:** Base64 NO es encriptación, es solo codificación.

```bash
# Codificar
echo -n "password" | base64
# cGFzc3dvcmQ=

# Decodificar
echo -n "cGFzc3dvcmQ=" | base64 -d
# password
```

**NUNCA** guardes Secrets sin encriptación en control de versiones.

==== Consumo de Secrets

**Opción 1: Variables de entorno**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: myapp:latest
    env:
    - name: DB_USER
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: username
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: password
----

**Opción 2: Volumen (recomendado para secretos)**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: myapp:latest
    volumeMounts:
    - name: secrets
      mountPath: /etc/secrets
      readOnly: true  # Recomendado
  volumes:
  - name: secrets
    secret:
      secretName: db-credentials
      defaultMode: 0400  # Solo lectura
      items:
      - key: username
        path: db-user
      - key: password
        path: db-pass
----

Los archivos están en `/etc/secrets/db-user` y `/etc/secrets/db-pass`.

**Opción 3: Docker Registry (ImagePullSecrets)**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  imagePullSecrets:
  - name: regcred  # Secret Docker Registry
  containers:
  - name: app
    image: private-registry.com/myapp:latest
----

==== Buenas prácticas de seguridad

1. *Encriptación en reposo*
   - Habilitar `EncryptionConfiguration` en API server
   - Secretos encriptados en etcd

2. *RBAC sobre Secrets*
   - Limitar acceso a Secrets específicos
   - Solo aplicaciones que los necesitan

3. *Rotación de Secrets*
   - Cambiar contraseñas regularmente
   - Actualizar tokens vencidos

4. *Auditoría*
   - Loguear acceso a Secrets
   - Monitoreo de cambios

5. *No pushear a git*
   - Usar secrets operators
   - Sealed Secrets, Vault, etc

6. *Monta como volumen, no env**
   - Volumen es más seguro
   - Evita leaks en logs de ambiente

7. *readOnly en volumeMounts*
   - Previene modificación accidental
   - Buena defensa en profundidad

8. *Limpia Secrets no usados*
   - Reduce superficie de ataque
   - Auditoría simple

==== Encriptación en reposo

**Habilitar EncryptionConfiguration:**

[source,yaml]
----
# /etc/kubernetes/manifests/encryption-config.yaml
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
- resources:
  - secrets
  providers:
  - aescbc:
      keys:
      - name: key1
        secret: <BASE64_32_BYTES>  # 32 bytes base64
  - identity: {}
----

**Luego editar kube-apiserver:**

```bash
--encryption-provider-config=/etc/kubernetes/manifests/encryption-config.yaml
```

=== Gestión de Variables de Entorno

**¿Cuándo usar qué?**

|===
| Método | Cuándo usar | Ventajas | Desventajas

| Env literal
| Valores simples
| Simple, directo
| No flexible

| ConfigMap env
| Configuración compartida
| Reutilizable
| Requiere reiniciar Pod para cambios

| ConfigMap volumen
| Archivos de config
| Actualizaciones dinámicas
| Más complejo

| Secret env
| Credenciales simples
| Simple
| Seguridad menor que volumen

| Secret volumen
| Credenciales importantes
| Más seguro
| Más setup

| Downward API
| Información del Pod
| Dinámico, no requiere config
| Solo datos del Pod

|===

==== EnvFrom: Inyección bulk

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: myapp:latest
    envFrom:
    # Inyectar ConfigMap completo
    - configMapRef:
        name: app-config
    # Inyectar Secret completo
    - secretRef:
        name: db-credentials
    # Prefijo opcional
    - configMapRef:
        name: cache-config
        prefix: CACHE_
----

Resultados:
- Claves de app-config se convierten en env vars
- Claves de db-credentials se inyectan
- cache-config claves con prefijo `CACHE_`

**Reglas de conversión:**

- `database.host` → `DATABASE_HOST`
- `app-name` → `APP_NAME`
- Puntos y guiones se convierten a underscores
- Se convierten a mayúsculas

==== Downward API: Información del Pod

Inyectar información del Pod sin ConfigMaps/Secrets:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
  labels:
    app: myapp
    version: v1
spec:
  containers:
  - name: app
    image: myapp:latest
    env:
    # Información del Pod
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    # Labels del Pod
    - name: POD_LABELS
      valueFrom:
        fieldRef:
          fieldPath: metadata.labels['version']
    # Requests/Limits
    - name: MEMORY_LIMIT
      valueFrom:
        resourceFieldRef:
          containerName: app
          resource: limits.memory
    - name: CPU_REQUEST
      valueFrom:
        resourceFieldRef:
          containerName: app
          resource: requests.cpu
----

**Campos disponibles:**

- `metadata.name`: Nombre del Pod
- `metadata.namespace`: Namespace
- `metadata.uid`: UID único
- `metadata.labels['key']`: Valor de label
- `metadata.annotations['key']`: Valor de anotación
- `status.podIP`: IP del Pod
- `spec.nodeName`: Nombre del nodo

==== Downward API con volumen

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
  labels:
    app: myapp
  annotations:
    version: "1.0"
spec:
  containers:
  - name: app
    image: myapp:latest
    volumeMounts:
    - name: podinfo
      mountPath: /etc/podinfo
  volumes:
  - name: podinfo
    downwardAPI:
      items:
      - path: "name"
        fieldRef:
          fieldPath: metadata.name
      - path: "namespace"
        fieldRef:
          fieldPath: metadata.namespace
      - path: "labels"
        fieldRef:
          fieldPath: metadata.labels
      - path: "annotations"
        fieldRef:
          fieldPath: metadata.annotations
----

Archivos en `/etc/podinfo/`:
- `name`: Nombre del Pod
- `namespace`: Namespace del Pod
- `labels`: Todos los labels
- `annotations`: Todas las anotaciones

==== Best Practices para Configuración

1. *Usa ConfigMaps para no-secretos*
   - Configuración de aplicación
   - Valores que pueden cambiar por ambiente

2. *Usa Secrets para datos sensibles*
   - Contraseñas
   - Tokens
   - Claves privadas

3. *Monta volúmenes para actualizaciones dinámicas*
   - ConfigMaps en volumen
   - Se actualizan automáticamente

4. *Inyecta variables para info estática*
   - Nombre del Pod
   - Namespace
   - Labels/annotations

5. *Documenta cada variable*
   - Propósito
   - Valores esperados
   - Rango de valores

6. *Valida en aplicación*
   - Valores por defecto
   - Manejo de valores faltantes

7. *Separa por componente*
   - Web app config separada de DB config
   - Facilita reutilización

8. *Usa namespaces para aislamiento*
   - ConfigMaps/Secrets por namespace
   - Mejor control de acceso

== Módulo 7: Seguridad

=== Authentication y Authorization

==== Conceptos fundamentales

La seguridad en Kubernetes se basa en dos conceptos clave:

**Autenticación (Authentication)**: Verificar *quién* es el usuario/cliente
- ¿Es realmente quien dice ser?
- Basado en credenciales, certificados o tokens

**Autorización (Authorization)**: Verificar *qué* puede hacer
- ¿Tiene permiso para realizar esta acción?
- Basado en políticas (RBAC, ABAC, Webhook)

**Acceso a Kubernetes:**

[source]
----
Cliente → Autenticación → ¿Es válido? → Autorización → ¿Tiene permisos? → Acción ejecutada
                              ↓ No                            ↓ No
                           Rechazado                      Rechazado
----

==== Métodos de autenticación

**1. Certificados X.509**

Kubernetes usa certificados X.509 para autenticar clientes. El servidor API verifica la cadena de confianza.

Ejemplo: Cliente kubectl se autentica con certificado:

[source,bash]
----
# Ver certificado usado por kubectl
kubectl config view

# Generalmente en ~/.kube/config:
# - client-certificate: /path/to/client.crt
# - client-key: /path/to/client.key
# - certificate-authority: /path/to/ca.crt
----

El flujo de autenticación con certificados:

[source]
----
1. Cliente envía solicitud HTTPS con certificado
2. Servidor verifica:
   - El certificado está firmado por una CA confiable
   - El certificado no ha expirado
   - El CN (Common Name) se usa como nombre de usuario
3. Si todo es válido → Usuario autenticado
----

Crear usuario con certificado:

[source,bash]
----
# 1. Generar clave privada
openssl genrsa -out juan.key 2048

# 2. Crear Certificate Signing Request (CSR)
openssl req -new \
  -key juan.key \
  -out juan.csr \
  -subj "/CN=juan/O=developers"

# 3. Firmar con CA del cluster (normalmente hecho por admin)
openssl x509 -req \
  -in juan.csr \
  -CA /etc/kubernetes/pki/ca.crt \
  -CAkey /etc/kubernetes/pki/ca.key \
  -CAcreateserial \
  -out juan.crt \
  -days 365

# 4. Crear entrada en kubeconfig
kubectl config set-credentials juan \
  --client-certificate=juan.crt \
  --client-key=juan.key
----

**2. Bearer Tokens**

Tokens estáticos: Útiles para service accounts y automación

[source,bash]
----
# Token en header Authorization
curl -H "Authorization: Bearer <token>" https://kubernetes:6443/api/v1/namespaces

# Token en kubeconfig
kubectl config set-credentials myapp --token=<token>
----

Tokens de Service Account (automáticos):

[source,bash]
----
# Cada service account tiene un token automático
kubectl get serviceaccount myapp -o jsonpath='{.secrets[0].name}'

# Ver token
kubectl get secret myapp-token-xyz -o jsonpath='{.data.token}' | base64 -d
----

**3. OpenID Connect (OIDC)**

Integración con proveedores de identidad externos:

[source,yaml]
----
# Configuración en kube-apiserver
--oidc-issuer-url=https://accounts.google.com
--oidc-client-id=kubernetes.apps.googleusercontent.com
--oidc-username-claim=email
--oidc-groups-claim=groups
----

Ventajas:
- Gestión centralizada de identidades
- SSO (Single Sign-On)
- Integración con providers como Google, Okta, Azure

==== RBAC (Role-Based Access Control)

RBAC es el modelo de autorización recomendado. Define *qué recurso* puede *hacer qué* el usuario/grupo.

**Componentes RBAC:**

1. **Role**: Permisos dentro de un namespace
2. **ClusterRole**: Permisos a nivel de cluster
3. **RoleBinding**: Vincula Role a usuarios/grupos en un namespace
4. **ClusterRoleBinding**: Vincula ClusterRole a usuarios/grupos globalmente

**Creando un Role:**

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: default
rules:
# Regla 1: Permite leer (get, list, watch) Pods
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]

# Regla 2: Permite leer logs
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get"]

# Regla 3: Permite acceder a configmaps específicas
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["app-config"]
  verbs: ["get"]
----

Conceptos:
- `apiGroups`: Grupo de API ("" para core, "apps" para Deployments, etc.)
- `resources`: Qué recursos (pods, services, deployments, etc.)
- `verbs`: Qué acciones (get, list, create, delete, etc.)
- `resourceNames`: Opcional - específicas instancias

**Creando un ClusterRole:**

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-reader-all-namespaces
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list", "watch"]
----

La diferencia: `ClusterRole` se aplica a todo el cluster, `Role` solo a un namespace.

**RoleBinding: Asignando permisos**

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pod-reader
subjects:
# Usuario individual
- kind: User
  name: juan@example.com
  apiGroup: rbac.authorization.k8s.io

# Grupo de usuarios
- kind: Group
  name: developers
  apiGroup: rbac.authorization.k8s.io

# Service Account
- kind: ServiceAccount
  name: myapp
  namespace: default
----

**ClusterRoleBinding: Permisos globales**

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-nodes
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view
subjects:
- kind: Group
  name: monitoring
  apiGroup: rbac.authorization.k8s.io
----

**Verbos comunes en RBAC:**

|===
| Verbo | Descripción | Ejemplo
| get | Obtener recurso específico | kubectl get pod myapp
| list | Listar recursos | kubectl get pods
| watch | Observar cambios | kubectl get pods --watch
| create | Crear recurso | kubectl create deployment
| update | Actualizar recurso | kubectl set image
| patch | Modificar parcialmente | kubectl patch
| delete | Eliminar recurso | kubectl delete pod
| exec | Ejecutar comando en contenedor | kubectl exec
| port-forward | Redireccionar puertos | kubectl port-forward
| logs | Ver logs | kubectl logs
|===

**Predicados de recurso (Resource Names):**

Limitar acceso a recursos específicos:

[source,yaml]
----
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "delete"]
  # Solo el pod "production-app"
  resourceNames: ["production-app"]
----

**Verbos especiales (escalables):**

[source,yaml]
----
# Los términos escalables (scale) requieren verbos especiales
- apiGroups: ["apps"]
  resources: ["deployments/scale"]
  verbs: ["get", "create", "update"]

# Acceso a status
- apiGroups: ["apps"]
  resources: ["deployments/status"]
  verbs: ["get"]
----

**Roles predefinidas en Kubernetes:**

[source,bash]
----
# Ver roles predefinidas
kubectl get clusterroles | grep -E "system:|cluster"

# Principales:
# - view: Leer la mayoría de recursos (excepto RBAC y secrets)
# - edit: Crear, actualizar, eliminar recursos de aplicación
# - admin: Control total en namespace
# - cluster-admin: Control total del cluster
----

==== Mejores prácticas de autenticación y autorización

1. *Usar RBAC en lugar de AllowAll*
   - Denegar por defecto, permitir explícitamente
   - Menos riesgo de acceso no autorizado

2. *Principio de menor privilegio (Least Privilege)*
   - Solo permisos necesarios
   - Revisar regularmente accesos

3. *Auditar acceso a recursos sensibles*
   - Secrets, RBAC itself, kubelet
   - Logs de auditoría en API server

4. *Usar grupos para gestión de usuarios*
   - Cambios en un solo lugar
   - Escalable para muchos usuarios

5. *Separar responsabilidades*
   - Desarrolladores: pods, deployments, logs
   - DevOps: nodes, persistent storage
   - Seguridad: RBAC, secrets, policies

6. *Rotación periódica de certificados*
   - Certificados no deben ser eternos
   - Automatizar con herramientas

7. *Usar OpenID Connect para clusters grandes*
   - Mejor manejo de identidades
   - Integración con Active Directory, etc.

8. *Monitorear intentos fallidos de autenticación*
   - Logs de auditoría
   - Alertas sobre patrones sospechosos

=== Service Accounts

==== ¿Qué es un Service Account?

Un Service Account es una identidad de Kubernetes para procesos ejecutando en Pods. Permite:

- *Identificación de Pods* dentro del cluster
- *Autenticación automática* con API server
- *Control de acceso* via RBAC
- *Montaje automático* de tokens

Un Service Account = Usuario para aplicaciones (diferente de Usuarios para humanos)

Ejemplo conceptual:

[source]
----
Aplicación en Pod → Service Account → Token JWT → Acceso a API

Cada Pod obtiene automáticamente:
- Token en /var/run/secrets/kubernetes.io/serviceaccount/token
- CA certificate en /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
- Namespace en /var/run/secrets/kubernetes.io/serviceaccount/namespace
----

==== Creación y gestión de Service Accounts

**Crear Service Account:**

[source,bash]
----
# Método 1: Con kubectl
kubectl create serviceaccount myapp

# Método 2: Declarativo
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: myapp
  namespace: default
EOF

# Listar service accounts
kubectl get serviceaccounts
kubectl describe sa myapp
----

**YAML completo con opciones:**

[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: myapp
  namespace: default
  annotations:
    # Metadatos opcionales
    description: "Service account para aplicación myapp"
automountServiceAccountToken: true
----

**Asignación a Pods:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  # Especificar qué Service Account usar
  serviceAccountName: myapp  # por defecto: "default"

  containers:
  - name: app
    image: myapp:latest

    # Token montado automáticamente en:
    # /var/run/secrets/kubernetes.io/serviceaccount/
----

Dentro del contenedor:

[source,bash]
----
# Token disponible en
cat /var/run/secrets/kubernetes.io/serviceaccount/token

# CA certificate
cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

# Namespace actual
cat /var/run/secrets/kubernetes.io/serviceaccount/namespace
----

==== Token de autenticación

**Estructura del Token:**

El token es un JWT (JSON Web Token) con tres partes:

[source]
----
Token = [Header].[Payload].[Signature]

Header: {"alg":"RS256","typ":"JWT"}
Payload: {
  "iss": "kubernetes/serviceaccount",
  "kubernetes.io/serviceaccount/namespace": "default",
  "kubernetes.io/serviceaccount/secret.name": "myapp-token-abc123",
  "kubernetes.io/serviceaccount/service-account.name": "myapp",
  "sub": "system:serviceaccount:default:myapp"
}
Signature: Firmado con clave privada del cluster
----

**Obtener token actual:**

[source,bash]
----
# Desde dentro de un Pod
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)

# Usar token para hacer requests
curl -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  https://kubernetes.default.svc.cluster.local/api/v1/namespaces/default/pods

# Decodificar token (sin verificar firma)
echo $TOKEN | cut -d. -f2 | base64 -d | jq .
----

**Desde fuera del cluster:**

[source,bash]
----
# Ver secreto del token
kubectl get secret <service-account>-token-xyz -o jsonpath='{.data.token}' | base64 -d
----

==== Montaje automático (automountServiceAccountToken)

**Comportamiento por defecto:**

- Todo Pod obtiene automáticamente el token del Service Account
- Se monta como volumen en `/var/run/secrets/kubernetes.io/serviceaccount/`

**Deshabilitar montaje automático:**

A nivel de Pod:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  # Deshabilitarn automountaje para este Pod
  automountServiceAccountToken: false

  containers:
  - name: app
    image: myapp:latest
----

A nivel de Service Account:

[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: myapp
# Deshabilitarn automountaje por defecto para esta SA
automountServiceAccountToken: false
----

Pod puede sobreescribir esta configuración.

**¿Cuándo deshabilitar?**

- Pods que no necesitan acceder a API server
- Por seguridad (minimizar exposición de token)
- Pods que usan otro mecanismo de autenticación

==== RBAC para Service Accounts

**Vincular Role a Service Account:**

[source,yaml]
----
# 1. Definir Role con permisos
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: default
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list"]
---
# 2. Vincular con RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pod-reader
subjects:
# Usar Service Account como subject
- kind: ServiceAccount
  name: myapp
  namespace: default
---
# 3. Pod usa Service Account
apiVersion: v1
kind: Pod
metadata:
  name: monitor-app
spec:
  serviceAccountName: myapp
  containers:
  - name: app
    image: python:3.11
    command:
    - python
    - -c
    - |
      import os
      import json
      import requests

      # Leer credenciales montadas
      token = open('/var/run/secrets/kubernetes.io/serviceaccount/token').read()
      ca = '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'

      # Hacer request a API (con permisos definidos en Role)
      headers = {'Authorization': f'Bearer {token}'}
      url = 'https://kubernetes.default/api/v1/namespaces/default/pods'

      resp = requests.get(url, headers=headers, verify=ca)
      print(json.dumps(resp.json(), indent=2))
----

**Dar permisos a múltiples Service Accounts:**

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: app-readers
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pod-reader
subjects:
# Múltiples service accounts
- kind: ServiceAccount
  name: monitor-app
  namespace: default
- kind: ServiceAccount
  name: logging-app
  namespace: default
# También puedes incluir usuarios y grupos
- kind: User
  name: alice@example.com
  apiGroup: rbac.authorization.k8s.io
----

**Service Accounts a nivel de Cluster (ClusterRoleBinding):**

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: view-all-namespaces
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view
subjects:
- kind: ServiceAccount
  name: monitoring-app
  namespace: monitoring
----

==== Caso de uso: Aplicación que accede a API

Ejemplo completo: Aplicación que lista Pods del cluster

[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pod-lister
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: list-pods
  namespace: default
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: list-pods-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: list-pods
subjects:
- kind: ServiceAccount
  name: pod-lister
  namespace: default
---
apiVersion: v1
kind: Deployment
metadata:
  name: pod-monitor
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-monitor
  template:
    metadata:
      labels:
        app: pod-monitor
    spec:
      serviceAccountName: pod-lister
      containers:
      - name: monitor
        image: python:3.11
        env:
        - name: KUBERNETES_SERVICE_HOST
          value: "kubernetes.default"
        - name: KUBERNETES_SERVICE_PORT
          value: "443"
        command:
        - python
        - -c
        - |
          import os
          import sys
          import json
          from kubernetes import client, config

          # Cargar config desde ServiceAccount
          config.load_incluster_config()

          # Crear cliente
          v1 = client.CoreV1Api()

          # Listar Pods
          print("Pods en el namespace:")
          pods = v1.list_namespaced_pod(namespace='default')
          for pod in pods.items:
              print(f"  - {pod.metadata.name}")
----

==== Best Practices para Service Accounts

1. *Crear Service Account por aplicación*
   - No reutilizar entre aplicaciones
   - Facilita control de acceso granular

2. *Usar RBAC con least privilege*
   - Solo permisos necesarios
   - Revisar mensualmente

3. *Deshabilitar automountaje si no es necesario*
   - Reduce superficie de ataque
   - Pod toma decision consciente

4. *Rotar tokens regularmente*
   - Secrets vencen (configurable)
   - Automatizar rotación

5. *Auditar uso de Service Accounts*
   - Logs de auditoría
   - Alertas de uso inusual

6. *Usar namespaces para aislamiento*
   - Service Accounts por namespace
   - Mejor control de acceso

7. *No compartir tokens*
   - Cada proceso su propia SA
   - Evita sobreprivilegios

8. *Documentar permisos*
   - Qué puede hacer cada SA
   - Por qué necesita esos permisos

=== Security Context

==== ¿Qué es un Security Context?

Un Security Context define restricciones y privilegios de seguridad para:

- **Nivel de contenedor**: Afecta solo al contenedor especificado
- **Nivel de Pod**: Afecta a todos los contenedores del Pod (puede ser sobrescrito)

Define:
- Usuario y grupo que ejecuta el contenedor (UID/GID)
- Capacidades Linux (capabilities)
- SELinux y AppArmor
- Lectura solo para root filesystem
- Acceso privilegiado

Ejemplo conceptual:

[source]
----
┌─ Pod ────────────────────────────────┐
│  securityContext:                    │
│    runAsUser: 1000                   │
│    fsGroup: 2000                     │
│                                       │
│  ┌─ Contenedor 1 ────────────────┐  │
│  │ securityContext:              │  │
│  │   readOnlyRootFilesystem: ... │  │
│  │ (Hereda de Pod si no override)│  │
│  └───────────────────────────────┘  │
└──────────────────────────────────────┘
----

==== Configuración a nivel de Pod

**Ejecutar como usuario no-root:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: non-root-pod
spec:
  # Security context a nivel de Pod
  securityContext:
    runAsUser: 1000      # UID: 1000
    runAsGroup: 3000     # GID: 3000
    fsGroup: 2000        # GID para volúmenes

  containers:
  - name: app
    image: myapp:latest
    # El contenedor hereda estos valores
    # a menos que sobrescriba en securityContext propio
----

**Campos principales a nivel de Pod:**

|===
| Campo | Descripción | Valores
| runAsUser | UID que ejecuta contenedores | 0-65535 (0 = root)
| runAsGroup | GID principal | 0-65535
| fsGroup | GID para volúmenes | GID
| runAsNonRoot | Rechazar si rootUID | true/false
| seLinuxOptions | Contexto SELinux | {level, role, type}
| seccompProfile | Perfil seccomp | type, localhostProfile
| windowsOptions | Windows-only | runAsUsername
|===

**Ejecutar como no-root (seguro):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: secure-app
spec:
  securityContext:
    runAsNonRoot: true          # Rechazar si UID=0
    runAsUser: 1000             # Usuario myapp
    runAsGroup: 1000
    fsGroup: 1000

  containers:
  - name: app
    image: myapp:latest
    # Si imagen intenta ejecutar como root: FALLA
----

**Cambiar propiedad de volúmenes (fsGroup):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-volume
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 2000  # GID de archivos en volúmenes

  containers:
  - name: app
    image: myapp:latest
    volumeMounts:
    - name: data
      mountPath: /data

  volumes:
  - name: data
    emptyDir: {}

  # Resultado: /data es propiedad del grupo 2000
  # Contenedor (UID 1000, GID 2000) puede escribir
----

==== Configuración a nivel de Contenedor

**Sobrescribir securityContext del Pod:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: mixed-users
spec:
  securityContext:
    runAsUser: 1000

  containers:
  # Contenedor 1: Hereda runAsUser: 1000
  - name: app1
    image: myapp:latest

  # Contenedor 2: Sobrescribe
  - name: app2
    image: different-app:latest
    securityContext:
      runAsUser: 2000  # Diferente del Pod
      runAsNonRoot: true
----

**Contenedor con privilegios (peligroso):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: privileged-pod
spec:
  containers:
  - name: privileged-app
    image: privileged:latest
    securityContext:
      privileged: true  # Acceso completo al host

  # USAR SOLO PARA:
  # - Drivers de dispositivo
  # - Networking plugins
  # - Monitoreo de bajo nivel
  # NUNCA para aplicaciones normales
----

**ReadOnlyRootFilesystem:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: read-only-app
spec:
  containers:
  - name: app
    image: myapp:latest
    securityContext:
      readOnlyRootFilesystem: true  # / es solo lectura

    volumeMounts:
    # Necesario para directorios escribibles
    - name: tmp
      mountPath: /tmp
    - name: var-log
      mountPath: /var/log

  volumes:
  - name: tmp
    emptyDir: {}
  - name: var-log
    emptyDir: {}

  # Resultado: aplicación no puede modificar imagen
  # Solo puede escribir en volúmenes específicos
----

==== Capacidades Linux (Linux Capabilities)

Las capacidades dividen los permisos de root en unidades específicas.

**Sin capacidades (no-root):**

[source,yaml]
----
# Usuario normal (UID 1000) sin capacidades especiales
apiVersion: v1
kind: Pod
metadata:
  name: basic-app
spec:
  securityContext:
    runAsUser: 1000
    runAsNonRoot: true

  containers:
  - name: app
    image: myapp:latest
    # No puede:
    # - Escuchar puertos <1024
    # - Cambiar UID/GID
    # - Montar filesystems
    # - Hacer muchas cosas que requieren privilegios
----

**Agregar capacidades específicas:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: cap-net-bind-service
spec:
  securityContext:
    runAsUser: 1000
    runAsNonRoot: true

  containers:
  - name: app
    image: myapp:latest
    securityContext:
      capabilities:
        add:
        - NET_BIND_SERVICE  # Escuchar puertos <1024
        - NET_ADMIN         # Administración de red

  # Resultado: usuario 1000 puede escuchar puerto 80
----

**Remover capacidades (seguro):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: minimal-caps
spec:
  containers:
  - name: app
    image: myapp:latest
    securityContext:
      # Remover ALL capabilities
      capabilities:
        drop:
        - ALL

  # Resultado: contenedor tiene CERO capacidades
  # Solo puede acceder a recursos explícitamente montados
----

**Capacidades comunes:**

|===
| Capacidad | Permite
| NET_BIND_SERVICE | Escuchar puertos <1024
| NET_ADMIN | Configuración de red avanzada
| SYS_ADMIN | Muchas operaciones del sistema
| NET_RAW | Raw sockets (ping)
| CHOWN | Cambiar dueño de archivos
| DAC_OVERRIDE | Ignorar permisos de archivos
| SETUID/SETGID | Cambiar UID/GID
| SYS_PTRACE | Tracing de procesos
|===

==== RunAsUser y RunAsGroup

**Concepto:**

- `runAsUser`: UID del usuario que ejecuta
- `runAsGroup`: GID del grupo principal
- `fsGroup`: GID para archivos en volúmenes

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: user-group-example
spec:
  securityContext:
    runAsUser: 1000      # Proceso con UID 1000
    runAsGroup: 3000     # Grupo principal GID 3000
    fsGroup: 2000        # Archivos en volúmenes propiedad de GID 2000

  containers:
  - name: app
    image: myapp:latest
    volumeMounts:
    - name: config
      mountPath: /config

  volumes:
  - name: config
    configMap:
      name: app-config

  # Resultado:
  # ps aux: muestra UID 1000
  # groups: 1000 (primary), 3000 (suplementary)
  # ls /config: propiedad de root:2000
----

**Verificar dentro del contenedor:**

[source,bash]
----
# Dentro del contenedor
id        # uid=1000 gid=3000 groups=3000,2000

# Ver permisos de volumen
ls -l /config  # -rw-r--r-- root 2000

# Ver filesystem raíz
stat /  # Uid: ( 0/ root)   Gid: ( 0/ root)
----

==== SELinux y AppArmor

**SELinux (Security Enhanced Linux):**

Modelo de control de acceso mandatorio (MAC).

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: selinux-pod
spec:
  securityContext:
    seLinuxOptions:
      level: "s0:c123,c456"
      type: "container_t"
      role: "system_r"

  containers:
  - name: app
    image: myapp:latest
----

**AppArmor (solo Linux Kernel):**

Modelo MAC más simple que SELinux.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: apparmor-pod
  # Especificar perfil AppArmor
  annotations:
    container.apparmor.security.beta.kubernetes.io/app: localhost/myapp-profile

spec:
  containers:
  - name: app
    image: myapp:latest
----

**Ejemplo de perfil AppArmor (en worker node):**

[source]
----
# Archivo: /etc/apparmor.d/myapp-profile
#include <tunables/global>

profile myapp-profile flags=(attach_disconnected,mediate_deleted) {
  #include <abstractions/base>

  capability setuid,
  capability setgid,
  capability sys_admin,

  / r,
  /app/** rw,
  /tmp/** rw,

  # Denegar acceso a /root
  deny /root/** rwk,
}
----

==== Caso de uso: Aplicación segura

Aplicación con máxima seguridad:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: secure-app
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    seLinuxOptions:
      type: "container_t"
    seccompProfile:
      type: RuntimeDefault

  containers:
  - name: app
    image: myapp:latest

    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE

    volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: logs
      mountPath: /var/log
    - name: config
      mountPath: /etc/config
      readOnly: true

  volumes:
  - name: tmp
    emptyDir: {}
  - name: logs
    emptyDir: {}
  - name: config
    configMap:
      name: app-config
      defaultMode: 0444

  # Protecciones:
  # - No puede ser root
  # - No puede elevar privilegios
  # - / es solo lectura
  # - Sin capacidades (excepto NET_BIND_SERVICE)
  # - SELinux context restringido
----

==== Best Practices para Security Context

1. *Ejecutar como no-root siempre*
   - runAsNonRoot: true
   - Especificar UID explícitamente
   - Validar en Dockerfile

2. *Usar readOnlyRootFilesystem*
   - Protege contra modificaciones
   - Requiere volúmenes para /tmp, /var/log
   - Aumenta detectabilidad de intrusiones

3. *Remover todas las capacidades*
   - capabilities.drop: ["ALL"]
   - Agregar solo las necesarias
   - Menor superficie de ataque

4. *Denegar escalada de privilegios*
   - allowPrivilegeEscalation: false
   - Complementa runAsNonRoot

5. *Usar fsGroup para volúmenes*
   - Facilita permisos de volumen
   - Automático para pods multi-contenedor

6. *Evitar contenedores privilegiados*
   - privileged: true solo para drivers/plugins
   - Requiere justificación explícita
   - Auditar uso

7. *Usar seccomp profiles*
   - RuntimeDefault: disables syscalls peligrosos
   - Mejora de seguridad por défecto

8. *Validar en tiempo de admisión*
   - Pod Security Policies (deprecated)
   - Pod Security Standards (actual)
   - Admision controllers

=== Pod Security Standards

==== ¿Qué son Pod Security Standards?

Pod Security Standards (PSS) son políticas de seguridad built-in de Kubernetes que definen diferentes niveles de restricción:

Reemplazan a la deprecated Pod Security Policies (PSP).

**Tres niveles de restricción:**

|===
| Nivel | Propósito | Restricciones
| Privileged | Máxima compatibilidad | Ninguna restricción
| Baseline | Seguridad mínima | Evita vulnerabilidades conocidas
| Restricted | Seguridad máxima | Cumple hardening completo
|===

**Cómo funcionan:**

[source]
----
Pod manifest
    ↓
Pod Security Admission Controller (en API server)
    ↓
¿Qué nivel es el Pod?
    ├─ ¿Cumple Privileged? → Permitido
    ├─ ¿Cumple Baseline? → audit/warn (si está habilitado)
    └─ ¿Cumple Restricted? → enforce (si está habilitado)
----

==== Nivel Privileged

**Propósito:** Máxima compatibilidad, mínimas restricciones. Para pods de sistema/infraestructura.

**Restricciones:** Ninguna - permite todo

**Cuándo usar:**

- Plugins de red
- Drivers de dispositivo
- Logging/monitoring del sistema
- Operadores de infrastructure

**Ejemplo:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: privileged-system-pod
spec:
  containers:
  - name: privileged-app
    image: infra/network-plugin:latest
    securityContext:
      privileged: true

  # Sin restricciones - no necesita cumplir estándares
----

==== Nivel Baseline

**Propósito:** Prevenir vulnerabilidades conocidas. Aplicaciones normales.

**Restricciones (NO se permite):**

1. Contenedores privilegiados (`privileged: true`)
2. Acceso a host PID (`hostPID: true`)
3. Acceso a host IPC (`hostIPC: true`)
4. Escalar privilegios (`allowPrivilegeEscalation: true`)
5. Campos de kernel no root (`securityContext.seLinuxOptions`, `securityContext.windowsOptions`)
6. Capacidades no permitidas (solo AUDIT_WRITE, CHOWN, DAC_OVERRIDE, FOWNER, FSETID, KILL, NET_BIND_SERVICE, SETFCAP, SETGID, SETPCAP, SETUID, SYS_CHROOT)

**Campos requeridos:**

- `securityContext.allowPrivilegeEscalation: false` o
- `securityContext.capabilities.drop: ["ALL"]`

**Ejemplo válido (Baseline):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: baseline-app
spec:
  securityContext:
    runAsNonRoot: false  # Permitido en Baseline

  containers:
  - name: app
    image: myapp:latest
    securityContext:
      allowPrivilegeEscalation: false  # Required
      capabilities:
        drop:
        - NET_RAW
        # Otras capabilities están permitidas

  # ✓ Cumple Baseline
  # ✗ NO cumple Restricted (falta runAsNonRoot, readOnlyRootFilesystem)
----

**Ejemplo inválido (viola Baseline):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: baseline-fail
spec:
  containers:
  - name: app
    image: myapp:latest
    securityContext:
      privileged: true  # ✗ NO PERMITIDO en Baseline

  # ✗ Falla Baseline - rechazado con enforce=baseline
----

==== Nivel Restricted

**Propósito:** Máxima seguridad. Cumplir hardening completo.

**Restricciones (NO se permite):**

1. TODO lo prohibido en Baseline, más:
2. RunAsUser debe ser no-root
3. RunAsNonRoot debe ser true
4. Root filesystem debe ser read-only
5. Capabilities deben ser ALL dropped
6. No se permite escalada de privilegios
7. seccomp debe ser RuntimeDefault o Localhost

**Campos requeridos:**

```yaml
securityContext:
  runAsNonRoot: true
  runAsUser: <non-zero>
  readOnlyRootFilesystem: true
  seccompProfile:
    type: RuntimeDefault
  capabilities:
    drop:
    - ALL
containers[].securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
```

**Ejemplo válido (Restricted):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: restricted-app
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    readOnlyRootFilesystem: true
    seccompProfile:
      type: RuntimeDefault

  containers:
  - name: app
    image: myapp:latest

    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true

    volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: var-log
      mountPath: /var/log

  volumes:
  - name: tmp
    emptyDir: {}
  - name: var-log
    emptyDir: {}

  # ✓ Cumple Restricted
----

**Ejemplo con privilegios específicos (Restricted):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: restricted-with-caps
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    seccompProfile:
      type: RuntimeDefault

  containers:
  - name: app
    image: myapp:latest

    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE  # Solo si es necesario

      readOnlyRootFilesystem: true

  # ✓ Cumple Restricted (capacidad específica es OK)
----

==== Pod Security Admission

**Pod Security Admission Controller** aplica PSS automáticamente.

**Modos de operación:**

1. **enforce**: Rechaza Pods que violen el estándar
2. **audit**: Permite el Pod pero lo registra como violación (en audit logs)
3. **warn**: Permite el Pod pero muestra warning (en response headers)

**Configuración a nivel de Namespace:**

[source,yaml]
----
# Ejemplo: Enforcement de "restricted" en namespace
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
----

**Labels disponibles:**

```
pod-security.kubernetes.io/enforce: <level>
pod-security.kubernetes.io/enforce-version: <version>
pod-security.kubernetes.io/audit: <level>
pod-security.kubernetes.io/audit-version: <version>
pod-security.kubernetes.io/warn: <level>
pod-security.kubernetes.io/warn-version: <version>
```

Valores de `<level>`: `privileged`, `baseline`, `restricted`

**Ejemplo completo:**

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: app-namespace
  labels:
    # Enforce: Rechazar Pods que no cumplan Restricted
    pod-security.kubernetes.io/enforce: restricted
    # Audit: Registrar Pods que no cumplan Baseline
    pod-security.kubernetes.io/audit: baseline
    # Warn: Advertir sobre Pods que no cumplan Restricted
    pod-security.kubernetes.io/warn: restricted
---
# Este Pod será rechazado en enforce
apiVersion: v1
kind: Pod
metadata:
  name: insecure-app
  namespace: app-namespace
spec:
  containers:
  - name: app
    image: myapp:latest
    securityContext:
      privileged: true  # ✗ Violates Restricted → RECHAZADO

# Este Pod será permitido (pero auditado y advertido)
---
apiVersion: v1
kind: Pod
metadata:
  name: secure-app
  namespace: app-namespace
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
  containers:
  - name: app
    image: myapp:latest
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
  # ✓ Cumple Restricted → PERMITIDO
----

**Exemptions (Excepciones):**

Permitir Pods privilegiados en namespaces específicos:

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: system-namespace
  labels:
    pod-security.kubernetes.io/enforce: restricted
    # Excepciones para Pods del sistema
    pod-security.kubernetes.io/enforce-version: latest

  # Nota: Las excepciones se configuran en la política
  # (requiere configuración en kube-apiserver)
----

==== Configuración en kube-apiserver

Habilitar Pod Security Admission:

[source,bash]
----
# En /etc/kubernetes/manifests/kube-apiserver.yaml
spec:
  containers:
  - name: kube-apiserver
    command:
    - kube-apiserver
    # Habilitar plugin
    - --admission-control=PodSecurity
    # Configuración de default
    - --pod-security-policy-file=/etc/kubernetes/pss/policy.yaml
----

==== Mejor estrategia de implementación

**Fase 1: Awareness**

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    # Solo warn - sin enforcement
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/audit: baseline
----

**Fase 2: Audit**

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
----

**Fase 3: Enforcement**

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
----

==== Best Practices para Pod Security Standards

1. *Usar Restricted por defecto*
   - Máxima seguridad
   - Migraciones gradual si es necesario

2. *Implementar en fases*
   - Warn primero
   - Audit después
   - Enforce finalmente

3. *Documentar excepciones*
   - Por qué un Pod necesita Baseline/Privileged
   - Revisiones periódicas

4. *Validar en desarrollo*
   - Probar localmente con PSS
   - No esperar a production

5. *Auditar violaciones*
   - Ver audit logs
   - Identificar Pods problemáticos

6. *Gradual para clusters existentes*
   - Empezar con warn
   - Medir impacto
   - Incrementar restricciones

7. *Combinar con otros controles*
   - RBAC
   - Network Policies
   - Security Context

8. *Mantener actualizado*
   - Nuevas versiones de PSS
   - Nuevas restricciones de seguridad

=== Network Security

==== Network Policies Avanzadas

Network Policies ya se cubrieron en el Módulo 4. Aquí vemos configuraciones avanzadas y casos de uso.

**Recordatorio: Conceptos básicos**

Network Policies controlan tráfico de red entre Pods:
- Ingress: Tráfico entrante
- Egress: Tráfico saliente

**Política compleja: Multi-tier con aislamiento**

Scenario: Web → API → Database

[source,yaml]
----
# Namespace para la aplicación
apiVersion: v1
kind: Namespace
metadata:
  name: app
---
# 1. Denegar TODO por defecto (ingress)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: app
spec:
  podSelector: {}  # Aplica a todos los Pods
  policyTypes:
  - Ingress
---
# 2. Denegar TODO por defecto (egress)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-egress
  namespace: app
spec:
  podSelector: {}
  policyTypes:
  - Egress
---
# 3. Permitir: Ingress exterior → web
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-web-from-outside
  namespace: app
spec:
  podSelector:
    matchLabels:
      tier: web
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector: {}  # Cualquier namespace
    ports:
    - protocol: TCP
      port: 80
    - protocol: TCP
      port: 443
---
# 4. Permitir: Web → API
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-web-to-api
  namespace: app
spec:
  podSelector:
    matchLabels:
      tier: api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: web
    ports:
    - protocol: TCP
      port: 8080
---
# 5. Permitir: API → Database
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-api-to-db
  namespace: app
spec:
  podSelector:
    matchLabels:
      tier: database
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: api
    ports:
    - protocol: TCP
      port: 5432
---
# 6. Permitir: Web → Egress (a API)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: web-allow-outbound-to-api
  namespace: app
spec:
  podSelector:
    matchLabels:
      tier: web
  policyTypes:
  - Egress
  egress:
  # A pods API
  - to:
    - podSelector:
        matchLabels:
          tier: api
    ports:
    - protocol: TCP
      port: 8080
  # A DNS (necesario para resolución)
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
---
# 7. Permitir: API → Egress (a Database + DNS)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-allow-outbound-to-db
  namespace: app
spec:
  podSelector:
    matchLabels:
      tier: api
  policyTypes:
  - Egress
  egress:
  # A database
  - to:
    - podSelector:
        matchLabels:
          tier: database
    ports:
    - protocol: TCP
      port: 5432
  # A DNS
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
---
# 8. Permitir: Database → Egress (solo DNS)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-allow-outbound-dns
  namespace: app
spec:
  podSelector:
    matchLabels:
      tier: database
  policyTypes:
  - Egress
  egress:
  # Solo DNS
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
----

**IPBLOCK: Control por rango de IPs**

Permitir acceso solo desde subnet específica:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-corporate-network
  namespace: app
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - ipBlock:
        cidr: 10.0.0.0/8
        except:
        - 10.0.5.0/24  # Excepto esta subnet
    ports:
    - protocol: TCP
      port: 8080
----

**Política con múltiples condiciones (AND):**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: complex-policy
spec:
  podSelector:
    matchLabels:
      app: api
  ingress:
  - from:
    # TODAS estas condiciones deben cumplirse (AND lógico)
    - podSelector:
        matchLabels:
          role: frontend
      namespaceSelector:
        matchLabels:
          environment: production
    ports:
    - protocol: TCP
      port: 8080

  # Solo permitir conexiones de Pods etiquetados como frontend
  # en namespaces etiquetados como production
----

==== Seguridad de Servicios

**Service sin selector (acceso externo controlado):**

[source,yaml]
----
# Exposer servicio solo a ciertos clientes
apiVersion: v1
kind: Service
metadata:
  name: internal-api
spec:
  type: ClusterIP
  # Usar Network Policy para controlar acceso
  selector:
    app: api

---
# Endpoint manual para control específico
apiVersion: v1
kind: Endpoints
metadata:
  name: internal-api
subsets:
- addresses:
  - ip: 10.0.0.5  # IP específica
  ports:
  - port: 8080
----

**LoadBalancer con IP restringida (cloud-specific):**

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: api
spec:
  type: LoadBalancer
  selector:
    app: api
  ports:
  - port: 443
    targetPort: 8443
  # Restricción de IPs (AWS/GCP/Azure)
  loadBalancerSourceRanges:
  - 203.0.113.0/24  # Solo desde estos ranges

  # ExternalTrafficPolicy
  externalTrafficPolicy: Local
  # Mantiene IP del cliente (vs Cluster = NAT)
----

==== TLS/mTLS en comunicación Pod-to-Pod

**Necesidad:** Encriptar tráfico entre Pods

Opciones:
1. **Manual TLS**: Certificados en cada Pod
2. **Service Mesh**: Automático (Istio, Linkerd)

**Manual TLS (ejemplo básico):**

[source,yaml]
----
# 1. Crear certificados TLS
apiVersion: v1
kind: Secret
metadata:
  name: api-tls
type: kubernetes.io/tls
data:
  tls.crt: <base64-encoded-cert>
  tls.key: <base64-encoded-key>
---
# 2. Pod que expone TLS
apiVersion: v1
kind: Pod
metadata:
  name: api-server
spec:
  containers:
  - name: api
    image: api:latest
    volumeMounts:
    - name: tls
      mountPath: /etc/tls
      readOnly: true
    env:
    - name: TLS_CERT_FILE
      value: /etc/tls/tls.crt
    - name: TLS_KEY_FILE
      value: /etc/tls/tls.key
  volumes:
  - name: tls
    secret:
      secretName: api-tls
      defaultMode: 0400

  # Aplicación debe escuchar en HTTPS
----

==== Service Mesh (Introducción)

Un Service Mesh es una capa de infraestructura que maneja comunicación service-to-service.

**Problemas que resuelve:**

1. **Criptografía automática** (mTLS)
2. **Load balancing** inteligente
3. **Retry y timeout** automáticos
4. **Circuit breaking** para fallos
5. **Observabilidad** de tráfico
6. **Control de tráfico** granular

**Mesh más populares:**

|===
| Mesh | Característica | Complejidad
| Istio | Muy completo, control granular | Alta
| Linkerd | Simple, performante | Baja
| Cilium | Integrado con eBPF | Media
| Kuma | Agnóstico a infraestructura | Media
|===

**Istio: Ejemplo básico**

[source,yaml]
----
# 1. Habilitar sidecar injection en namespace
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    istio-injection: enabled
---
# 2. Definir VirtualService (enrutamiento)
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: api
  namespace: production
spec:
  hosts:
  - api
  http:
  - match:
    - uri:
      prefix: "/v1"
    route:
    - destination:
        host: api-v1
        port:
          number: 8080
  - match:
    - uri:
      prefix: "/v2"
    route:
    - destination:
        host: api-v2
        port:
          number: 8080
---
# 3. Definir DestinationRule (política de comunicación)
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: api
  namespace: production
spec:
  host: api
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 100
        http2MaxRequests: 1000
    loadBalancer:
      simple: ROUND_ROBIN
    outlierDetection:
      consecutive5xxErrors: 5
      interval: 30s
      baseEjectionTime: 30s
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2

  # Automático mTLS
----

**Linkerd: Más simple**

[source,bash]
----
# 1. Instalar Linkerd
linkerd install | kubectl apply -f -

# 2. Inyectar en namespace
kubectl annotate namespace production \
  linkerd.io/inject=enabled

# 3. Desplegar aplicación
kubectl apply -f app.yaml

# Resultado: Automático mTLS + observabilidad
----

==== Encriptación de tráfico

**Opciones de encriptación:**

1. **En Kubernetes:**
   - mTLS entre Pods (Service Mesh)
   - TLS para Services (Ingress, LoadBalancer)
   - Network encryption plugin (Cilium)

2. **En aplicación:**
   - HTTPS/TLS
   - Aplicación-level encryption

**Cilium: Encriptación automática**

[source,yaml]
----
# Cilium con encriptación IPSec
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: default-encryption
spec:
  # Encriptar todo tráfico
  nodeSelector:
    matchLabels: {}

  # Configurar en helm values
  # --set encryption.enabled=true
  # --set encryption.type=ipsec
----

==== Best Practices para Network Security

1. *Network Policy denegar por defecto*
   - `default-deny-ingress` y `default-deny-egress`
   - Permitir explícitamente lo necesario

2. *Usar pod selectors, no IPBLOCK*
   - Más mantenible
   - Dinámico (cambia con labels)

3. *Separar namespaces por seguridad*
   - Policies por namespace
   - Aislar workloads sensibles

4. *Incluir DNS en egress*
   - Port 53 UDP
   - Necesario para resolución

5. *Usar Service Mesh para aplicaciones críticas*
   - Automático mTLS
   - Observabilidad mejorada
   - Canary deployments

6. *TLS Ingress para tráfico externo*
   - Https obligatorio
   - Certificados válidos

7. *Auditar políticas de red*
   - Revisar mensualmente
   - Eliminar políticas obsoletas

8. *Combinar con otras capas*
   - RBAC (quién puede ver Policies)
   - Security Context (uid/gid)
   - Pod Security Standards

== Módulo 8: Observabilidad

=== Logging

==== ¿Por qué logging es crítico?

El logging captura eventos y errores de aplicaciones para:

- **Debugging**: Encontrar causa raíz de problemas
- **Auditoría**: Rastrear acciones de usuarios/sistemas
- **Compliance**: Cumplir requisitos regulatorios
- **Performance analysis**: Identificar cuellos de botella
- **Security**: Detectar intrusions y comportamiento anómalo

**Desafíos en Kubernetes:**

- Contenedores efímeros - logs desaparecen cuando muere Pod
- Múltiples nodos - logs distribuidos en el cluster
- Múltiples contenedores - coordinación de logs
- Volumen - aplicaciones generan miles de logs/segundo

==== Logs de Contenedores

**Ver logs de Pod:**

[source,bash]
----
# Logs del contenedor más reciente
kubectl logs pod-name

# Logs con formato más detallado
kubectl logs pod-name --all-containers=true

# Logs en tiempo real (follow)
kubectl logs pod-name -f

# Ver logs de contenedor específico (multi-container Pod)
kubectl logs pod-name -c container-name

# Ver logs de Pods anteriores (si se reinició)
kubectl logs pod-name --previous

# Últimas 100 líneas
kubectl logs pod-name --tail=100

# Desde los últimos 1 hora
kubectl logs pod-name --since=1h
----

**Logs de Deployment/StatefulSet:**

[source,bash]
----
# Logs de todos los Pods del Deployment
kubectl logs deployment/my-app

# Logs más recientes
kubectl logs deployment/my-app --tail=50 -f

# Con labels para filtrar
kubectl logs -l app=myapp -f
----

**Límite de logs en contenedor:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: myapp:latest

    # Redireccionar logs a archivo
    stdout: /var/log/app.log
    stderr: /var/log/app.log

  # El contenedor no debería escribir directamente a disco
  # Kubernetes maneja logs automáticamente
----

**Los logs están en:**

[source,bash]
----
# En el worker node
/var/log/pods/<namespace>_<pod-name>_<uid>/<container-name>/
/var/log/pods/default_myapp-abc123_uuid/myapp/
----

==== Estrategias de Logging

**1. Logging estándar (stdout/stderr)**

La mejor práctica - la aplicación escribe a stdout/stderr:

[source,python]
----
# Python - Logging a stdout
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    stream=sys.stdout  # IMPORTANTE: a stdout, no a archivo
)

logger = logging.getLogger(__name__)
logger.info("Aplicación iniciada")
----

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-logger
spec:
  containers:
  - name: app
    image: myapp:latest
    # Kubernetes automáticamente captura stdout/stderr
    # No requiere configuración adicional
----

Ventajas:
- Simple y eficiente
- Kubernetes lo maneja automáticamente
- Fácil de agregar (Fluentd, ELK, etc.)

**2. Sidecar logging (patrón ambassador)**

Contenedor adicional que recolecta y procesa logs:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-sidecar
spec:
  containers:
  # Contenedor principal
  - name: app
    image: myapp:latest
    volumeMounts:
    - name: log-volume
      mountPath: /var/log/app

  # Sidecar logging
  - name: log-processor
    image: fluentd:latest
    volumeMounts:
    - name: log-volume
      mountPath: /var/log/app

    # El sidecar procesa los logs de app
    # y los envía a sistema central

  volumes:
  - name: log-volume
    emptyDir: {}
----

Caso de uso:
- Múltiples contenedores en Pod
- Procesamiento específico de logs
- Enriquecimiento de logs con metadata

**3. Init container para logs históricos**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-log-init
spec:
  initContainers:
  - name: setup-logging
    image: alpine:latest
    command:
    - sh
    - -c
    - |
      mkdir -p /var/log/app
      chmod 777 /var/log/app

  containers:
  - name: app
    image: myapp:latest
    volumeMounts:
    - name: log-volume
      mountPath: /var/log/app

  volumes:
  - name: log-volume
    emptyDir: {}
----

==== Logs a nivel de Cluster

**Logs de componentes del sistema:**

[source,bash]
----
# Logs del kubelet (en worker nodes)
journalctl -u kubelet -f

# Logs del kube-apiserver
kubectl logs -n kube-system pod/kube-apiserver-node1

# Logs del kube-controller-manager
kubectl logs -n kube-system -l component=kube-controller-manager

# Logs del scheduler
kubectl logs -n kube-system -l component=kube-scheduler
----

**Audit logs (API server)**

Registra TODOS los requests al API server:

[source,bash]
----
# Ubicación (en control plane node)
/var/log/kubernetes/audit.log

# Contenido: quién, qué, cuándo, resultado
{
  "level": "RequestResponse",
  "auditID": "abc-123",
  "stage": "ResponseComplete",
  "requestObject": {...},
  "responseObject": {...},
  "user": {
    "username": "admin",
    "uid": "123"
  },
  "verb": "create",
  "objectRef": {
    "resource": "pods",
    "name": "myapp",
    "namespace": "default"
  },
  "sourceIPAddress": "192.168.1.1",
  "requestReceivedTimestamp": "2024-01-15T10:30:00Z"
}
----

**Eventos de Kubernetes:**

[source,bash]
----
# Ver eventos del cluster
kubectl get events

# Ver eventos con más detalle
kubectl describe pod myapp

# Ver eventos de un namespace específico
kubectl get events -n production

# Eventos ordenados por tiempo
kubectl get events --sort-by='.lastTimestamp'

# Eventos de los últimos 30 minutos
kubectl get events --field-selector involvedObject.name=myapp
----

==== Agregación de Logs

**Stack ELK (Elasticsearch, Logstash, Kibana) - mejor: EFK (Fluentd)**

Arquitectura:

[source]
----
┌─ Worker Node 1 ────────┐
│ Pod logs → Fluentd      │
└──────────┬──────────────┘
           │
           ├─────────────────┐
           │                 │
┌─ Worker Node 2 ────────┐  │
│ Pod logs → Fluentd      │  │
└──────────┬──────────────┘  │
           │                 │
           v                 v
    ┌────────────────┐
    │  Elasticsearch │ (almacenamiento)
    └────────┬───────┘
             │
          ┌──v──┐
          │Kibana│ (visualización)
          └──────┘
----

**Instalación con Helm:**

[source,bash]
----
# Agregar repositorio de elastic
helm repo add elastic https://helm.elastic.co
helm repo update

# 1. Instalar Elasticsearch
helm install elasticsearch elastic/elasticsearch \
  --namespace logging --create-namespace \
  --set replicas=3 \
  --set resources.requests.memory=1Gi

# 2. Instalar Kibana
helm install kibana elastic/kibana \
  --namespace logging \
  --set elasticsearch.hosts=elasticsearch:9200

# 3. Instalar Fluentd
helm install fluent-bit fluent/fluent-bit \
  --namespace logging \
  -f fluent-bit-values.yaml
----

**Configuración Fluentd:**

[source,yaml]
----
# DaemonSet Fluentd (corre en cada nodo)
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      serviceAccountName: fluentd
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: elasticsearch
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX
          value: logstash
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_FORMAT
          value: "true"

        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true

      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers

      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
----

**RBAC para Fluentd:**

[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: logging
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - namespaces
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fluentd
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: logging
----

==== Búsqueda de logs en Kibana

**Dashboard Kibana:**

```
1. Create Index Pattern: "logstash-*"
2. Discover tab: ver logs en tiempo real
3. Visualize: crear gráficos
4. Dashboard: combinar visualizaciones
```

**Búsquedas útiles:**

```
# Logs de un Pod específico
kubernetes.pod_name: "myapp*"

# Logs de error
level: ERROR

# Logs de un namespace
kubernetes.namespace_name: "production"

# Logs de un contenedor
kubernetes.container_name: "api"

# Logs en rango de tiempo
@timestamp: [2024-01-15T10:00:00Z TO 2024-01-15T11:00:00Z]

# Combinaciones (AND)
kubernetes.pod_name: "api*" AND level: ERROR
```

==== Rotación y retención de logs

**Rotación automática en kubelet:**

[source,bash]
----
# En /etc/kubernetes/kubelet/kubelet-config.yaml
logging:
  # Máximo tamaño de log
  containerLogMaxSize: 10Mi
  # Máximo número de archivos de log
  containerLogMaxFiles: 5
----

**Con logrotate (en worker nodes):**

[source,bash]
----
# /etc/logrotate.d/kubernetes
/var/log/pods/*/*/*.log {
    rotate 10
    daily
    maxage 30
    compress
    copytruncate
    missingok
    notifempty
}
----

**En sistema de agregación:**

Elasticsearch automáticamente rota por día:
- `logstash-2024-01-15`
- `logstash-2024-01-16`
- etc.

Política de retención: eliminar logs >30 días

[source,bash]
----
# Eliminar índices antiguos (con CLI)
curl -X DELETE "localhost:9200/logstash-2023-12-01"
----

==== Best Practices para Logging

1. *Logs a stdout/stderr*
   - Kubernetes lo captura automáticamente
   - Fácil de agregar
   - No escribir a disco

2. *Usar niveles de log apropiados*
   - ERROR: Errores críticos
   - WARN: Situaciones anómalas
   - INFO: Eventos importantes
   - DEBUG: Información detallada

3. *Estructurar logs (JSON)*
   - Más fácil de parsear
   - Mejor para búsqueda

   [source,json]
   ----
   {"timestamp": "2024-01-15T10:30:00Z", "level": "ERROR", "message": "DB connection failed", "service": "api", "pod": "api-xyz"}
   ----

4. *Agregar contexto a logs*
   - Pod name
   - Namespace
   - Container name
   - IP del cliente
   - Request ID

5. *No loguear datos sensibles*
   - Nunca: passwords, tokens, keys
   - Nunca: PII (personal info)
   - Usar redacción: `password=***`

6. *Monitorear volumen de logs*
   - Logs voluminosos afectan performance
   - Usar sampling si es necesario

7. *Retención basada en compliance*
   - Regulatorios (PCI-DSS, HIPAA)
   - Típicamente: 30, 90, o 365 días

8. *Centralizar logs del cluster*
   - ELK, EFK, Splunk, Datadog, etc.
   - Crítico para debugging distribuido

=== Monitoring

==== Métricas en Kubernetes

El monitoreo captura datos sobre:

- **Utilización de recursos**: CPU, memoria, disco
- **Salud de aplicación**: latencia, errores, throughput
- **Salud de cluster**: nodos disponibles, capacidad
- **Eventos**: crashes, restarts, errores

**Tipos de métricas:**

|===
| Métrica | Fuente | Uso
| CPU usage | kubelet | HPA, capacity planning
| Memory usage | kubelet | OOM prevention, resource limits
| Network I/O | CNI plugin | Bandwidth analysis
| Disk I/O | kubelet | Storage performance
| Application metrics | aplicación | Business metrics, SLA
| API latency | API server | Cluster health
|===

**Sistema de métricas en Kubernetes:**

[source]
----
┌──────────────────────────────────────────┐
│ Componentes generan métricas             │
│ - kubelet (CPU, memory)                  │
│ - API server (request latency)           │
│ - controller-manager (reconciliation)    │
└───────────────┬──────────────────────────┘
                │
          ┌─────v──────┐
          │Metrics API │ (metrics.k8s.io)
          └─────┬──────┘
                │
       ┌────────┴─────────┐
       │                  │
   ┌───v──┐          ┌─────v──┐
   │ HPA  │          │Dashboards
   └──────┘          └────────┘
----

==== Metrics Server

Metrics Server proporciona metrics de CPU y memoria en tiempo real.

**Instalación:**

[source,bash]
----
# Ver si ya está instalado
kubectl get deployment metrics-server -n kube-system

# Instalar (minikube incluye por defecto)
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Verificar
kubectl get deployment metrics-server -n kube-system
kubectl top nodes
----

**Ver métricas de Nodos:**

[source,bash]
----
# CPU y memoria de todos los nodos
kubectl top nodes

# Salida:
# NAME           CPU(cores)   CPU%     MEMORY(Mi)   MEMORY%
# worker-node-1  500m         25%      2048Mi       50%
# worker-node-2  250m         12%      1024Mi       25%

# Nodo específico
kubectl top node worker-node-1
----

**Ver métricas de Pods:**

[source,bash]
----
# Todos los Pods en namespace
kubectl top pods

# Todos los Pods en todos los namespaces
kubectl top pods -A

# Pods ordenados por CPU
kubectl top pods --sort-by=cpu

# Pods ordenados por memoria
kubectl top pods --sort-by=memory

# Salida:
# NAME                    CPU(m)   MEMORY(Mi)
# api-deployment-abc123   200m     256Mi
# web-deployment-xyz789   100m     128Mi
----

**Integración con HPA:**

Metrics Server proporciona datos para Horizontal Pod Autoscaler:

[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 2
  maxReplicas: 10
  metrics:
  # Métrica de CPU (de Metrics Server)
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  # Métrica de memoria (de Metrics Server)
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
----

==== Prometheus y Grafana

**Prometheus**: Base de datos de time-series para métricas

**Grafana**: Visualización de métricas

**Arquitectura:**

[source]
----
┌─────────────────────────────────────────────┐
│ Prometheus Scraper                          │
│ - Kubelet metrics                           │
│ - API server metrics                        │
│ - Application metrics                       │
│ - Node exporter metrics                     │
└────────────┬────────────────────────────────┘
             │
      ┌──────v────────┐
      │ Prometheus DB │ (Time-series)
      └──────┬────────┘
             │
      ┌──────v──────┐
      │  Grafana    │ (Visualization)
      └─────────────┘
----

**Instalación con Helm:**

[source,bash]
----
# Agregar repositorio Prometheus
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Instalar Prometheus Stack
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring --create-namespace

# Acceder a Grafana
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80

# URL: http://localhost:3000
# Usuario/Password: admin/prom-operator (por defecto)
----

**Configuración Prometheus manual:**

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    scrape_configs:
    # Kubernetes API server
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
      - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

    # Kubelet metrics
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

    # Pods con anotación prometheus.io/scrape
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
----

**Exponer métricas de aplicación:**

La aplicación necesita exponer métricas en `/metrics`:

[source,python]
----
# Python con Prometheus client
from prometheus_client import Counter, Histogram, start_http_server
import time

# Definir métricas
request_count = Counter('app_requests_total', 'Total requests')
request_latency = Histogram('app_request_latency_seconds', 'Request latency')

# Iniciar servidor en puerto 8000
start_http_server(8000)

# En tu código
with request_latency.time():
    # Tu código aquí
    request_count.inc()
    time.sleep(0.1)
----

**Pod con métricas:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: metrics-app
  annotations:
    # Prometheus scrape automático
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
    prometheus.io/path: "/metrics"
spec:
  containers:
  - name: app
    image: myapp:latest
    ports:
    - name: metrics
      containerPort: 8000

    # Liveness probe para /metrics endpoint
    livenessProbe:
      httpGet:
        path: /metrics
        port: metrics
      initialDelaySeconds: 30
      periodSeconds: 10
----

**Dashboards Grafana útiles:**

1. **Cluster Overview**
   - Nodos disponibles
   - Utilización de CPU/Memoria
   - Pods corriendo

2. **Pod Performance**
   - CPU usage por Pod
   - Memory usage por Pod
   - Network I/O

3. **Application Metrics**
   - Requests/segundo
   - Latencia de requests
   - Error rate

==== Alerting

**AlertManager**: Componente que maneja alertas

**Flujo:**

[source]
----
Prometheus → Evalúa reglas → Alert disparado
                ↓
          AlertManager
                ↓
    ├─ Email
    ├─ Slack
    ├─ PagerDuty
    └─ Webhook
----

**Configuración de alertas:**

[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: app-alerts
  namespace: monitoring
spec:
  groups:
  - name: app.rules
    interval: 30s
    rules:
    # Alerta: CPU alto
    - alert: PodHighCPU
      expr: |
        sum(rate(container_cpu_usage_seconds_total[5m])) by (pod) > 0.8
      for: 5m
      annotations:
        summary: "Pod {{ $labels.pod }} CPU uso alto"
        description: "CPU > 80% por más de 5 minutos"

    # Alerta: Memoria alta
    - alert: PodHighMemory
      expr: |
        sum(container_memory_usage_bytes) by (pod) > 1073741824
      for: 5m
      annotations:
        summary: "Pod {{ $labels.pod }} memoria > 1Gi"

    # Alerta: Pod restartando
    - alert: PodRestarting
      expr: |
        rate(kube_pod_container_status_restarts_total[15m]) > 0.1
      for: 5m
      annotations:
        summary: "Pod {{ $labels.pod }} restartando frecuentemente"

    # Alerta: Nodo no disponible
    - alert: NodeNotReady
      expr: |
        kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      annotations:
        summary: "Nodo {{ $labels.node }} no está listo"
----

**Configuración AlertManager:**

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

    route:
      receiver: 'default'
      group_by: ['alertname', 'cluster']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 4h

      routes:
      # Critical alerts a PagerDuty
      - match:
          severity: critical
        receiver: pagerduty

      # Warnings a Slack
      - match:
          severity: warning
        receiver: slack

    receivers:
    - name: 'default'
      slack_configs:
      - channel: '#alerts'
        title: 'Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

    - name: 'pagerduty'
      pagerduty_configs:
      - service_key: 'YOUR-SERVICE-KEY'

    - name: 'slack'
      slack_configs:
      - channel: '#warnings'
        title: 'Warning: {{ .GroupLabels.alertname }}'
----

==== Custom Metrics

Métricas específicas de la aplicación (no CPU/Memory)

**Tipos:**

1. **Pod Metrics**: Desde aplicación
2. **External Metrics**: Desde sistema externo (cloud, etc.)
3. **Object Metrics**: De recursos específicos

**Custom Metrics en HPA:**

[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api
  minReplicas: 2
  maxReplicas: 20
  metrics:
  # Métrica custom: requests por segundo
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"

  # Métrica externa: queue length
  - type: External
    external:
      metric:
        name: queue_length
        selector:
          matchLabels:
            queue_name: requests
      target:
        type: AverageValue
        averageValue: "100"

  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
      selectPolicy: Max
----

**Adaptador de métricas custom:**

Prometheus adapter convierte Prometheus metrics en Custom Metrics:

[source,bash]
----
# Instalar Prometheus adapter
helm install prometheus-adapter prometheus-community/prometheus-adapter \
  --namespace monitoring

# Ver Custom Metrics disponibles
kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1 | jq .
----

==== Best Practices para Monitoring

1. *Metrics vitales del cluster*
   - CPU y memoria por nodo
   - Pods por nodo
   - Capacidad disponible

2. *Metrics vitales de aplicación*
   - Request rate
   - Latency (p50, p95, p99)
   - Error rate
   - Business metrics

3. *SLI/SLO*
   - SLI: Service Level Indicator (métrica medible)
   - SLO: Service Level Objective (meta)
   - Ejemplo: "99.9% de requests < 100ms"

4. *Alertas significativas*
   - No generar "alert fatigue"
   - Solo alertas que requieren acción
   - Documentar qué hacer en cada alerta

5. *Retención de métricas*
   - Prometheus: 15 días por defecto
   - Long-term storage: InfluxDB, Thanos
   - Ajustar según necesidad

6. *Instrumentación de aplicación*
   - Usar Prometheus client libraries
   - Métricas de negocio (orders, conversions)
   - Exponer en /metrics

7. *Monitoreo multi-layer*
   - Infraestructura: CPU, memory, disk
   - Kubernetes: Pod, nodo, cluster
   - Aplicación: requests, latency, errors

8. *Correlacionar logs y métricas*
   - Usar timestamps
   - Request IDs entre sistemas
   - Facilita debugging

=== Health Checks

==== ¿Qué son Health Checks?

Health checks permiten a Kubernetes determinar el estado de una aplicación:

- **¿Está corriendo?** (Liveness)
- **¿Está lista para recibir tráfico?** (Readiness)
- **¿Terminó de iniciarse?** (Startup)

Sin health checks, Kubernetes no sabe si un Pod está sano:

[source]
----
┌─ Pod ──────────────────┐
│ Aplicación iniciada    │ ← ¿Realmente está lista?
│ Pero no responde       │
└────────────────────────┘

Kubernetes asume: Pod running = Pod sano ✗
----

Con health checks:

[source]
----
┌─ Pod ──────────────────┐
│ Liveness: ¿Vivo?       │ → Restart si falla
│ Readiness: ¿Listo?     │ → Remover de LB si falla
│ Startup: ¿Iniciado?    │ → Esperar antes de chequear
└────────────────────────┘
----

==== Liveness Probes

Determina si el Pod debe ser reiniciado.

**¿Cuándo falla un liveness probe?**

- Aplicación está en deadlock
- Out of memory
- Proceso muerto pero kubelet no lo detectó
- Bug que causa hang

**Configuración básica (HTTP):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-liveness
spec:
  containers:
  - name: app
    image: myapp:latest
    ports:
    - containerPort: 8080

    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 30    # Esperar 30s antes de chequear
      periodSeconds: 10          # Chequear cada 10s
      timeoutSeconds: 5          # Timeout 5s
      failureThreshold: 3        # Reiniciar después de 3 fallos
      successThreshold: 1        # OK con 1 éxito
----

**Liveness con TCP:**

[source,yaml]
----
livenessProbe:
  tcpSocket:
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
  failureThreshold: 3

  # Util para aplicaciones que escuchan TCP
  # pero no exponen endpoint HTTP
----

**Liveness con Exec (comando):**

[source,yaml]
----
livenessProbe:
  exec:
    command:
    - /bin/sh
    - -c
    - |
      # Script customizado
      curl -f http://localhost:8080/health || exit 1

  initialDelaySeconds: 30
  periodSeconds: 10
  failureThreshold: 3

  # Útil para lógica compleja de healthcheck
----

**Ciclo de vida con Liveness:**

[source]
----
Pod inicia
    ↓
Esperan initialDelaySeconds (30s)
    ↓
Cada periodSeconds (10s):
    ├─ Chequear probe
    ├─ Si OK → seguir
    └─ Si FAIL:
        ├─ Contar fallo
        └─ Si failureThreshold (3) alcanzado:
            └─ RESTART Pod
----

==== Readiness Probes

Determina si el Pod está listo para recibir tráfico.

**¿Cuándo falla?**

- Aplicación iniciando (carga de datos, conexión a DB)
- Procesando migración de datos
- No puede conectar a dependencia
- En maintenance mode

**Diferencia key: Liveness reinicia, Readiness solo remueve del LB**

**Configuración (HTTP):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-readiness
spec:
  containers:
  - name: app
    image: myapp:latest
    ports:
    - containerPort: 8080

    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      initialDelaySeconds: 10    # Esperar menos que liveness
      periodSeconds: 5           # Chequear más frecuente
      timeoutSeconds: 3
      failureThreshold: 2
      successThreshold: 1

      # El /ready endpoint debe retornar:
      # - 200 OK: Listo
      # - 503 Service Unavailable: No listo
----

**Readiness con dependencias:**

[source,yaml]
----
readinessProbe:
  exec:
    command:
    - /bin/sh
    - -c
    - |
      # Verificar que DB está conectada
      timeout 3 /app/check_db_connection.sh || exit 1

      # Verificar que cache está cargado
      curl -f http://localhost:8080/cache/status || exit 1

  initialDelaySeconds: 5
  periodSeconds: 5
  failureThreshold: 2
----

**Impacto en Service:**

[source]
----
Service con 3 Pods
    ├─ Pod 1: Ready=true   → Incluida en endpoints
    ├─ Pod 2: Ready=false  → REMOVIDA de endpoints
    └─ Pod 3: Ready=true   → Incluida en endpoints

Resultado: Tráfico solo a Pods 1 y 3
----

==== Startup Probes

Para aplicaciones que toman mucho tiempo en iniciarse.

**¿Cuándo usar?**

- Aplicaciones legacy que tardan minutos en iniciar
- Servicios que cargan datos al arrancar
- Compilación en tiempo de inicio

**Sin Startup Probe:**

[source,yaml]
----
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  failureThreshold: 3
  periodSeconds: 10
  # Máximo tiempo para iniciar: 3 fallos × 10s = 30s
  # Si app tarda 60s → muere antes de iniciar
----

**Con Startup Probe:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: slow-startup-app
spec:
  containers:
  - name: app
    image: legacy-app:latest
    ports:
    - containerPort: 8080

    # Startup: esperar hasta 120s para iniciar
    startupProbe:
      httpGet:
        path: /health
        port: 8080
      failureThreshold: 12
      periodSeconds: 10
      # Máximo: 12 × 10 = 120 segundos

    # Una vez iniciado, liveness chequea cada 10s
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      periodSeconds: 10
      failureThreshold: 3

    # Readiness chequea antes de recibir tráfico
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      periodSeconds: 5
      failureThreshold: 2
----

**Timeline:**

[source]
----
0s:   Pod inicia, kubelet espera...
0-120s: Startup probe chequea cada 10s
        ├─ Fallos permidos: 12
        └─ Si todos fallan → Restart
120s: Startup éxito
      ├─ Liveness activa (chequea cada 10s)
      └─ Readiness activa (chequea cada 5s)
----

==== Tipos de Probes

**1. httpGet (HTTP GET request)**

[source,yaml]
----
livenessProbe:
  httpGet:
    scheme: HTTP          # HTTP o HTTPS
    host: localhost       # Host (default: Pod IP)
    port: 8080            # Puerto
    path: /healthz        # Path
    httpHeaders:
    - name: Authorization
      value: Bearer token123
  initialDelaySeconds: 15
  periodSeconds: 20
  timeoutSeconds: 5

  # Considerado exitoso: HTTP 200-399
----

**2. tcpSocket (TCP connection)**

[source,yaml]
----
readinessProbe:
  tcpSocket:
    host: localhost
    port: 5432
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 5

  # Considerado exitoso: conexión TCP exitosa
  # NO valida respuesta, solo conecta
----

**3. exec (Ejecutar comando)**

[source,yaml]
----
livenessProbe:
  exec:
    command:
    - /bin/bash
    - -c
    - |
      if [ "$(redis-cli ping)" = "PONG" ]; then
        exit 0
      else
        exit 1
      fi
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5

  # Considerado exitoso: exit code 0
----

==== Parámetros comunes

|===
| Parámetro | Descripción | Defecto
| initialDelaySeconds | Segundos antes de primer chequeo | 0
| periodSeconds | Intervalo entre chequeos | 10
| timeoutSeconds | Timeout del probe | 1
| failureThreshold | Fallos antes de acción | 3
| successThreshold | Éxitos para pasar de fallido a exitoso | 1
|===

**initialDelaySeconds por tipo:**

- **Liveness**: 30-60s (esperar a que estabilice)
- **Readiness**: 5-15s (debería estar listo rápido)
- **Startup**: 5s-5m (depende de tiempo de inicio)

==== Best Practices para Health Checks

1. *Implementar endpoints de health*
   - `/health` o `/healthz` para liveness
   - `/ready` para readiness
   - Retornar info útil: dependencias, versión

2. *Liveness conservador*
   - Solo restart si realmente muerto
   - NO chequear dependencias externas
   - Fallos frecuentes = crashes frecuentes

3. *Readiness es más estricto*
   - Chequear todas las dependencias
   - Incluir estado de caché/datos
   - Más fácil fallando que liveness

4. *Parámetros realistas*
   - initialDelaySeconds > tiempo real de startup
   - timeoutSeconds < 1s para fallos rápidos
   - periodSeconds: 10-30s normalmente

5. *Diferenciar liveness y readiness*
   - NO usar mismo endpoint para ambos
   - Liveness: ¿está vivo?
   - Readiness: ¿puede procesar tráfico?

6. *Startup probe para aplicaciones lentas*
   - Evita restarts durante startup
   - Permite tiempos de inicialización largos

7. *Monitorear probe failures*
   - Alertar si muchos restarts
   - Ver logs de probe failures
   - Ajustar parámetros si es necesario

8. *Ejemplo completo: aplicación real*

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      containers:
      - name: api
        image: mycompany/api:v1.2.3
        ports:
        - name: http
          containerPort: 8080
        - name: metrics
          containerPort: 8081

        # Startup: esperar máx 60s
        startupProbe:
          httpGet:
            path: /startup
            port: http
          periodSeconds: 5
          failureThreshold: 12

        # Liveness: reiniciar si muerto
        livenessProbe:
          httpGet:
            path: /health
            port: http
            httpHeaders:
            - name: X-Health-Check
              value: "true"
          initialDelaySeconds: 30
          periodSeconds: 10
          failureThreshold: 3
          timeoutSeconds: 5

        # Readiness: remover de LB si no listo
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 2
          timeoutSeconds: 3

        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
----

=== Resource Monitoring

==== Resource Requests y Limits

**Requests**: Cantidad mínima garantizada

**Limits**: Máximo permitido

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: resource-aware
spec:
  containers:
  - name: app
    image: myapp:latest

    resources:
      # Mínimo garantizado (requerido)
      requests:
        cpu: 100m        # 0.1 CPU cores
        memory: 128Mi    # 128 megabytes

      # Máximo permitido (opcional)
      limits:
        cpu: 500m        # 0.5 CPU cores
        memory: 512Mi    # 512 megabytes
----

**Impacto:**

|===
| Métrica | Uso | Impacto
| requests | Scheduling | Kubelet encuentra nodo con espacio disponible
| limits | Enforcement | Kernel mata proceso si excede (OOMKilled)
| requests | HPA | Usadas para calcular utilización %
| limits | Protection | Evita que 1 Pod monopolice recursos
|===

==== CPU y Memoria

**CPU:**

- Unidad: `m` (milicore) o decimal
- `100m` = 0.1 cores
- `1000m` = 1 core
- `1` = 1 core

[source,yaml]
----
# Equivalentes:
cpu: 100m    # 100 milicores
cpu: 0.1     # 0.1 cores (igual)

# Range típico:
requests:
  cpu: 50m - 500m   # Para aplicaciones típicas
limits:
  cpu: 500m - 2000m
----

**Memoria:**

- Unidad: `Mi` (mebibytes), `Gi` (gibibytes), `M` (megabytes)
- `128Mi` = 128 mebibytes (≈134.2 MB)
- `1Gi` = 1 gibibyte (≈1.07 GB)

[source,yaml]
----
# Equivalentes:
memory: 128Mi    # 128 mebibytes
memory: 134M     # ≈134 megabytes

# Range típico:
requests:
  memory: 64Mi - 512Mi   # Para aplicaciones típicas
limits:
  memory: 256Mi - 2Gi
----

**CPU vs Memoria:**

|===
| CPU | Memoria
| Comprimible | No comprimible
| No mata proceso | Mata proceso si exceed (OOMKilled)
| Se throttle si excede | Se termina si excede
| Busybait saca recursos | No se puede compartir fácilmente
|===

**CPU es throttleable - no falla, solo lento**

[source]
----
Pod A: requests 500m, limit 1000m
Pod B: requests 500m, limit 1000m

En nodo con 1 CPU:
├─ Ambos quieren 1000m (límite)
├─ CPU disponible: 1000m
├─ Ambos se throttlean:
│  ├─ Pod A obtiene ~600m
│  └─ Pod B obtiene ~400m
└─ Sin error, solo más lento
----

**Memoria NO es comprimible - falla**

[source]
----
Pod A: requests 256Mi, limit 512Mi
Pod B: requests 256Mi, limit 512Mi

En nodo con 512Mi RAM:
├─ Pod A: usa 480Mi
├─ Pod B: intenta usar 480Mi
└─ NO hay espacio → Pod B muere (OOMKilled)
----

==== QoS Classes

Kubernetes asigna QoS (Quality of Service) basado en requests/limits:

**Guaranteed: Máxima prioridad**

- request == limit para CPU y Memoria
- Nunca evicted si nodo sin presión
- Último en ser terminado

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: guaranteed-pod
spec:
  containers:
  - name: app
    image: myapp:latest
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 500m        # Igual a request
        memory: 512Mi    # Igual a request

  # Resultado: QoS = Guaranteed
----

**Burstable: Prioridad media**

- request < limit (para al menos una métrica)
- Evicted si nodo con presión y otros Pods necesitan espacio
- Intermedio

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: burstable-pod
spec:
  containers:
  - name: app
    image: myapp:latest
    resources:
      requests:
        cpu: 250m
        memory: 256Mi
      limits:
        cpu: 1000m       # Mayor que request
        memory: 1Gi      # Mayor que request

  # Resultado: QoS = Burstable
----

**BestEffort: Prioridad baja**

- Sin requests/limits
- Evicted primero si nodo con presión
- "Best effort" - sin garantías

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: besteffort-pod
spec:
  containers:
  - name: app
    image: myapp:latest
    # Sin resources especificada

  # Resultado: QoS = BestEffort
----

**Eviction order (cuando kubelet necesita liberar recursos):**

[source]
----
1. BestEffort Pods (sin requests/limits)
   ├─ Terminados primero
   └─ Sin advertencia

2. Burstable Pods (usando más que requests)
   ├─ Terminados segundo
   └─ Si excediendo su request

3. Guaranteed Pods (request == limit)
   └─ Nunca evicted (con presión de recurso)
----

==== Definir Requests y Limits

**Paso 1: Medir uso real**

[source,bash]
----
# Desplegar sin limits, medir por 1-2 semanas
kubectl deploy app

# Ver uso actual
kubectl top pod -l app=myapp --sort-by=memory

# Monitoreo con Prometheus
# Guardar datos históricos
----

**Paso 2: Set requests basado en P50**

[source,bash]
----
# Si P50 (mediana) de CPU es 200m
# Set request a: P50 × 1.2 (20% overhead)
requests.cpu: 250m

# Si P50 de memoria es 300Mi
# Set request a: P50 × 1.5 (50% overhead para safety)
requests.memory: 450Mi
----

**Paso 3: Set limits basado en P95**

[source,bash]
----
# Si P95 de CPU es 600m
# Set limit a: P95 × 1.5
limits.cpu: 900m

# Si P95 de memoria es 600Mi
# Set limit a: P95 × 1.2 (no mucho más para evitar OOM)
limits.memory: 720Mi
----

**Ejemplo real:**

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: api
        image: mycompany/api:v1

        # Basado en monitoreo de 2 semanas:
        # - P50 CPU: 250m, P95: 600m
        # - P50 Mem: 300Mi, P95: 500Mi

        resources:
          requests:
            cpu: 300m        # P50 × 1.2
            memory: 450Mi    # P50 × 1.5

          limits:
            cpu: 900m        # P95 × 1.5
            memory: 600Mi    # P95 × 1.2
----

**Nodos con límites de capacidad:**

[source,bash]
----
# Ver capacidad de nodo
kubectl describe node node-1

# Allocatable (disponible para Pods)
# = Capacity - Reserved
# = (24 CPUs - 4 reserved) × 1000 = 20000m

# Total requests en node < Allocatable
# Si: sum(requests) > Allocatable → Rechazo nuevo Pod
----

==== Vertical Pod Autoscaler (VPA)

Automáticamente ajusta requests/limits basado en uso real.

**Instalación:**

[source,bash]
----
# Agregar repositorio
helm repo add fairwinds-stable https://charts.fairwinds.com/stable
helm repo update

# Instalar VPA
helm install vpa fairwinds-stable/vpa \
  --namespace kube-system
----

**Modos de VPA:**

1. **off**: Solo recomendaciones
2. **initial**: Ajusta requests al crear Pod
3. **recreate**: Mata Pod si cambios necesarios
4. **auto**: Mata Pod si necesario, sino ajusta

[source,yaml]
----
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api

  # Modo de operación
  updatePolicy:
    updateMode: "auto"    # off, initial, recreate, auto

  resourcePolicy:
    # Límites de escalado
    containerPolicies:
    - containerName: api
      minAllowed:
        cpu: 100m
        memory: 64Mi
      maxAllowed:
        cpu: 2000m
        memory: 2Gi
      controlledResources: ["cpu", "memory"]
      controlledValues: RequestsAndLimits
----

**Recomendaciones de VPA:**

[source,bash]
----
# Ver recomendaciones
kubectl describe vpa api-vpa

# Output:
# Recommendation (updated 5m ago):
#   - Target:
#       cpu: 250m
#       memory: 512Mi
#     Lower Bound:
#       cpu: 200m
#       memory: 400Mi
#     Upper Bound:
#       cpu: 500m
#       memory: 1Gi
----

**VPA vs Manual adjustment:**

|===
| Aspecto | VPA | Manual
| Setup | Automático | Manual
| Accuracy | Basado en datos históricos | Human estimate
| Adjustment | Automático | Manual deployment
| Testing | No separa staging | Puedo probar
| Learning curve | Necesita 2-4 semanas de datos | Inmediato
|===

==== Best Practices para Resource Monitoring

1. *Siempre set requests*
   - Crítico para scheduling
   - Sin requests = no garantías de espacio

2. *Set limits conservadores*
   - Evita que 1 Pod monopolice recursos
   - Especialmente para memoria

3. *Memoria limits > requests*
   - CPU comprimible, memoria no
   - Margen de ~20% para memory limits

4. *Monitorear QoS*
   - Garanteed para aplicaciones críticas
   - Burstable para aplicaciones normal
   - BestEffort solo para batch/testing

5. *Usar VPA para recomendaciones*
   - Dejar correr en "off" mode inicial
   - Ver recomendaciones antes de aplicar
   - Especialmente útil para aplicaciones variables

6. *Actualizar requests regularmente*
   - Conforme aplicación cambia
   - Trimestral: revisa logs de monitoreo
   - Si Pods siendo evicted: aumentar requests

7. *Evitar memory limits muy bajos*
   - Causa OOMKilled innecesarios
   - Mejor tener limite más alto que OOM frecuente

8. *Requests para HPA*
   - HPA usa requests para calcular % utilización
   - Sin requests = HPA no funciona bien
   - Asegura que requests sean realistas

== Módulo 9: Escalado y Performance

=== Horizontal Pod Autoscaler (HPA)

==== ¿Qué es HPA?

El Horizontal Pod Autoscaler automáticamente escala el número de Pods basado en métricas observadas.

**Flujo:**

[source]
----
Metrics Server/Prometheus
         ↓
    Recolecta métricas
         ↓
    HPA Controller evalúa
         ↓
CPU > 70%? → Escala UP (agregar Pods)
CPU < 30%? → Escala DOWN (remover Pods)
         ↓
Actualiza Deployment.replicas
----

**Diferencia HPA vs VPA:**

|===
| Aspecto | HPA | VPA
| Qué escala | Número de Pods | Recursos (CPU/Mem) de cada Pod
| Cuándo activa | Carga alta | Recursos mal configurados
| Efecto | Más instancias | Mismas instancias, más recursos
| Latencia | Rápida (segundos) | Lenta (mata Pod, requiere reinicio)
|===

==== Métricas de Escalado

**1. Resource Metrics (CPU/Memoria)**

Las más comunes - basadas en requests:

[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 2
  maxReplicas: 10

  metrics:
  # Escalar basado en CPU
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # % de request

  # Escalar basado en memoria
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
----

**Cálculo de utilización:**

[source]
----
Pod request: 100m (100 milicores)
Pod actual CPU: 75m

Utilización = (75m / 100m) × 100 = 75%

Si threshold=70%, entonces:
75% > 70% → ESCALAR ARRIBA
----

**2. Custom Metrics**

Métricas específicas de la aplicación (requests/sec, latency, etc.):

[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api
  minReplicas: 3
  maxReplicas: 30

  metrics:
  # Métrica custom: requests por segundo
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"  # 1000 req/s por Pod
----

**3. External Metrics**

Métricas externas (queue length, AWS SQS, etc.):

[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: queue-consumer-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: queue-consumer
  minReplicas: 2
  maxReplicas: 20

  metrics:
  # Métrica externa: length de cola SQS
  - type: External
    external:
      metric:
        name: sqs_queue_length
        selector:
          matchLabels:
            queue_name: tasks
      target:
        type: AverageValue
        averageValue: "10"  # 10 mensajes por Pod
----

==== Configuración de HPA

**HPA v1 (simple, deprecated):**

[source,yaml]
----
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa-v1
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70

  # Solo soporta 1 métrica (CPU)
----

**HPA v2 (recomendado, múltiples métricas):**

[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa-v2
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 2
  maxReplicas: 10

  # Múltiples métricas (AND - todas se evalúan)
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

  # Comportamiento de escalado (v2.4+)
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100        # Doblar Pods
        periodSeconds: 15
      - type: Pods
        value: 4          # O agregar 4 Pods
        periodSeconds: 15
      selectPolicy: Max   # Usar el mayor

    scaleDown:
      stabilizationWindowSeconds: 300  # Esperar 5 min
      policies:
      - type: Percent
        value: 50         # Remover 50%
        periodSeconds: 60
      selectPolicy: Min
----

**Ver estado de HPA:**

[source,bash]
----
# Ver HPAs
kubectl get hpa

# Salida:
# NAME       REFERENCE             TARGETS        MINPODS MAXPODS REPLICAS AGE
# app-hpa    Deployment/myapp      75%/70%, ...   2       10      5        2d

# Ver detalles
kubectl describe hpa app-hpa

# Output:
# Metrics:
#   resource cpu on pods (as a percentage of request):
#     Current / Target: 75% / 70%
# Scale Up Events:
#   Time              Reason             Message
#   ----              ------             -------
#   2024-01-15 10:30  SuccessfulRescale  scaled up from 2 to 4 replicas
----

==== Comportamiento de Escalado

**Timeline de escalado:**

[source]
----
Replica 2
    ↓
CPU 75% > 70% threshold → Trigger
    ↓
Evalúa policies, calcula: max(100%, +4 pods) = 4 nuevos
    ↓
0-15s: Scale UP a 6 replicas
    ↓
Espera 15s entre evaluaciones
    ↓
Si todavía > threshold: Scale UP a 12 replicas (máximo 10)
    ↓
Llega a maxReplicas: 10
    ↓
CPU cae a 50%
    ↓
CPU < 30% (downscale threshold)
    ↓
Estabilización: esperar 300s (5 min)
    ↓
Después de 5 min: Scale DOWN 50% = 5 replicas
    ↓
Espera 60s
    ↓
Si todavía < 30%: Scale DOWN a ~2-3 replicas
----

**Parámetros importantes:**

[source,yaml]
----
behavior:
  scaleUp:
    stabilizationWindowSeconds: 0
    # Tiempo antes de evaluar downscale
    # 0 = inmediato, >0 = esperar

  scaleDown:
    stabilizationWindowSeconds: 300
    # Esperar 300s antes de downscale
    # Evita oscilaciones rápidas

  policies:
  - type: Percent
    value: 100
    # Escalar 100% (doblar)
    # type: Pods también disponible
    # value: 4 (agregar 4 Pods)

  selectPolicy: Max
    # Max: usar mayor escalado
    # Min: usar menor escalado
    # Disable: no escalar en esa dirección
----

**Ejemplo práctico:**

[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web

  minReplicas: 2      # Mínimo 2
  maxReplicas: 50     # Máximo 50

  metrics:
  # Métrica 1: CPU
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  # Métrica 2: Memoria
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 85

  # Escalado agresivo arriba, conservador abajo
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0    # Inmediato
      policies:
      - type: Percent
        value: 100                      # Doblar
        periodSeconds: 30
      - type: Pods
        value: 8                        # O +8 Pods
        periodSeconds: 60
      selectPolicy: Max                 # Mayor escalado

    scaleDown:
      stabilizationWindowSeconds: 600   # Esperar 10 min
      policies:
      - type: Percent
        value: 25                       # Remover 25%
        periodSeconds: 60
      selectPolicy: Min                 # Escalado conservador
----

==== Custom Metrics

**Prometheus Adapter (convertir métricas Prometheus a Custom Metrics):**

[source,bash]
----
# Instalar Prometheus adapter
helm install prometheus-adapter prometheus-community/prometheus-adapter \
  --namespace monitoring \
  -f values.yaml
----

**Configuración Prometheus Adapter:**

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: adapter-config
  namespace: monitoring
data:
  rules.yaml: |
    rules:
    # Convertir métrica Prometheus a Custom Metric
    - seriesQuery: 'http_requests_total{namespace!=""}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        matches: "^(.*)_total"
        as: "${1}_per_second"
      metricsQuery: |
        rate(<<.Series>>{<<.LabelMatchers>>}[2m])

    # Queue length desde métrica externa
    - seriesQuery: 'sqs_queue_length{queue!=""}'
      resources:
        overrides:
          queue: {resource: "queue"}
      name:
        as: "queue_length"
      metricsQuery: |
        <<.Series>>{<<.LabelMatchers>>}
----

**HPA con Custom Metric:**

[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-custom-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api

  minReplicas: 2
  maxReplicas: 20

  metrics:
  # Métrica custom: requests per second
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "500"  # 500 req/s por Pod

  # Métrica custom: latency P95
  - type: Pods
    pods:
      metric:
        name: http_request_latency_p95
        selector:
          matchLabels:
            quantile: "0.95"
      target:
        type: AverageValue
        averageValue: "100m"  # 100ms
----

**Debug HPA:**

[source,bash]
----
# Ver eventos de HPA
kubectl describe hpa app-hpa

# Ver métricas actuales
kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1 | jq .

# Verificar Prometheus adapter
kubectl logs -n monitoring -l app.kubernetes.io/name=prometheus-adapter

# Ver métricas raw de Prometheus
kubectl port-forward -n monitoring svc/prometheus 9090:9090
# Acceder a http://localhost:9090
----

==== Best Practices para HPA

1. *Set resource requests correctamente*
   - Sin requests → HPA no funciona
   - Requests deben ser realistas

2. *Usar múltiples métricas*
   - No solo CPU
   - CPU + memoria + custom metrics
   - Evalúa todas (AND)

3. *Conservative downscale**
   - stabilizationWindowSeconds > 60s
   - Evita oscilaciones
   - Cuesta dinero escalar arriba/abajo

4. *Monitorea HPA eventos*
   - Alertas si scaling frecuente
   - Significa límites mal configurados
   - O aplicación tiene picos

5. *Combina con Pod Disruption Budgets*
   - Protege disponibilidad durante downscale
   - Evita interrupciones de tráfico

6. *Test con carga realista*
   - Simula picos de tráfico
   - Verifica que scaling es suficiente
   - Ajusta minReplicas/maxReplicas

7. *Metricsserver debe estar instalado*
   - Sin Metrics Server → HPA no funciona
   - Verificar: `kubectl get deployment metrics-server -n kube-system`

8. *Avoid rapid scaling oscillations*
   - Si escala arriba/abajo constantemente
   - Aumenta stabilizationWindowSeconds
   - Revisa thresholds

=== Vertical Pod Autoscaler (VPA)

==== Introducción a VPA

VPA ajusta automáticamente CPU/Memoria **requests** y **limits** basado en uso real.

**Diferencia clave VPA vs HPA:**

|===
| Aspecto | HPA | VPA
| Escala | Número de Pods | Recursos por Pod
| Métrica | Utilización de requests | Uso real
| Acción | Escala horizontal | Escala vertical
| Latencia | Segundos | Minutos (requiere reinicio)
| Mejora de costo | Muchas instancias pequeñas | Menos instancias, mejor sized
|===

**Cuándo usar VPA:**

- Requests mal configurados
- Aplicación con uso variable
- Reducir costos (usa menos Pods con mejor tamaño)
- Dev clusters (donde availability no es crítica)

**Cuándo NO usar VPA:**

- Aplicaciones de tráfico estable
- Cuando downtime = problema (requiere reinicio)
- Producción con SLA estricto
- Junto con HPA (pueden conflictuar)

==== Modos de Operación

**1. off: Solo recomendaciones**

[source,yaml]
----
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api

  updatePolicy:
    updateMode: "off"   # Solo recomendaciones

  # Ver recomendaciones sin aplicarlas
  # Útil para testing
----

[source,bash]
----
# Ver recomendaciones
kubectl describe vpa api-vpa

# Output:
# Recommendation:
#   Container Recommendations:
#   - Container Name: api
#     Lower Bound:
#       cpu: 100m
#       memory: 128Mi
#     Target:
#       cpu: 250m
#       memory: 512Mi
#     Upper Bound:
#       cpu: 500m
#       memory: 1Gi
----

**2. initial: Ajusta solo Pods nuevos**

[source,yaml]
----
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api

  updatePolicy:
    updateMode: "initial"

  # Pods existentes: NO cambian
  # Pods nuevos (después de upgrade): usan recomendaciones
  # Bueno para rollout gradual
----

**3. recreate: Mata Pods si cambios necesarios**

[source,yaml]
----
updatePolicy:
  updateMode: "recreate"

# VPA mata Pod → Deployment lo re-crea con nuevos resources
# Tiempo de downtime: 5-30 segundos
----

**4. auto: Mejor esfuerzo (recomendado)**

[source,yaml]
----
updatePolicy:
  updateMode: "auto"  # Por defecto "recreate"

# VPA intenta upgradear in-place si es posible
# Sino: mata Pod
----

==== Recomendaciones de VPA

**Componentes que genera VPA:**

[source]
----
1. Recommender: Recolecta métricas históricas, calcula recomendaciones
2. Updater: Aplica cambios (mata/recrea Pods)
3. Admission Controller: Intercept nuevos Pods, inyecta requests recomendadas
----

**Cálculo de recomendaciones:**

[source]
----
VPA analiza: P50, P95, P99 de uso histórico

Para CPU:
  - 95-percentil de uso histórico
  - Más margen si oscila mucho

Para Memoria:
  - 95-percentil de uso (memoria no es comprimible)
  - Margen para picos

Genera 3 valores:
  - lowerBound: Mínimo recomendado (P25 aprox)
  - target: Recomendación principal (P95 aprox)
  - upperBound: Máximo sugerido (P99 aprox)
----

**Configuración con limits:**

[source,yaml]
----
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api

  updatePolicy:
    updateMode: "auto"

  # Limitar rango de recomendaciones
  resourcePolicy:
    containerPolicies:
    - containerName: api
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 2Gi
      # VPA no recomendará fuera de estos rangos

      controlledResources: ["cpu", "memory"]
      # Qué recursos ajustar (omit = todos)

      controlledValues: RequestsAndLimits
      # Ajustar requests Y limits
      # "Requests" solo = sin cambiar limits
----

==== Políticas de Actualización

**updatePolicy en VPA:**

[source,yaml]
----
updatePolicy:
  updateMode: auto

  # Parámetros de cómo actualizar
  maxUnavailable: 1    # Máximo Pods unavailable durante update

  minAvailable: 1      # Mínimo Pods disponibles

  minRichVersion: "1.12.0"  # Mínima K8s version requerida

  # Ejemplo: si Deployment tiene 3 replicas
  # maxUnavailable: 1 → máximo 1 Pod muere, mínimo 2 corriendo
----

**Evitar conflictos con HPA:**

[source,yaml]
----
# NO RECOMENDADO: Usar VPA + HPA juntos
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
spec:
  # ...

---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
spec:
  # Ambos intentan escalar → conflicto

# RECOMENDADO: Solo HPA para tráfico variable
# O: VPA para aplicaciones con recursos mal configured, sin HPA
----

==== Best Practices para VPA

1. *Empezar en modo "off"*
   - Ver recomendaciones por 2-4 semanas
   - Validar antes de aplicar automáticamente

2. *No usar con HPA*
   - Uno u otro, no ambos
   - Conflictúan en objetivos

3. *Usar initial mode en producción*
   - Minimiza downtime
   - Nuevos Pods usan recomendaciones

4. *Set minAllowed/maxAllowed realistas*
   - Evita recomendaciones extremas
   - Protege contra bugs

5. *Monitorea eventos de VPA*
   - Ver cuándo actualiza Pods
   - Alertar si cambios frecuentes

6. *Combinar con monitoring*
   - Verificar post-actualización
   - Asegurar que recomendaciones son correctas

7. *Para dev/staging only*
   - Producción con downtime: usar manual adjustment
   - O usar initial mode si acceptable

8. *Check Prometheus metrics*
   - `vpa_recommendation_updated_pods`
   - `vpa_container_recommendation_target`

=== Cluster Autoscaler

==== ¿Qué es Cluster Autoscaler?

Cluster Autoscaler automáticamente agrega/remueve **nodos** basado en demanda de Pods.

**Flujo:**

[source]
----
Pod no entra en ningún nodo
  ↓
Está en estado Pending
  ↓
Cluster Autoscaler lo detecta
  ↓
Crea nuevo nodo en el cluster
  ↓
Pod se scheduleá en nuevo nodo
  ↓

Y al revés:
Nodo sub-utilizado por X tiempo
  ↓
Cluster Autoscaler lo detecta
  ↓
Drenaja Pods (los mueve a otros nodos)
  ↓
Termina nodo
----

**Diferencia con HPA:**

|===
| Aspecto | HPA | Cluster Autoscaler
| Escala | Pods | Nodos
| Trigger | CPU/Mem alto | Pods Pending
| Acción | Más Pods | Más nodos
| Reversible | Rápido downscale | Lento (drena nodos)
|===

==== Instalación y Configuración

**Instalar Cluster Autoscaler (EKS, GKE, AKS):**

[source,bash]
----
# EKS
helm repo add autoscaler https://kubernetes.github.io/autoscaler
helm install cluster-autoscaler autoscaler/cluster-autoscaler \
  --namespace kube-system \
  --set autoDiscovery.clusterName=my-cluster \
  --set awsRegion=us-east-1 \
  --set cloudProvider=aws

# GKE (automático por defecto)

# AKS
helm install cluster-autoscaler autoscaler/cluster-autoscaler \
  --namespace kube-system \
  --set cloudProvider=azure
----

**Configuración:**

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-status
  namespace: kube-system
data:
  nodes.max: "100"        # Máximo nodos
  nodes.min: "3"          # Mínimo nodos
  scale-down-enabled: "true"
  scale-down-delay-after-add: "10m"
  scale-down-unneeded-time: "10m"  # Nodo sin usar por 10m → remover
----

**Nodos con Auto-Scaling Groups (ASG):**

[source]
----
Cluster Autoscaler monitorea ASG
  ├─ Cuando Pods Pending: aumenta desiredCapacity
  ├─ Cuando nodos sub-utilizados: disminuye desiredCapacity
  └─ Cloud provider (AWS EC2) crea/termina instancias
----

==== Políticas de Escalado

**Scale-down policy:**

[source]
----
scale-down-enabled: true
scale-down-delay-after-add: 10m
  - Esperar 10 min después de agregar nodo antes de remover

scale-down-unneeded-time: 10m
  - Nodo debe estar sub-utilizado por 10 min antes de remover

scale-down-utilization-threshold: 0.65
  - Nodo con < 65% utilización es candidato para remover
----

**Evitar scale-down de nodos importantes:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: critical-app
  annotations:
    # Indica que Pod no puede ser evicted
    cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
spec:
  containers:
  - name: app
    image: critical-app:latest
----

**Pod Disruption Budgets:**

[source,yaml]
----
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: critical

  # Durante scale-down: mínimo 1 Pod debe estar disponible
  # Cluster Autoscaler respeta esto
----

==== Monitoreo de Cluster Autoscaler

[source,bash]
----
# Ver logs de Cluster Autoscaler
kubectl logs -n kube-system -l app=cluster-autoscaler -f

# Ver nodos y su utilización
kubectl top nodes

# Ver Pods Pending (causa de scale-up)
kubectl get pods --field-selector=status.phase=Pending -A

# Ver eventos de escalado
kubectl describe nodes | grep -A 5 "ScaleUp"
----

==== Best Practices para Cluster Autoscaler

1. *Configurar requests en Pods*
   - Sin requests → Cluster Autoscaler no puede calcular necesidad
   - Requests = clave para scheduling

2. *Set min/max nodes realisticemente*
   - Min: mínimo para aplicación crítica
   - Max: límite de costo

3. *Use Pod Disruption Budgets*
   - Protege aplicaciones durante scale-down
   - Asegura disponibilidad

4. *Combinar con HPA*
   - HPA escala Pods
   - Cluster Autoscaler escala nodos
   - Complementarios, no conflictivos

5. *Monitorea eventos de scale*
   - Alertar si frecuentes Pending Pods
   - Significa minReplicas too low

6. *Test con diferentes cargas*
   - Scale-up es fácil
   - Scale-down es más delicado (requiere draining)

7. *Configure scale-down conservadoramente*
   - scale-down-unneeded-time > 10min
   - Evita oscilaciones

8. *Considerar reserved capacity*
   - Mantener algunos nodos como buffer
   - Para rápido scaling de Pods

=== Resource Management

==== Resource Quotas

Resource Quotas limitan consumo de recursos por namespace.

**Crear Quota:**

[source,yaml]
----
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  # Limites de CPU
  hard:
    requests.cpu: "100"      # 100 CPUs totales
    limits.cpu: "200"        # 200 CPUs en limits

    # Limites de memoria
    requests.memory: "100Gi"  # 100 GB memoria
    limits.memory: "200Gi"

    # Conteos de objetos
    pods: "100"              # Máximo 100 Pods
    replicationcontrollers: "20"
    services: "10"
    persistentvolumeclaims: "4"

  # Scopes (aplicar a Pods específicos)
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["high", "medium"]
----

**Verificar uso de Quota:**

[source,bash]
----
# Ver Quotas
kubectl get resourcequota -n production

# Ver detalles
kubectl describe resourcequota production-quota -n production

# Output:
# Name:                    production-quota
# Namespace:               production
# Resource                 Used       Hard
# --------                 ----       ----
# limits.cpu               50         200
# limits.memory            50Gi       200Gi
# pods                     30         100
# requests.cpu             25         100
# requests.memory          25Gi       100Gi
----

**Impacto de Quota:**

[source]
----
Intento crear Pod que viola Quota
  ↓
API Server rechaza (403 Forbidden)
  ↓
Error: "exceeded quota: requests.cpu"

Si Quota alcanzado:
  ├─ Nuevos Pods rechazados
  ├─ Deployments quedan Pending
  └─ Requiere eliminar Pods o aumentar Quota
----

==== Limit Ranges

Limit Ranges especifican min/max de requests/limits por contenedor.

[source,yaml]
----
apiVersion: v1
kind: LimitRange
metadata:
  name: container-limits
  namespace: production
spec:
  limits:
  # Limits por contenedor
  - max:
      cpu: "2"          # CPU máxima por contenedor
      memory: "1Gi"
    min:
      cpu: "100m"       # CPU mínima por contenedor
      memory: "128Mi"
    default:
      cpu: "500m"       # Default si no especificado
      memory: "512Mi"
    defaultRequest:
      cpu: "250m"
      memory: "256Mi"
    type: Container

  # Limits por Pod
  - max:
      cpu: "4"          # CPU máxima en Pod
      memory: "4Gi"
    min:
      cpu: "200m"
      memory: "256Mi"
    type: Pod
----

**Validación de LimitRange:**

[source]
----
Crear Pod sin requests/limits
  ↓
LimitRange Controller inyecta defaults
  ↓
Pod termina con defaults especificados

Crear Pod con requests > max
  ↓
API Server rechaza
  ↓
Error: "cpu maximum exceeded"
----

==== Priority Classes

Priority Classes especifican importancia de Pods.

[source,yaml]
----
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000
globalDefault: false
description: "Para aplicaciones críticas"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 100
globalDefault: false
description: "Para batch jobs"
----

**Usar Priority Class en Pod:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: critical-app
spec:
  priorityClassName: high-priority

  containers:
  - name: app
    image: critical-app:latest

  # Durante escasez de recursos:
  # Pods con prioridad baja son evicted
  # Pods con prioridad alta son protegidos
----

**Preemption:**

Cuando recurso escaso y Pod con prioridad alta necesita entrar:
1. Pods con prioridad baja son terminados
2. Pod con prioridad alta es scheduled
3. Clusters evita starvation de Pods críticos

[source]
----
Recursos:
  ├─ Pod A (priority: 1000) ← CRÍTICA
  ├─ Pod B (priority: 100)  ← batch
  └─ Pod C (priority: 100)  ← batch

Pod A necesita espacio
  ↓
Kubelet termina Pods B y C
  ↓
Pod A se scheduleá
----

==== Best Practices para Resource Management

1. *Implementar Resource Quotas por namespace*
   - Control de costos
   - Previene monopolio de recursos

2. *Usar Limit Ranges para defaults*
   - Asegura que Pods tienen requests/limits
   - Protege contra mal-configuración

3. *Definir Priority Classes*
   - Crítica: aplicaciones de producción
   - Normal: aplicaciones estándar
   - Baja: batch, dev, testing

4. *Monitorar consumo de Quota*
   - Alertas cuando > 80% usado
   - Planificación de capacidad

5. *Separar namespaces por importancia*
   - Production: recursos garantizados
   - Staging: recursos limitados
   - Dev: best-effort

6. *Combinar Quota + LimitRange*
   - Quota: límites totales
   - LimitRange: límites por Pod
   - Ambos necesarios

7. *Revisar Quotas regularmente*
   - Conforme aplicaciones crecen
   - Evitar bottlenecks de recursos

8. *Documentar políticas*
   - Por qué estos valores
   - Cuándo aumentar

=== Performance Tuning

==== Node Affinity

Controla qué nodos pueden correr Pods.

**requiredDuringSchedulingIgnoredDuringExecution (requerido):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-required
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-type
            operator: In
            values:
            - gpu
            - high-memory

  containers:
  - name: app
    image: myapp:latest

  # Pod SOLO puede ir a nodos con label:
  # node-type=gpu OR node-type=high-memory
  # Si no hay nodos válidos: Pending
----

**preferredDuringSchedulingIgnoredDuringExecution (preferencia):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-preferred
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: cloud.google.com/gke-preemptible
            operator: NotIn
            values:
            - "true"
      - weight: 50
        preference:
          matchExpressions:
          - key: node.kubernetes.io/instance-type
            operator: In
            values:
            - t3.large
            - t3.xlarge

  containers:
  - name: app
    image: myapp:latest

  # Prefiere: nodos no-preemptible (weight 100)
  # Y: nodos t3.large/xlarge (weight 50)
  # Pero acepta otros si necesario
----

==== Pod Affinity y Anti-Affinity

Controla relación entre Pods.

**Pod Affinity (Pods cercanos):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: web-pod
spec:
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - cache
          topologyKey: kubernetes.io/hostname

  containers:
  - name: web
    image: web:latest

  # Prefiere: mismo nodo que Pods con label app=cache
  # topologyKey=hostname → mismo nodo físico
----

**Pod Anti-Affinity (Pods separados):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: db-pod
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - database
        topologyKey: kubernetes.io/hostname

  containers:
  - name: db
    image: postgres:latest

  # Requerido: NO mismo nodo que otros db Pods
  # Garantiza que DB instances están en nodos diferentes
----

**topologyKey valores:**

[source]
----
kubernetes.io/hostname → Diferente nodo físico
topology.kubernetes.io/zone → Diferente availability zone
topology.kubernetes.io/region → Diferente región
custom-label → Custom label en nodos
----

==== Taints y Tolerations

Taints: marca que nodo rechaza Pods

**Agregar Taint a nodo:**

[source,bash]
----
# Taint sin toleración → Pod rechazado
kubectl taint nodes node-1 gpu=yes:NoSchedule

# Efectos:
# NoSchedule: No scheduleá sin toleration
# NoExecute: Termina Pods sin toleration
# PreferNoSchedule: Prefiere otro nodo

# Remover taint
kubectl taint nodes node-1 gpu=yes:NoSchedule-
----

**Toleration en Pod:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: gpu-app
spec:
  containers:
  - name: app
    image: gpu-app:latest

  tolerations:
  - key: gpu
    operator: Equal
    value: "yes"
    effect: NoSchedule

  # Pod puede entrar en nodos con taint gpu=yes:NoSchedule
----

**Caso de uso: GPU nodes:**

[source,yaml]
----
# 1. Marcar nodo GPU
kubectl taint nodes gpu-node-1 gpu=required:NoSchedule

# 2. Pod que necesita GPU
apiVersion: v1
kind: Pod
metadata:
  name: ml-training
spec:
  # Requerir nodo GPU
  nodeSelector:
    gpu: "true"

  tolerations:
  - key: gpu
    operator: Equal
    value: required
    effect: NoSchedule

  containers:
  - name: training
    image: ml-training:latest
    resources:
      limits:
        nvidia.com/gpu: 1

  # Pod solamente puede entrar en nodos GPU
----

**Caso de uso: Nodos dedicados**

[source,bash]
----
# Nodos de staging no aceptan Pods de producción
kubectl taint nodes staging-node env=staging:NoExecute

# Production Pods sin toleration → evicted de staging
# Staging Pods con toleration → pueden correr en staging
----

==== Best Practices para Performance Tuning

1. *Node Affinity para características especiales*
   - GPU nodes
   - High-memory nodes
   - Specific instance types

2. *Pod Anti-Affinity para redundancia*
   - Database replicas en diferentes nodos
   - Web servers distribuidos
   - Garantiza alta disponibilidad

3. *Taints para nodos especializados*
   - GPU, SSD, high-memory
   - Aislamiento de workloads

4. *Monitorear scheduling decisions*
   - Ver si Pods se schedulean donde se esperan
   - Debugging con kubectl describe pod

5. *Combinar affinity con resources*
   - Affinity: dónde puede ir
   - Resources: cuántos pueden entrar

6. *Test affinity rules*
   - Cambios mal configurados = Pods Pending
   - Validar antes de producción

7. *Usar preferred, no required*
   - required es inflexible
   - preferred = mejor experiencia

8. *Documentar topología de cluster*
   - Qué nodos tienen qué características
   - Labels, taints, zones

== Módulo 10: Helm

=== Introducción a Helm

==== ¿Qué es Helm?

Helm es el **package manager** de Kubernetes. Similiar a apt (Linux), pip (Python), o npm (Node.js).

**Problemas que resuelve Helm:**

[source]
----
Sin Helm:
  ├─ Desplegar aplicación = escribir 20+ YAML manifests
  ├─ Valores hardcodeados en YAML
  ├─ Actualizar versión = editar múltiples archivos
  ├─ Rollback = restaurar YAML anteriores
  └─ Reutilización = copiar/pegar YAML de otros

Con Helm:
  ├─ Desplegar aplicación = helm install chart-name
  ├─ Valores parametrizados (--set, values.yaml)
  ├─ Actualizar versión = helm upgrade release
  ├─ Rollback automático = helm rollback release
  └─ Reutilización = compartir Charts en repositorios
----

**Componentes clave:**

|===
| Componente | Descripción | Analogía
| Chart | Template/Blueprint de aplicación | RPM, .deb package
| Release | Instancia en vivo de un Chart | Installed package
| Repository | Colección de Charts | Package registry (apt, npm)
| Values | Parámetros de configuración | Config file
|===

==== Ventajas del Package Management

**1. Simplificación**

[source,bash]
----
# Sin Helm: escribir 30 líneas de YAML
kubectl apply -f service.yaml
kubectl apply -f deployment.yaml
kubectl apply -f configmap.yaml
kubectl apply -f secret.yaml
# ... más archivos

# Con Helm: una línea
helm install myapp ./charts/myapp
----

**2. Parametrización**

[source,bash]
----
# Sin Helm: editar YAML para cada ambiente
# prod-deployment.yaml, staging-deployment.yaml, dev-deployment.yaml

# Con Helm: único Chart, diferentes valores
helm install myapp-prod ./charts/myapp --values values-prod.yaml
helm install myapp-staging ./charts/myapp --values values-staging.yaml
helm install myapp-dev ./charts/myapp -set replicas=1
----

**3. Versionamiento**

[source]
----
Helm automáticamente mantiene historial de releases:
├─ myapp-1: version 1.0.0
├─ myapp-2: version 1.1.0 (upgrade)
├─ myapp-3: version 1.2.0 (upgrade)
└─ Rollback a myapp-2 = una línea de comando
----

**4. Reutilización**

[source,bash]
----
# Descargar Chart de repositorio público
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install postgres bitnami/postgresql

# Chart pregrabado con mejores prácticas
# No necesita configuración manual
----

**5. Dependencias**

[source]
----
Chart de aplicación puede depender de otros Charts:
  ├─ postgres (database)
  ├─ redis (cache)
  └─ prometheus (monitoring)

Helm automáticamente instala todas las dependencias
----

==== Arquitectura de Helm 3

**Helm 3 vs Helm 2:**

|===
| Aspecto | Helm 2 | Helm 3
| Tiller | Server-side component | Removido (más seguro)
| Storage | Etcd o ConfigMaps | ConfigMaps o Secrets
| Permissions | Requireria cluster-admin | No requiere permisos especiales
| Templating | Go templates | Go templates (igual)
| Seguridad | Menor (Tiller tenía acceso) | Mayor (cliente-only)
|===

**Flujo de Helm 3:**

[source]
----
1. Cliente Helm (helm command)
   ↓ Lee Chart local o repositorio
   ↓
2. Procesa Templates (sustituye valores)
   ↓ Genera YAML puro de Kubernetes
   ↓
3. Kubectl API (helm internamente usa kubectl)
   ↓ Envia manifests a API server
   ↓
4. API Server procesa YAML
   ↓ Crea Deployments, Services, etc.
   ↓
5. Helm almacena metadata
   ↓ ConfigMap con historial de release
   ↓
6. Kubelet ejecuta Pods
----

**Almacenamiento de Releases:**

[source,bash]
----
# Helm 3 almacena releases como Secrets (encrypted)
kubectl get secrets -n default | grep sh.helm.release

# Ver contenido de un release
kubectl get secret sh.helm.release.v1.myapp.v1 -o yaml | jq .data.release | base64 -d | gunzip | jq .
----

==== Instalación de Helm

**Instalación en Linux/Mac:**

[source,bash]
----
# Descargar script de instalación
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Verificar instalación
helm version

# Output:
# version.BuildInfo{Version:"v3.12.0", GitCommit:"c9f554..."}
----

**Instalación en Windows:**

[source,bash]
----
# Con Chocolatey
choco install kubernetes-helm

# O descargar manualmente desde
# https://github.com/helm/helm/releases
----

**Autocompletado:**

[source,bash]
----
# Bash
helm completion bash | sudo tee /etc/bash_completion.d/helm

# Zsh
helm completion zsh | sudo tee /usr/share/zsh/site-functions/_helm

# Fish
helm completion fish | sudo tee /usr/share/fish/vendor_completions.d/helm.fish
----

**Configurar contexto de Kubernetes:**

[source,bash]
----
# Helm usa el contexto actual de kubectl
kubectl config current-context

# Cambiar contexto si es necesario
kubectl config use-context production-cluster

# Helm ahora deployará a ese cluster
helm install myapp ./charts/myapp
----

==== Primeros pasos con Helm

**Agregar repositorio oficial:**

[source,bash]
----
# Repositorio oficial de Charts (comunidad)
helm repo add stable https://charts.helm.sh/stable

# Bitnami (Charts de buena calidad)
helm repo add bitnami https://charts.bitnami.com/bitnami

# Actualizar índice local
helm repo update

# Ver repositorios agregados
helm repo list
----

**Buscar Charts:**

[source,bash]
----
# Buscar nginx
helm search repo nginx

# Output:
# NAME                            CHART VERSION APP VERSION DESCRIPTION
# bitnami/nginx                   13.2.24       1.24.0      NGINX Open Source is a web server...
# stable/nginx-ingress            4.6.0         1.4.0       An ingress controller that uses...

# Ver detalles de un Chart
helm show chart bitnami/nginx
helm show values bitnami/nginx    # Ver valores por defecto
helm show all bitnami/nginx       # Todo
----

**Instalar un Chart:**

[source,bash]
----
# Instalación simple
helm install my-nginx bitnami/nginx

# Instalar con valores personalizados
helm install my-nginx bitnami/nginx \
  --set replicaCount=3 \
  --set service.type=LoadBalancer

# Instalar desde archivo values.yaml
helm install my-nginx bitnami/nginx -f values.yaml

# Ver release instalado
helm list

# Salida:
# NAME      NAMESPACE STATUS    CHART         APP VERSION LAST DEPLOYED
# my-nginx  default   deployed  nginx-13.2.24 1.24.0      2024-01-15
----

**Ver status:**

[source,bash]
----
# Status de release específica
helm status my-nginx

# Ver todos los Pods creados por release
kubectl get pods -l app.kubernetes.io/instance=my-nginx

# Ver servicios
kubectl get svc -l app.kubernetes.io/instance=my-nginx
----

**Actualizar release:**

[source,bash]
----
# Upgrade a nueva versión
helm upgrade my-nginx bitnami/nginx --set replicaCount=5

# Ver historial de cambios
helm history my-nginx

# Rollback a versión anterior
helm rollback my-nginx 1

# Desinstalar
helm uninstall my-nginx
----

==== Best Practices para Helm

1. *Usar Charts de repositorios confiables*
   - Bitnami (mantenido, auditoría)
   - Oficiales de aplicaciones (nginx, postgresql)
   - Evitar Charts desconocidos

2. *Versionear values.yaml en Git*
   - Cada ambiente: values-prod.yaml, values-staging.yaml
   - Auditoria de cambios
   - Reproducibilidad

3. *Usar valores en lugar de editar templates*
   - Mantener Charts limpio
   - Fácil de actualizar Chart
   - Menos conflictos

4. *Documentar custom values*
   - Por qué cada valor está configurado así
   - Impacto en producción
   - Cuándo cambiar

5. *Probar Charts antes de producción*
   - helm lint para validación
   - helm template para ver YAML generado
   - helm test para tests (si Chart los soporta)

6. *Usar namespaces*
   - helm install myapp ./chart -n production
   - Separación entre enviroments
   - Mejor control de RBAC

7. *Mantener Helm actualizado*
   - Nuevas versiones = mejoras de seguridad
   - helm version para ver versión actual
   - Fácil de actualizar (solo binario)

8. *Combinar con GitOps*
   - Stores values en Git
   - ArgoCD/Flux observa cambios
   - Helm automáticamente actualiza cluster

=== Charts

Un Chart de Helm es un paquete que contiene toda la información necesaria para desplegar una aplicación en Kubernetes. Es similar a un paquete en apt, npm o pip, pero específicamente diseñado para Kubernetes. Un Chart es simplemente un directorio con una estructura definida que contiene plantillas YAML, valores por defecto, metadatos y, opcionalmente, dependencias de otros Charts.

Los Charts son la unidad fundamental de reutilización en Helm. Permiten definir una aplicación de forma paramétrica, facilitando el despliegue en múltiples ambientes (desarrollo, staging, producción) con configuraciones diferentes. Un Chart bien diseñado abstrae la complejidad de Kubernetes del usuario final, quien solo necesita proporcionar valores específicos.

==== Estructura de un Chart

La estructura de un Chart sigue un patrón estándar que Helm espera encontrar:

[source,text]
----
my-chart/                          # Directorio raíz del Chart
├── Chart.yaml                     # Metadatos del Chart (nombre, versión, etc.)
├── values.yaml                    # Valores por defecto
├── README.md                       # Documentación
├── LICENSE                         # Licencia
├── templates/                      # Plantillas de Kubernetes
│   ├── deployment.yaml            # Plantilla de Deployment
│   ├── service.yaml               # Plantilla de Service
│   ├── ingress.yaml               # Plantilla de Ingress
│   ├── configmap.yaml             # Plantilla de ConfigMap
│   ├── secret.yaml                # Plantilla de Secret
│   ├── hpa.yaml                   # Plantilla de HPA
│   ├── _helpers.tpl               # Plantillas reutilizables (helpers)
│   └── NOTES.txt                  # Notas de post-instalación
├── values/                        # (Opcional) Valores por ambiente
│   ├── values-dev.yaml
│   ├── values-staging.yaml
│   └── values-prod.yaml
├── charts/                        # (Opcional) Dependencias de Charts
│   ├── postgresql-11.0.0/
│   └── redis-15.0.0/
├── .helmignore                    # Archivos ignorados al empaquetar
└── CHANGELOG.md                   # Registro de cambios
----

**Directorios y archivos clave:**

* *Chart.yaml*: Archivo YAML que define el Chart. Contiene metadatos como nombre, descripción, versión, mantenedores, repositorio y dependencias.

* *values.yaml*: Archivo con valores por defecto. Helm los usa si el usuario no proporciona overrides. Define toda la configuración de la aplicación.

* *templates/*: Directorio que contiene todas las plantillas de Kubernetes. Cada archivo plantilla se procesa con Go templating engine para generar manifiestos YAML finales.

* *charts/*: Directorio que contiene Charts dependientes (subchart). Permite reutilizar Charts dentro de otros Charts.

* *_helpers.tpl*: Archivo especial con plantillas reutilizables (helpers/macros). Evita repetición de código en otras plantillas.

* *NOTES.txt*: Archivo de plantilla que se muestra al usuario después de instalar el Chart. Típicamente contiene instrucciones de post-instalación.

* *.helmignore*: Similar a .gitignore. Define qué archivos ignorar al empaquetar el Chart.

**Diagrama de flujo de un Chart:**

[source,text]
----
┌─────────────────────────────────────────────────────────────┐
│                   Chart de Helm                             │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  values.yaml (valores por defecto)                          │
│  ↓ (usuario puede hacer override con --set, -f values.yaml) │
│                                                              │
│  ┌─────────────────────────────────────────────────────┐    │
│  │ Helm Template Engine (Go templating)                │    │
│  │                                                     │    │
│  │  deployment.yaml  → Deployment.yaml (YAML final)   │    │
│  │  service.yaml     → Service.yaml   (YAML final)    │    │
│  │  ingress.yaml     → Ingress.yaml   (YAML final)    │    │
│  │  configmap.yaml   → ConfigMap.yaml (YAML final)    │    │
│  │  ...                                                │    │
│  └─────────────────────────────────────────────────────┘    │
│  ↓                                                           │
│  Manifiestos YAML compilados                               │
│  ↓                                                           │
│  Kubernetes API Server (kubectl apply)                      │
│  ↓                                                           │
│  Recursos desplegados en cluster                            │
└─────────────────────────────────────────────────────────────┘
----

==== Valores (Values) y values.yaml

Los valores son la forma principal de parametrizar un Chart. Permiten al usuario personalizar el comportamiento del Chart sin modificar las plantillas.

**Archivo values.yaml básico:**

[source,yaml]
----
# Configuración de réplicas y imagen
replicaCount: 3

image:
  repository: nginx
  pullPolicy: IfNotPresent
  tag: "1.24.0"

# Configuración del servicio
service:
  type: ClusterIP          # ClusterIP, NodePort, LoadBalancer
  port: 80
  targetPort: 8080
  annotations: {}

# Recursos
resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

# Escalado automático
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# Configuración de aplicación
app:
  name: myapp
  environment: production
  debug: false
  logLevel: info

# Base de datos
database:
  enabled: true
  host: postgres.default.svc.cluster.local
  port: 5432
  name: myapp_db
  user: admin
  # Password debería venir de un Secret externo
  passwordSecret:
    name: db-secret
    key: password

# Afinidad de nodos
nodeSelector: {}
  # disktype: ssd

tolerations: []

affinity: {}
----

**Anidamiento y estructura:**

Los valores pueden anidarse en múltiples niveles. Se acceden en templates usando la sintaxis de punto (dot notation):

[source,yaml]
----
# En values.yaml
config:
  database:
    host: localhost
    credentials:
      username: admin
      password: secret123

# En template, se acceden como:
# {{ .Values.config.database.host }}
# {{ .Values.config.database.credentials.username }}
----

**Overriding de valores:**

Los usuarios pueden hacer override de valores de varias formas:

[source,bash]
----
# Opción 1: Usar --set desde línea de comandos
helm install myapp ./chart \
  --set replicaCount=5 \
  --set image.tag="1.25.0" \
  --set app.environment=staging

# Opción 2: Usar archivo values externo
helm install myapp ./chart -f values-staging.yaml

# Opción 3: Combinar ambos (el segundo override al primero)
helm install myapp ./chart -f values-base.yaml -f values-prod.yaml

# Opción 4: Mergear valores complejos
helm install myapp ./chart \
  -f values-prod.yaml \
  --set-string database.host=prod-db.example.com

# Ver valores resueltos (después de merges)
helm get values myapp
----

**Valores especiales (Built-in):**

Helm proporciona algunos valores especiales disponibles automáticamente:

[source,yaml]
----
# En cualquier plantilla, estos están disponibles:

# Información de la versión del Chart
.Chart.Name              # Nombre del Chart
.Chart.Version           # Versión del Chart (ej: 1.0.0)
.Chart.AppVersion        # Versión de la aplicación (ej: 1.24.0)
.Chart.Description       # Descripción
.Chart.Type              # Tipo (application o library)
.Chart.Maintainers       # Lista de mantenedores

# Información de la Release (instalación)
.Release.Name            # Nombre de la Release (ej: myapp)
.Release.Namespace       # Namespace donde se instala
.Release.IsUpgrade       # true si es un upgrade
.Release.IsInstall       # true si es una instalación nueva

# Información del Helm
.Helm.Version            # Versión de Helm (ej: v3.12.0)

# Ejemplo de uso en plantilla:
metadata:
  name: {{ .Release.Name }}
  labels:
    app: {{ .Chart.Name }}
    version: {{ .Chart.Version }}
    managed-by: Helm
    managed-by-version: {{ .Helm.Version }}
----

==== Plantillas (Templates)

Las plantillas son archivos YAML que usan Go templating engine. Permiten lógica condicional, loops, funciones y manipulación de strings.

**Sintaxis básica de templates:**

[source,yaml]
----
# 1. Variables simples (acceso a valores)
name: {{ .Values.app.name }}

# 2. Condicionales
{{ if .Values.autoscaling.enabled }}
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{ .Release.Name }}
spec:
  minReplicas: {{ .Values.autoscaling.minReplicas }}
{{ end }}

# 3. Loops
ports:
{{ range .Values.containers }}
  - name: {{ .name }}
    port: {{ .port }}
{{ end }}

# 4. Funciones y pipelines
labels:
  app: {{ .Values.app.name | lower }}  # Convertir a minúsculas
  version: {{ .Chart.Version | quote }} # Agregar comillas

# 5. Funciones de strings
name: {{ .Release.Name }}-{{ randAlphaNum 5 }}  # Generar nombre aleatorio
name: {{ .Release.Name | upper }}                # Mayúsculas
name: {{ .Release.Name | trunc 63 }}             # Truncar a 63 caracteres

# 6. Funciones matemáticas
replicas: {{ .Values.replicaCount | int }}
memory: {{ .Values.resources.memory | int }}Mi
----

**Ejemplo de Deployment con templates:**

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-{{ .Chart.Name }}
  namespace: {{ .Release.Namespace }}
  labels:
    app: {{ .Values.app.name }}
    version: {{ .Chart.Version }}
    managed-by: Helm
spec:
  # Replicas puede ser override con --set replicaCount=X
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: {{ .Values.app.name }}
      instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app: {{ .Values.app.name }}
        instance: {{ .Release.Name }}
      {{- if .Values.podAnnotations }}
      annotations:
        {{- toYaml .Values.podAnnotations | nindent 8 }}
      {{- end }}
    spec:
      serviceAccountName: {{ .Release.Name }}
      containers:
      - name: {{ .Chart.Name }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        {{- if .Values.app.debug }}
        env:
        - name: DEBUG
          value: "true"
        - name: LOG_LEVEL
          value: {{ .Values.app.logLevel }}
        {{- end }}
        resources:
          {{- toYaml .Values.resources | nindent 10 }}
        {{- if .Values.livenessProbe }}
        livenessProbe:
          {{- toYaml .Values.livenessProbe | nindent 10 }}
        {{- end }}
      {{- if .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml .Values.nodeSelector | nindent 8 }}
      {{- end }}
      {{- if .Values.tolerations }}
      tolerations:
        {{- toYaml .Values.tolerations | nindent 8 }}
      {{- end }}
----

**Plantillas auxiliares (_helpers.tpl):**

El archivo `_helpers.tpl` contiene macros reutilizables:

[source,yaml]
----
{{/*
Expandir nombre del Chart
*/}}
{{- define "myapp.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 }}
{{- end }}

{{/*
Crear nombre calificado del Chart
*/}}
{{- define "myapp.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 }}
{{- end }}
{{- end }}
{{- end }}

{{/*
Etiquetas comunes
*/}}
{{- define "myapp.labels" -}}
helm.sh/chart: {{ include "myapp.chart" . }}
{{ include "myapp.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "myapp.selectorLabels" -}}
app.kubernetes.io/name: {{ include "myapp.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

# Uso en otros templates:
# metadata:
#   labels:
#     {{- include "myapp.labels" . | nindent 4 }}
----

==== Chart.yaml: Metadatos del Chart

El archivo `Chart.yaml` define el Chart y sus propiedades:

[source,yaml]
----
apiVersion: v2                    # Versión de API del Chart (v1 es Helm 2, v2 es Helm 3)
name: myapp                       # Nombre único del Chart
version: 1.2.3                    # Versión del Chart (debe seguir semver)
appVersion: "2.0.1"               # Versión de la aplicación que empaqueta

type: application                 # application o library
                                  # library: no genera Pods, solo templates reutilizables

description: "My awesome application"
keywords:
  - myapp
  - web
  - database

# Información sobre el Chart
home: https://github.com/user/myapp
sources:
  - https://github.com/user/myapp

# Licencia del Chart (no de la aplicación)
license: Apache-2.0

# Mantenedores
maintainers:
  - name: John Doe
    email: john@example.com
    url: https://github.com/johndoe

# Icono del Chart (URL a imagen)
icon: https://bitnami.com/assets/stacks/myapp/img/myapp-stack-220x234.png

# Búsqueda en hub.helm.sh
kubeVersion: ">=1.20.0"            # Versión mínima de Kubernetes

# Dependencias del Chart (subchart dependencies)
dependencies:
  - name: postgresql              # Nombre del chart dependiente
    version: "11.0.0"              # Versión exacta
    repository: https://charts.bitnami.com/bitnami
    condition: postgresql.enabled  # Se instala si este valor es true
    tags:
      - database                  # Etiquetas para instalar selectivamente
    alias: mydb                    # Alias para el subchart
    import-values:
      - child                      # Importar valores del subchart

  - name: redis
    version: "17.0.0"
    repository: "oci://registry-1.docker.io/bitnamicharts"
    condition: redis.enabled

# Anotaciones personalizadas
annotations:
  category: Database
  marketplace.bitnami.com/name: "MyApp"
----

**Actualizar dependencias:**

[source,bash]
----
# Descargar las dependencias definidas en Chart.yaml
helm dependency update ./myapp

# Listar dependencias
helm dependency list ./myapp

# Salida:
# NAME         VERSION REPOSITORY                              STATUS
# postgresql   11.0.0  https://charts.bitnami.com/bitnami      ok
# redis        17.0.0  oci://registry-1.docker.io/bitnamicharts ok
----

==== Dependencias de Charts (Subcharts)

Los Subcharts son Charts reutilizables incluidos dentro de otro Chart. Permiten modularizar aplicaciones complejas:

[source,yaml]
----
# En Chart.yaml del Chart principal (myapp)
dependencies:
  - name: postgresql
    version: "11.x"
    repository: "https://charts.bitnami.com/bitnami"
    condition: postgresql.enabled
    alias: db                      # Permite múltiples instancias
    import-values:
      - child

  - name: redis
    version: "17.x"
    repository: "https://charts.bitnami.com/bitnami"
    condition: redis.enabled

# En values.yaml, configurar los subcharts:
postgresql:
  enabled: true
  auth:
    username: myapp
    password: changeme
    database: myapp_db
  primary:
    persistence:
      enabled: true
      size: 10Gi

redis:
  enabled: true
  auth:
    enabled: true
    password: changeme
  replica:
    replicaCount: 2

# O usar alias
db:                  # Alias del postgresql subchart
  enabled: true
  auth:
    username: myapp
    password: changeme
----

**Acceder a valores del subchart:**

[source,yaml]
----
# Los valores del subchart están disponibles con el nombre de subchart o alias:
env:
  DATABASE_HOST: "{{ .Release.Name }}-postgresql"
  DATABASE_PORT: "5432"
  DATABASE_NAME: "{{ .Values.postgresql.auth.database }}"
  DATABASE_USER: "{{ .Values.postgresql.auth.username }}"

  REDIS_HOST: "{{ .Release.Name }}-redis-master"
  REDIS_PORT: "6379"
----

**Estructura de directorios con subcharts:**

[source,text]
----
myapp-chart/
├── Chart.yaml
├── values.yaml
├── templates/
├── charts/                        # Subcharts se descargan aquí
│   ├── postgresql-11.0.0/
│   │   ├── Chart.yaml
│   │   ├── values.yaml
│   │   └── templates/
│   ├── redis-17.0.0/
│   │   ├── Chart.yaml
│   │   ├── values.yaml
│   │   └── templates/
│   └── Chart.lock                 # Lock file (similar a package-lock.json)
└── Chart.lock
----

==== Best Practices para Charts

1. *Estructura clara y consistente*
   - Mantener nombres simples y descriptivos
   - Usar convenciones de nombres consistentes
   - Documentar archivos complejos con comentarios
   - Incluir un README.md completo con ejemplos

2. *Valores bien organizados*
   - Agrupar valores lógicamente (app, database, cache, etc.)
   - Proporcionar valores por defecto seguros
   - Validar rangos razonables (min replicas, max resources)
   - Documentar cada valor en values.yaml

3. *Templates parametrizados*
   - Evitar valores hardcodeados en templates
   - Usar helpers para evitar repetición
   - Mantener templates legibles y simples
   - Comentar lógica compleja (condicionales, loops)

4. *Seguridad en Charts*
   - No incluir secretos en values.yaml por defecto
   - Usar referencias a Secrets externos
   - Restringir permisos en RBAC definitions
   - Escanear imágenes OCI en valores por defecto
   - Usar SecurityContext restrictivos

5. *Control de versiones*
   - Usar Semantic Versioning (MAJOR.MINOR.PATCH)
   - Cambiar Chart.version con cada release
   - Mantener CHANGELOG.md actualizado
   - Usar tags en repositorio de Git

6. *Testing y validación*
   - Usar helm lint regularmente
   - Probar con helm template antes de instalar
   - Crear casos de test con valores diferentes
   - Validar con kubeval o kubeconform

7. *Dependencias explícitas*
   - Especificar versiones exactas o rangos apropiados
   - Usar conditions para habilitar/deshabilitar opcionalmente
   - Documentar por qué se necesita cada dependencia
   - Mantener dependencies actualizadas

8. *Documentación completa*
   - Incluir NOTES.txt con instrucciones post-instalación
   - Documentar valores complejos en values.yaml
   - Incluir ejemplos de diferentes scenarios
   - Mantener README sincronizado con cambios

=== Trabajando con Helm

Trabajar con Helm implica interactuar con Charts desde repositorios remotos, instalarlos en clusters de Kubernetes, actualizarlos según cambios de versión o configuración, y revertir cambios cuando sea necesario. Esta sección cubre los flujos de trabajo más comunes y cómo gestionar efficientemente releases en tus clusters.

==== Gestión de Repositorios de Helm

Los repositorios son colecciones remotas de Charts. Helm necesita conocer dónde obtener los Charts antes de instalarlos.

**Agregar repositorios:**

[source,bash]
----
# Agregar repositorio oficial (Charts de la comunidad)
helm repo add stable https://charts.helm.sh/stable

# Agregar Bitnami (Charts de alta calidad, bien mantenidos)
helm repo add bitnami https://charts.bitnami.com/bitnami

# Agregar repositorio personal o corporativo
helm repo add mycompany https://charts.mycompany.com

# Agregar repositorio con autenticación (si es privado)
helm repo add mycompany https://charts.mycompany.com \
  --username myuser \
  --password mypassword

# Agregar repositorio OCI (Docker registry-like)
helm repo add myrepo oci://registry.example.com/helm-charts

# Listar repositorios agregados
helm repo list

# Salida:
# NAME         URL
# stable       https://charts.helm.sh/stable
# bitnami      https://charts.bitnami.com/bitnami
# mycompany    https://charts.mycompany.com
----

**Actualizar índices de repositorios:**

[source,bash]
----
# Actualizar el índice local de todos los repositorios
helm repo update

# Salida:
# Hang tight while we grab the latest from your chart repositories...
# ...Successfully got an update from the "stable" chart repository
# ...Successfully got an update from the "bitnami" chart repository
# Update Complete. ⎈ Happy Helming!

# Actualizar repositorio específico
helm repo update stable

# Ver cuándo fue la última actualización
helm repo list
----

**Remover y gestionar repositorios:**

[source,bash]
----
# Remover un repositorio
helm repo remove stable

# Listar después de remover
helm repo list

# Buscar en todos los repositorios (si el índice está cacheado)
helm search repo nginx

# Salida:
# NAME                            CHART VERSION   APP VERSION   DESCRIPTION
# bitnami/nginx                   15.1.1          1.25.2        NGINX Open Source is a web server...
# stable/nginx-ingress            4.6.0           1.4.0         An ingress controller that uses...
----

**Gestión segura de repositorios:**

[source,bash]
----
# Verificar integridad de Chart descargar archivo de índice
helm repo update --strict

# Listar repositorios con detalles
helm repo list -o json | jq .

# Ejemplo de salida JSON:
# [
#   {
#     "name": "bitnami",
#     "url": "https://charts.bitnami.com/bitnami",
#     "cache": "/root/.cache/helm/repository"
#   }
# ]
----

==== Búsqueda de Charts

Antes de instalar, necesitas encontrar el Chart adecuado. Helm proporciona varias formas de buscar.

**Búsqueda en repositorios locales:**

[source,bash]
----
# Buscar Charts por nombre (búsqueda en índice cacheado)
helm search repo nginx

# Salida:
# NAME                            CHART VERSION   APP VERSION   DESCRIPTION
# bitnami/nginx                   15.1.1          1.25.2        NGINX Open Source is a web server...
# bitnami/nginx-ingress-controller 9.8.1          1.8.1         NGINX Ingress Controller Helm Chart

# Buscar con patrón (expresión regular)
helm search repo "^bitnami/.*-ingress"

# Buscar en Hub de Helm (requiere conexión a internet)
helm search hub nginx

# Salida:
# URL                                              CHART VERSION   DESCRIPTION
# https://hub.helm.sh/charts/bitnami/nginx         15.1.1          NGINX Open Source is a web server...
# https://hub.helm.sh/charts/stable/nginx-ingress  4.6.0           An ingress controller that uses...

# Listar todas las versiones de un Chart
helm search repo bitnami/postgresql --versions

# Salida:
# NAME                 CHART VERSION   APP VERSION   DESCRIPTION
# bitnami/postgresql   12.1.0          15.2          PostgreSQL is an object-relational database...
# bitnami/postgresql   12.0.0          15.2          PostgreSQL is an object-relational database...
# bitnami/postgresql   11.9.13         15.2          PostgreSQL is an object-relational database...
----

**Ver detalles de un Chart:**

[source,bash]
----
# Ver metadatos del Chart (Chart.yaml)
helm show chart bitnami/postgresql

# Ver archivo README del Chart
helm show readme bitnami/postgresql

# Ver valores por defecto
helm show values bitnami/postgresql

# Ver todo junto
helm show all bitnami/postgresql

# Ver específicamente valores con explicaciones
helm show values bitnami/postgresql | head -50
# Salida (primeras líneas):
# auth:
#   existingSecret: ""
#   postgresPassword: ""
#   username: ""
#   password: ""
#   ...

# Inspecionar una versión específica
helm show chart bitnami/postgresql --version 11.9.13
----

==== Instalación de Charts

La instalación crea una "Release" de Helm en el cluster. Una Release es una instancia de un Chart con valores específicos.

**Instalación básica:**

[source,bash]
----
# Instalación simplest (con valores por defecto)
helm install my-release bitnami/nginx

# Salida:
# NAME: my-release
# LAST DEPLOYED: Mon Jan 15 14:23:45 2024
# NAMESPACE: default
# STATUS: deployed
# REVISION: 1
# NOTES:
# ...

# Verificar que se instaló correctamente
helm list

# Salida:
# NAME         NAMESPACE STATUS    CHART        APP VERSION LAST DEPLOYED
# my-release   default   deployed  nginx-15.1.1 1.25.2      Mon Jan 15 14:23:45 2024

# Ver status detallado
helm status my-release

# Ver recursos creados
kubectl get all -l app.kubernetes.io/instance=my-release
----

**Instalación con valores personalizados:**

[source,bash]
----
# Opción 1: Usar --set para cambiar valores individuales
helm install my-nginx bitnami/nginx \
  --set replicaCount=3 \
  --set image.tag="1.25.0" \
  --set service.type=LoadBalancer

# Opción 2: Usar --set-string para strings complejos (evita conversiones)
helm install my-app myrepo/myapp \
  --set-string config.datasource="jdbc:postgresql://localhost:5432/mydb"

# Opción 3: Usar --set-json para valores JSON complejos
helm install my-app myrepo/myapp \
  --set-json resources='{"limits":{"cpu":"1","memory":"1Gi"}}'

# Opción 4: Usar archivo values externo
helm install my-nginx bitnami/nginx -f values.yaml

# Opción 5: Usar múltiples archivos (mergean en orden)
helm install my-app myrepo/myapp \
  -f values-base.yaml \
  -f values-prod.yaml \
  --set app.environment=production

# Ver valores resueltos después de instalación
helm get values my-nginx
----

**Instalación en namespaces:**

[source,bash]
----
# Instalar en namespace específico (lo crea si no existe)
helm install my-app myrepo/myapp -n production --create-namespace

# Instalar con privilegios de cluster-admin (si lo necesita)
helm install my-app myrepo/myapp --service-account admin

# Ver todas las releases en un namespace
helm list -n production

# Ver todas las releases en todos los namespaces
helm list -A

# Salida:
# NAMESPACE    NAME    STATUS    CHART        APP VERSION
# default      nginx   deployed  nginx-15.1.1 1.25.2
# production   myapp   deployed  myapp-1.0.0  1.0.0
# staging      myapp   deployed  myapp-1.0.0  1.0.0
----

**Instalación con opciones avanzadas:**

[source,bash]
----
# Dryrun: ver qué se instalaría sin realmente instalar
helm install my-app myrepo/myapp --dry-run --debug

# Salida: manifiestos YAML compilados

# Instalar pero no esperar a que esté listo
helm install my-app myrepo/myapp --no-hooks

# Instalar sin ejecutar tests (si Chart los define)
helm install my-app myrepo/myapp --no-hooks

# Instalar con timeout personalizado
helm install my-app myrepo/myapp --timeout 10m0s

# Instalar sin actualizar dependencias
helm install my-app myrepo/myapp --skip-refresh

# Instalar desde Chart local (desarrollo)
helm install my-app ./mychart --debug

# Ver manifiestos que se instalarían
helm template my-app myrepo/myapp --values values.yaml

# Instalación atómica: rollback automático si algo falla
helm install my-app myrepo/myapp --atomic
----

==== Actualización y Rollback de Releases

Cuando hay nuevas versiones o cambios de configuración, puedes actualizar releases existentes:

**Actualización (upgrade):**

[source,bash]
----
# Upgrade a nueva versión del Chart
helm upgrade my-nginx bitnami/nginx

# Upgrade con nuevas versión específica
helm upgrade my-nginx bitnami/nginx --version 15.2.0

# Upgrade y cambiar valores
helm upgrade my-nginx bitnami/nginx \
  --set replicaCount=5 \
  --set image.tag="1.25.1"

# Upgrade desde archivo values
helm upgrade my-nginx bitnami/nginx -f values-prod.yaml

# Upgrade atómico (rollback automático si falla)
helm upgrade my-nginx bitnami/nginx --atomic

# Upgrade con limpieza de hooks viejos
helm upgrade my-nginx bitnami/nginx --cleanup-on-fail

# Ver cambios que haría upgrade (simulación)
helm upgrade my-nginx bitnami/nginx --dry-run --debug

# Upgrade sin recrear Pods
helm upgrade my-nginx bitnami/nginx \
  --set replicaCount=3 \
  --force
----

**Historial y Rollback:**

[source,bash]
----
# Ver historial de releases
helm history my-nginx

# Salida:
# REVISION STATUS      CHART       APP VERSION DESCRIPTION
# 1        superseded  nginx-15.1.1 1.25.2      Install complete
# 2        superseded  nginx-15.1.1 1.25.2      Upgrade complete
# 3        deployed    nginx-15.2.0 1.25.3      Upgrade complete

# Rollback a versión anterior
helm rollback my-nginx 2

# Salida:
# Rollback was a success! Happy Helming!

# Ver estado después de rollback
helm status my-nginx

# Rollback a la versión anterior inmediata
helm rollback my-nginx

# Rollback con limpieza de datos nuevos
helm rollback my-nginx 1 --cleanup-on-fail
----

**Downgrade (cambiar a versión anterior del Chart):**

[source,bash]
----
# Downgrade a versión anterior específica del Chart
helm upgrade my-nginx bitnami/nginx --version 15.1.1

# Ver Chart version disponibles
helm search repo bitnami/nginx --versions | head -20

# Downgrade y cambiar valores
helm upgrade my-nginx bitnami/nginx --version 15.1.1 \
  --set replicaCount=2
----

==== Gestión de Releases

**Ver información de releases:**

[source,bash]
----
# Listar todas las releases
helm list

# Listar releases en namespace específico
helm list -n production

# Listar todas las releases (todos los namespaces)
helm list -A

# Listar con detalles adicionales
helm list --output json

# Ver valores de una release
helm get values my-nginx

# Ver todos los valores (incluyendo defaults del Chart)
helm get values my-nginx --all

# Ver el Chart.yaml de una release
helm get chart my-nginx

# Ver manifiestos YAML desplegados
helm get manifest my-nginx

# Ver notas (NOTES.txt) de una release
helm get notes my-nginx
----

**Desinstalación de releases:**

[source,bash]
----
# Desinstalar release
helm uninstall my-nginx

# Salida:
# release "my-nginx" uninstalled

# Desinstalar manteniendo el histórico (permite reinstalar)
helm uninstall my-nginx --keep-history

# Ver histórico después de desinstalar
helm history my-nginx

# Reinstalar desde histórico
helm upgrade --install my-nginx bitnami/nginx

# Desinstalar con timeout personalizado
helm uninstall my-nginx --timeout 5m0s

# Desinstalar todo en un namespace
helm uninstall --all -n production
----

==== Estrategias de Instalación Avanzadas

**Instalación o actualización (Helm Install or Upgrade):**

[source,bash]
----
# Comando que instala si no existe, actualiza si existe
# (Muy útil en CI/CD pipelines)
helm upgrade --install my-app myrepo/myapp \
  -f values.yaml \
  -n production \
  --create-namespace

# Con opciones adicionales
helm upgrade --install my-app myrepo/myapp \
  --values values-prod.yaml \
  --set image.tag="v2.0.0" \
  --namespace production \
  --create-namespace \
  --atomic \
  --timeout 10m0s
----

**Instalaciones paralelas de mismo Chart:**

[source,bash]
----
# Instalar múltiples releases del mismo Chart
helm install nginx-prod bitnami/nginx \
  --namespace production \
  --set service.type=LoadBalancer \
  --set replicaCount=5

helm install nginx-staging bitnami/nginx \
  --namespace staging \
  --set service.type=ClusterIP \
  --set replicaCount=2

helm install nginx-dev bitnami/nginx \
  --namespace development \
  --set service.type=ClusterIP \
  --set replicaCount=1

# Ver todas
helm list -A

# Todos comparten el mismo Chart pero con configuraciones diferentes
----

**Actualización en cascada de múltiples releases:**

[source,bash]
----
#!/bin/bash
# Script para actualizar múltiples releases en secuencia

RELEASES=("myapp-dev" "myapp-staging" "myapp-prod")
CHART="myrepo/myapp"
VERSION="2.0.0"

for release in "${RELEASES[@]}"; do
  echo "Actualizando $release..."
  helm upgrade "$release" "$CHART" --version "$VERSION"

  # Esperar a que se estabilice
  kubectl rollout status deployment/"$release" -n "${release%-*}"

  echo "$release actualizado correctamente"
done
----

==== Best Practices para Trabajar con Helm

1. *Siempre usar archivos values en Git*
   - Versionear values-prod.yaml, values-staging.yaml en Git
   - Facilita auditoría y reversión de cambios
   - Reproducibilidad: el mismo archivo = resultados idénticos
   - Comentar valores complejos en YAML

2. *Validación antes de aplicar cambios*
   - Usar helm lint para validar Chart locales
   - Usar helm template para ver manifiestos finales
   - Usar helm diff para ver cambios antes de upgrade
   - Usar --dry-run antes de instalar en producción

3. *Usar helm upgrade --install en CI/CD*
   - Idempotente: puede ejecutarse múltiples veces
   - Funciona en instalación inicial y upgrades
   - Simplifica pipelines de deployment
   - Usar --atomic para rollback automático en errores

4. *Gestión de versiones de Charts*
   - Especificar versión exacta en producción
   - helm upgrade --version X.Y.Z myapp myrepo/myapp
   - No usar "latest" en producción
   - Mantener versionamiento semántico

5. *Monitoreo de releases*
   - Revisar regularmente: helm list -A
   - Documentar qué versión está en qué ambiente
   - Monitorear helm get values para cambios
   - Auditar con: helm history release_name

6. *Rollback seguro*
   - Mantener backup de releases importante: helm get manifest
   - Entender qué hace cada revision antes de rollback
   - Usar --atomic en upgrades para evitar estados inconsistentes
   - Probar en staging antes de producción

7. *Namespace y RBAC*
   - Usar --create-namespace para namespaces nuevos
   - Usar RBAC para restringir quién puede instalar/actualizar
   - Usar service accounts con permisos limitados
   - Implementar policies: solo ciertas personas en producción

8. *Documentación y mantenimiento*
   - Mantener README con instrucciones de instalación
   - Documentar cambios de configuración por versión
   - Explicar por qué ciertos valores están en producción
   - Mantener rotación de versiones viejas

=== Creación de Charts

Crear Charts es la forma más poderosa de usar Helm, permitiendo empaquetar aplicaciones complejas con toda su configuración, valores personalizables y lógica de despliegue. Los Charts bien diseñados hacen posible desplegar la misma aplicación en múltiples ambientes con mínimas variaciones.

==== Creación de un Chart desde Cero

La forma más fácil de comenzar es usando el comando `helm create`, que genera una estructura de Chart estándar:

[source,bash]
----
# Crear Chart básico
helm create myapp

# Esto genera la siguiente estructura:
# myapp/
# ├── Chart.yaml              # Metadatos del Chart
# ├── values.yaml             # Valores por defecto
# ├── charts/                 # Dependencias (subdirectorio vacío)
# ├── templates/              # Plantillas
# │   ├── deployment.yaml
# │   ├── service.yaml
# │   ├── ingress.yaml
# │   ├── NOTES.txt
# │   ├── _helpers.tpl
# │   └── tests/
# │       └── test-connection.yaml
# └── .helmignore

# Examinar la estructura
tree myapp

# Entrar al directorio
cd myapp

# Validar el Chart
helm lint .

# Ver qué se generaría
helm template myapp .
----

**Configurar Chart.yaml:**

[source,yaml]
----
apiVersion: v2
name: myapp                    # Nombre único del Chart
description: "Mi aplicación"   # Descripción
type: application              # application o library

version: 0.1.0                 # Versión del Chart (semver)
appVersion: "1.0"              # Versión de la aplicación

keywords:
  - myapp
  - deployment
  - production

home: https://github.com/user/myapp
sources:
  - https://github.com/user/myapp

maintainers:
  - name: John Doe
    email: john@example.com

kubeVersion: ">=1.20.0"         # Versión mínima de Kubernetes

# Dependencias (si las hay)
dependencies: []
----

**Estructurar values.yaml:**

[source,yaml]
----
# Información global
global:
  environment: production
  domain: example.com

# Configuración de réplicas
replicaCount: 3

# Configuración de imagen
image:
  repository: myapp
  pullPolicy: IfNotPresent
  tag: "1.0.0"

# Configuración de servicio
service:
  type: ClusterIP
  port: 80
  targetPort: 8080
  annotations: {}

# Recursos
resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

# Escalado automático
autoscaling:
  enabled: false
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80

# Sondeos de salud
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5

# Persistencia
persistence:
  enabled: false
  storageClass: ""
  size: 1Gi
  mountPath: /data

# Afinidad y tolerancias
nodeSelector: {}
tolerations: []
affinity: {}

# Anotaciones del pod
podAnnotations: {}

# Labels adicionales
podLabels: {}

# Security Context
securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
----

==== Template Functions y Pipelines

Las funciones transforman y manipulan valores. Los pipelines encadenan funciones.

**Funciones de string:**

[source,yaml]
----
# Convertir a minúsculas
name: {{ .Values.app.name | lower }}

# Convertir a mayúsculas
env: {{ .Values.environment | upper }}

# Agregar comillas
version: {{ .Values.appVersion | quote }}

# Truncar a N caracteres
short-name: {{ .Values.app.name | trunc 10 }}

# Agregar prefijo/sufijo
instance: {{ .Release.Name | printf "instance-%s" }}

# Reemplazar strings
modified: {{ .Values.text | replace "old" "new" }}

# Dividir y join
path: {{ .Values.paths | join ":" }}

# Cadena de pipes (encadenar funciones)
label: {{ .Values.label | lower | upper | quote }}

# Conversión de tipos
count: {{ .Values.replicas | int }}
ratio: {{ .Values.ratio | float64 }}
boolean: {{ .Values.debug | default false | not }}
----

**Funciones de listas:**

[source,yaml]
----
# Primer elemento
first: {{ first .Values.items }}

# Último elemento
last: {{ last .Values.items }}

# Iniciales (todos menos el último)
initial: {{ initial .Values.items }}

# Tail (todos menos el primero)
rest: {{ rest .Values.items }}

# Contar elementos
count: {{ .Values.items | len }}

# Invertir orden
reversed: {{ .Values.items | reverse }}

# Uniq (remover duplicados)
unique: {{ .Values.items | uniq }}

# Ordenar
sorted: {{ .Values.items | sortAlpha }}

# Índice específico
second: {{ index .Values.items 1 }}

# Buscar en lista
contains: {{ has "value" .Values.items }}
----

**Funciones de diccionarios:**

[source,yaml]
----
# Obtener valor con default
value: {{ .Values.config.key | default "default-value" }}

# Empty check
{{- if .Values.config.optional }}
enabled: true
{{- end }}

# Merge dos maps
merged: {{ merge .Values.defaults .Values.custom }}

# Claves del map
keys: {{ keys .Values.config }}

# Valores del map
values: {{ values .Values.config }}

# Has key
{{- if hasKey .Values.config "special" }}
found: true
{{- end }}
----

**Funciones de conversión:**

[source,yaml]
----
# YAML a string
config: |
  {{ toYaml .Values.configMap | nindent 2 }}

# JSON
json: {{ toPrettyJson .Values.config }}

# A mapa/diccionario
parsed: {{ fromJson .Values.jsonString }}

# Codificación base64
encoded: {{ b64enc "secret-value" }}
decoded: {{ b64dec "c2VjcmV0LXZhbHVl" }}
----

**Funciones matemáticas:**

[source,yaml]
----
# Suma/Resta/Multiplicación/División
result: {{ add 5 3 }}          # 8
result: {{ sub 10 4 }}         # 6
result: {{ mul 3 4 }}          # 12
result: {{ div 20 4 }}         # 5

# Módulo
remainder: {{ mod 10 3 }}      # 1

# Min/Max
minimum: {{ min 5 2 8 }}       # 2
maximum: {{ max 5 2 8 }}       # 8

# Redondear
rounded: {{ 3.14159 | round 2 }}   # 3.14
ceil: {{ 3.2 | ceil }}             # 4
floor: {{ 3.8 | floor }}           # 3
----

**Funciones de generación:**

[source,yaml]
----
# Generar UUID
uuid: {{ uuidv4 }}

# Generar strings aleatorios
random: {{ randAlphaNum 10 }}
random-alpha: {{ randAlpha 8 }}
random-numeric: {{ randNumeric 5 }}

# Generar con seed (reproducible)
seeded: {{ randAlphaNumSeed (now | unixEpoch) 10 }}
----

==== Control de Flujo

Las sentencias de control permiten lógica condicional y loops en templates.

**Condicionales if/else:**

[source,yaml]
----
# If simple
{{- if .Values.autoscaling.enabled }}
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{ .Release.Name }}
spec:
  minReplicas: {{ .Values.autoscaling.minReplicas }}
{{- end }}

# If/else
{{- if .Values.database.enabled }}
DATABASE_HOST: "{{ .Values.database.host }}"
{{- else }}
DATABASE_HOST: "localhost"
{{- end }}

# If/else if/else
{{- if eq .Values.environment "production" }}
REPLICAS: 5
{{- else if eq .Values.environment "staging" }}
REPLICAS: 3
{{- else }}
REPLICAS: 1
{{- end }}

# Operadores de comparación
{{- if eq .Values.type "web" }}...{{- end }}      # igual
{{- if ne .Values.type "web" }}...{{- end }}      # no igual
{{- if lt .Values.count 5 }}...{{- end }}         # menor que
{{- if le .Values.count 5 }}...{{- end }}         # menor o igual
{{- if gt .Values.count 5 }}...{{- end }}         # mayor que
{{- if ge .Values.count 5 }}...{{- end }}         # mayor o igual

# Operadores lógicos
{{- if and .Values.tls.enabled .Values.tls.cert }}...{{- end }}      # AND
{{- if or .Values.debug .Values.verbose }}...{{- end }}              # OR
{{- if not .Values.disabled }}...{{- end }}                          # NOT

# With (cambiar scope)
{{- with .Values.database }}
host: {{ .host }}
port: {{ .port }}
{{- end }}
----

**Loops y rangos:**

[source,yaml]
----
# Iterar sobre array
ports:
{{- range .Values.ports }}
  - name: {{ .name }}
    port: {{ .port }}
{{- end }}

# Iterar con índice
containers:
{{- range $index, $container := .Values.containers }}
  - name: {{ $container.name }}
    index: {{ $index }}
{{- end }}

# Iterar sobre map/dictionary
env:
{{- range $key, $value := .Values.environment }}
  - name: {{ $key }}
    value: {{ $value | quote }}
{{- end }}

# Iterar con counter
items:
{{- range $i := until 5 }}
  - item: {{ $i }}
{{- end }}

# Iterar sobre range numérico
numbers:
{{- range .Values.numbers }}
  - number: {{ . }}
{{- end }}
----

**Indentación correcta con nindent:**

[source,yaml]
----
# nindent: Indenta líneas nuevas (importante para YAML)
apiVersion: v1
kind: ConfigMap
data:
  config.yaml: |
    {{- .Values.config | toYaml | nindent 4 }}

# Esto produce:
# apiVersion: v1
# kind: ConfigMap
# data:
#   config.yaml: |
#     key: value
#     nested:
#       key: value

# Sin nindent (INCORRECTO):
# apiVersion: v1
# kind: ConfigMap
# data:
#   config.yaml: |
#   key: value      # <-- Indentación incorrecta
----

==== Named Templates y Helpers

Las plantillas nombradas (defined templates) permiten reutilizar código entre templates:

**Definir y usar templates:**

[source,yaml]
----
# En _helpers.tpl (archivo especial para helpers)

{{- define "myapp.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 -}}
{{- end -}}

{{- define "myapp.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 -}}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride -}}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 -}}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 -}}
{{- end -}}
{{- end -}}
{{- end -}}

{{- define "myapp.labels" -}}
helm.sh/chart: {{ include "myapp.chart" . }}
{{ include "myapp.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{- define "myapp.selectorLabels" -}}
app.kubernetes.io/name: {{ include "myapp.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

# Usar los helpers en otros templates:
# metadata:
#   labels:
#     {{- include "myapp.labels" . | nindent 4 }}

# selector:
#   matchLabels:
#     {{- include "myapp.selectorLabels" . | nindent 4 }}
----

**Variables en templates:**

[source,yaml]
----
# Declarar variables con :=
{{- $name := include "myapp.fullname" . }}
{{- $labels := include "myapp.labels" . }}

# Usar variables
metadata:
  name: {{ $name }}
  labels:
    {{- $labels | nindent 4 }}

# Variables en loops
labels:
{{- range $key, $value := .Values.labels }}
{{- $fullKey := printf "%s-%s" $name $key }}
  {{ $fullKey }}: {{ $value }}
{{- end }}
----

**Scope en templates:**

[source,yaml]
----
# El . (dot) representa el contexto actual
# $root captura el contexto raíz
{{- $root := . }}

{{- range .Values.containers }}
# Aquí, . es el container, no el root
name: {{ .name }}

# Usar $root para acceder valores del root
labels: {{ include "myapp.labels" $root | nindent 2 }}
{{- end }}
----

==== Testing de Charts

Validar y probar Charts antes de usarlos en producción:

**Validación con helm lint:**

[source,bash]
----
# Validación básica
helm lint ./myapp

# Salida:
# ==> Linting ./myapp
# [INFO] Chart.yaml: icon is recommended
# [WARNING] templates/: object name does not conform to DNS naming rules: "my_app"
# 1 chart(s) linted, 0 error(s), 1 warning(s)

# Validación estricta (falla en warnings)
helm lint ./myapp --strict

# Validación con valores personalizados
helm lint ./myapp -f values-prod.yaml

# Validación de todas las versiones
helm lint ./myapp --versions
----

**Verificar templates con helm template:**

[source,bash]
----
# Ver manifiestos generados
helm template my-release ./myapp

# Salida: manifiestos YAML compilados

# Con valores específicos
helm template my-release ./myapp -f values-prod.yaml

# Con valores desde --set
helm template my-release ./myapp \
  --set replicaCount=5 \
  --set image.tag="1.5.0"

# Guardar manifiestos en archivo
helm template my-release ./myapp > manifests.yaml

# Validar manifiestos con kubeval
helm template my-release ./myapp | kubeval

# Salida:
# PASS - pods-deployment.yaml contains a valid Deployment
# PASS - pods-service.yaml contains a valid Service
----

**Testing automático en Charts:**

[source,bash]
----
# Charts pueden incluir tests en templates/tests/
# Crear test de conectividad:
----

En `templates/tests/test-connection.yaml`:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: "{{ include "myapp.fullname" . }}-test-connection"
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['{{ include "myapp.fullname" . }}:{{ .Values.service.port }}']
  restartPolicy: Never
----

**Ejecutar tests:**

[source,bash]
----
# Instalar Chart y ejecutar tests
helm install my-app ./myapp
helm test my-app

# Salida:
# Pod my-app-test-connection pending
# Pod my-app-test-connection succeeded
# NAME: my-app
# LAST DEPLOYED: ...
# NAMESPACE: default
# STATUS: deployed
# ...
# TEST SUITE:     my-app-test-connection
# Last Started:   ...
# Last Completed: ...
# Phase:          Succeeded

# Ver logs del test
kubectl logs my-app-test-connection
----

**Testing frameworks:**

[source,bash]
----
# Chart Testing (ct) - Framework de testing para Charts
# Instalar: https://github.com/helm/chart-testing

ct list-changed

# Ejecutar linting en Charts modificados
ct lint --chart-dirs . --validate-maintainers=false

# Instalar y probar Charts en cluster real
ct install --chart-dirs . --chart-repos bitnami=https://charts.bitnami.com/bitnami

# Configuración en .ct/lintconf.yaml:
# general:
#   chart-testing-image: quay.io/helmpack/chart-testing:v3.10.0
# lint:
#   strict: true
#   chart-repos:
#     - bitnami=https://charts.bitnami.com/bitnami
----

**Testing unitario con Helm:**

[source,bash]
----
# Usar helm unittest (plugin)
helm plugin install https://github.com/helm-unittest/helm-unittest.git

# En tests/deployment_test.yaml:
----

[source,yaml]
----
suite: test deployment
templates:
  - deployment.yaml
tests:
  - it: should have correct replicas
    set:
      replicaCount: 3
    asserts:
      - equal:
          path: spec.replicas
          value: 3

  - it: should set resource limits
    set:
      resources:
        limits:
          cpu: "1"
    asserts:
      - equal:
          path: spec.template.spec.containers[0].resources.limits.cpu
          value: "1"

  - it: should not have hpa when disabled
    set:
      autoscaling.enabled: false
    template: hpa.yaml
    asserts:
      - hasDocuments:
          count: 0
----

**Ejecutar tests unitarios:**

[source,bash]
----
helm unittest ./myapp

# Salida:
# === RUN   suite: test deployment
# === RUN   suite: test deployment  it should have correct replicas
# === RUN   suite: test deployment  it should set resource limits
# === RUN   suite: test deployment  it should not have hpa when disabled
# === RUN PASS suite: test deployment
# --- PASS: myapp/templates/deployment.yaml (0.15s)
# --- PASS: myapp/templates/hpa.yaml (0.10s)
# --- PASS: myapp/templates/service.yaml (0.08s)
# PASS - 12 tests in 0.35s
----

==== Best Practices para Crear Charts

1. *Estructura consistente*
   - Seguir estructura estándar (Chart.yaml, values.yaml, templates/)
   - Usar helm create para empezar
   - Mantener helpers en _helpers.tpl
   - Documentar helpers complejos

2. *Values bien diseñados*
   - Organizar lógicamente (app, database, cache, etc.)
   - Proporcionar defaults seguros
   - Documentar cada valor
   - Validar ranges (min replicas, max resources)
   - Ejemplos en comentarios

3. *Templates limpios*
   - Usar funciones reutilizables
   - Evitar hardcoding de valores
   - Mantener indentación consistente
   - Usar nindent para YAML correcto
   - Comentar lógica compleja

4. *Seguridad*
   - No incluir secrets en values.yaml
   - Usar referencias externas a Secrets
   - Implementar RBAC restrictivo
   - SecurityContext en Pods
   - Validar inputs de usuarios

5. *Testing exhaustivo*
   - Usar helm lint regularmente
   - helm template antes de instalar
   - Tests en templates/tests/
   - Chart Testing (ct) framework
   - Unit tests con helm unittest

6. *Control de versiones*
   - Semantic Versioning (MAJOR.MINOR.PATCH)
   - Cambiar version en cada release
   - CHANGELOG.md actualizado
   - Git tags con versión
   - Historial claro de cambios

7. *Compatibilidad y dependencias*
   - Especificar kubeVersion mínimo
   - Versiones de dependencias explícitas
   - Documentar requisitos previos
   - Probar en múltiples versiones
   - Mantener Chart compatible

8. *Documentación*
   - README.md completo
   - NOTES.txt con post-instalación
   - Ejemplos en values.yaml
   - Comentarios en templates complejos
   - Documentar breaking changes

=== Helm Avanzado

Los features avanzados de Helm permiten implementar patrones sofisticados de despliegue y gestión de aplicaciones. Esto incluye hooks para ejecuciones en momentos específicos del ciclo de vida, gestión de dependencias complejas, y distribución de Charts a través de repositorios.

==== Hooks (Ganchos de Ciclo de Vida)

Los hooks permiten ejecutar acciones en momentos específicos durante la instalación, actualización o eliminación de un release.

**Tipos de hooks disponibles:**

[source,text]
----
pre-install      Ejecuta ANTES de instalar un Chart
post-install     Ejecuta DESPUÉS de instalar un Chart
pre-upgrade      Ejecuta ANTES de actualizar un release
post-upgrade     Ejecuta DESPUÉS de actualizar un release
pre-delete       Ejecuta ANTES de eliminar un release
post-delete      Ejecuta DESPUÉS de eliminar un release
pre-rollback     Ejecuta ANTES de hacer rollback
post-rollback    Ejecuta DESPUÉS de hacer rollback
test             Ejecuta cuando se ejecuta "helm test"
----

**Diagrama del ciclo de vida con hooks:**

[source,text]
----
helm install myapp
         ↓
  pre-install Hook
         ↓
  Crear recursos (Deployment, Service, etc.)
         ↓
  post-install Hook
         ↓
  Release "deployed"

helm upgrade myapp
         ↓
  pre-upgrade Hook
         ↓
  Actualizar recursos
         ↓
  post-upgrade Hook
         ↓
  Release "deployed"

helm uninstall myapp
         ↓
  pre-delete Hook
         ↓
  Eliminar recursos
         ↓
  post-delete Hook
         ↓
  Release "uninstalled"
----

**Crear un hook post-install (ej: inicializar base de datos):**

[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "myapp.fullname" . }}-db-init
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-weight": "1"          # Peso (orden de ejecución)
    "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        {{- include "myapp.labels" . | nindent 8 }}
    spec:
      serviceAccountName: {{ include "myapp.fullname" . }}
      containers:
      - name: db-init
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        command: ["/bin/sh"]
        args:
          - -c
          - |
            echo "Inicializando base de datos..."
            /app/scripts/init-db.sh
        env:
        - name: DATABASE_HOST
          value: "{{ .Values.database.host }}"
        - name: DATABASE_USER
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: username
      restartPolicy: Never
----

**Hook de pre-upgrade (ej: backup de datos):**

[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "myapp.fullname" . }}-backup
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": pre-upgrade
    "helm.sh/hook-weight": "-5"         # Pesos negativos se ejecutan primero
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  backoffLimit: 1
  template:
    metadata:
      labels:
        {{- include "myapp.labels" . | nindent 8 }}
    spec:
      serviceAccountName: {{ include "myapp.fullname" . }}
      containers:
      - name: backup
        image: "postgresql:15"
        command: ["/bin/bash"]
        args:
          - -c
          - |
            echo "Backup de base de datos previo al upgrade..."
            pg_dump -h {{ .Values.database.host }} \
              -U {{ .Values.database.user }} \
              -d {{ .Values.database.name }} > /backups/db_backup_$(date +%s).sql
            echo "Backup completado"
        volumeMounts:
        - name: backup-storage
          mountPath: /backups
      volumes:
      - name: backup-storage
        persistentVolumeClaim:
          claimName: backup-pvc
      restartPolicy: Never
----

**Hook de test (validar instalación):**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: "{{ include "myapp.fullname" . }}-smoke-test"
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": test
spec:
  containers:
  - name: smoke-test
    image: curlimages/curl:latest
    command:
      - sh
      - -c
      - |
        echo "Ejecutando smoke tests..."

        # Test de health check
        curl -f http://{{ include "myapp.fullname" . }}:{{ .Values.service.port }}/health \
          || exit 1

        # Test de endpoint principal
        curl -f http://{{ include "myapp.fullname" . }}:{{ .Values.service.port }}/api/status \
          || exit 1

        echo "Todos los tests pasaron!"
  restartPolicy: Never
----

**Hook delete-policy (gestionar limpieza):**

[source,yaml]
----
# Opciones de hook-delete-policy:
"helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"

# before-hook-creation: Eliminar Hook previo si existe
# hook-succeeded:       Eliminar Hook después de ejecución exitosa
# hook-failed:          Eliminar Hook después de fallo
# none:                 No eliminar (por defecto)

# Ejemplo: Hook que persiste para debugging
annotations:
  "helm.sh/hook": post-install
  "helm.sh/hook-delete-policy": ""  # No eliminar, útil para investigar fallos
----

**Pesos de hooks para controlar orden:**

[source,bash]
----
# Los hooks con el mismo tipo se ordenan por peso (ascendente)
# Pesos negativos se ejecutan antes

# Ejecución de múltiples hooks post-install:
annotations:
  "helm.sh/hook": post-install
  "helm.sh/hook-weight": "-10"    # Primero (se ejecuta primero)
---
annotations:
  "helm.sh/hook": post-install
  "helm.sh/hook-weight": "0"      # Segundo
---
annotations:
  "helm.sh/hook": post-install
  "helm.sh/hook-weight": "10"     # Tercero
----

==== Gestión de Dependencias de Charts

Las dependencias permiten incluir otros Charts dentro del tuyo:

**Declarar dependencias en Chart.yaml:**

[source,yaml]
----
apiVersion: v2
name: myapp-stack
version: 1.0.0

dependencies:
  - name: postgresql
    version: "12.1.0"
    repository: "https://charts.bitnami.com/bitnami"
    condition: postgresql.enabled
    tags:
      - database
    alias: database

  - name: redis
    version: "17.3.0"
    repository: "https://charts.bitnami.com/bitnami"
    condition: redis.enabled
    tags:
      - cache

  - name: prometheus
    version: "15.0.0"
    repository: "https://prometheus-community.github.io/helm-charts"
    condition: prometheus.enabled
    tags:
      - monitoring
    import-values:
      - child

# En values.yaml, configurar las dependencias:
postgresql:
  enabled: true
  auth:
    username: myapp_user
    password: changeme123
    database: myapp_db
  primary:
    persistence:
      enabled: true
      size: 20Gi

redis:
  enabled: true
  auth:
    enabled: true
    password: cacheme123
  replica:
    replicaCount: 2

prometheus:
  enabled: false  # Deshabilitada por defecto
----

**Comandos para manejar dependencias:**

[source,bash]
----
# Descargar dependencias definidas en Chart.yaml
helm dependency update ./myapp-stack

# Salida:
# Hang tight while we grab the latest from your chart repositories...
# ...Successfully got an update from the "bitnami" chart repository
# Update Complete. ⎈ Happy Helming!
# Saving 3 charts
# Deleting outdated charts

# Ver estado de dependencias
helm dependency list ./myapp-stack

# Salida:
# NAME         VERSION REPOSITORY                              STATUS
# postgresql   12.1.0  https://charts.bitnami.com/bitnami      ok
# redis        17.3.0  https://charts.bitnami.com/bitnami      ok
# prometheus   15.0.0  https://prometheus-community.github.io   ok

# Ver estructura después de descargar dependencias
tree ./myapp-stack

# Salida:
# myapp-stack/
# ├── Chart.yaml
# ├── values.yaml
# ├── templates/
# ├── charts/
# │   ├── postgresql-12.1.0/
# │   ├── redis-17.3.0/
# │   ├── prometheus-15.0.0/
# │   └── Chart.lock
# └── Chart.lock
----

**Usar valores de dependencias (subcharts):**

[source,yaml]
----
# En templates, acceder a valores de dependencias:
env:
  DATABASE_HOST: "{{ .Release.Name }}-postgresql"
  DATABASE_PORT: "5432"
  DATABASE_NAME: "{{ .Values.postgresql.auth.database }}"
  DATABASE_USER: "{{ .Values.postgresql.auth.username }}"

  CACHE_HOST: "{{ .Release.Name }}-redis-master"
  CACHE_PORT: "6379"

  PROMETHEUS_ENABLED: "{{ .Values.prometheus.enabled }}"
  {{- if .Values.prometheus.enabled }}
  PROMETHEUS_URL: "http://{{ .Release.Name }}-prometheus:9090"
  {{- end }}
----

**Habilitar/Deshabilitar dependencias con tags:**

[source,bash]
----
# Instalar sin redis (deshabilitarlo)
helm install myapp ./myapp-stack \
  --set redis.enabled=false

# Instalar solo dependencias con tag "database"
helm install myapp ./myapp-stack \
  --set postgresql.enabled=true \
  --set redis.enabled=false \
  --set prometheus.enabled=false

# O usar tags:
helm install myapp ./myapp-stack \
  --set tags.database=true \
  --set tags.cache=false \
  --set tags.monitoring=false
----

==== Subcharts (Chart Reutilizables)

Un subchart es un Chart que se incluye dentro de otro. Permite reutilizar configuraciones comunes:

**Estructura de Subcharts:**

[source,text]
----
myapp-charts/
├── charts/
│   ├── common/                    # Subchart reutilizable
│   │   ├── Chart.yaml
│   │   ├── values.yaml
│   │   ├── templates/
│   │   │   ├── _helpers.tpl
│   │   │   ├── configmap.yaml
│   │   │   └── secret.yaml
│   │   └── README.md
│   ├── database-init/
│   │   ├── Chart.yaml
│   │   ├── templates/
│   │   │   └── job.yaml
│   │   └── values.yaml
│   └── monitoring/
│       ├── Chart.yaml
│       ├── templates/
│       └── values.yaml
├── Chart.yaml
├── values.yaml
└── templates/
----

**Usar alias para múltiples instancias de subchart:**

[source,yaml]
----
# En Chart.yaml: Usar mismo subchart varias veces con alias
dependencies:
  - name: database
    version: "1.0.0"
    repository: file://../charts/database
    alias: db_main        # Alias para base de datos principal
    condition: databases.main.enabled

  - name: database
    version: "1.0.0"
    repository: file://../charts/database
    alias: db_cache       # Alias para base de datos de cache
    condition: databases.cache.enabled

  - name: database
    version: "1.0.0"
    repository: file://../charts/database
    alias: db_backup      # Alias para base de datos de backup
    condition: databases.backup.enabled

# En values.yaml:
databases:
  main:
    enabled: true
    name: myapp
    replica: 3
    size: 100Gi

  cache:
    enabled: true
    name: myapp-cache
    replica: 1
    size: 10Gi

  backup:
    enabled: false
    name: myapp-backup
    replica: 1
    size: 50Gi
----

**Importar valores de subcharts:**

[source,yaml]
----
# Chart.yaml: Importar ciertos valores del subchart al padre
dependencies:
  - name: database
    version: "1.0.0"
    repository: "https://charts.example.com"
    import-values:
      - child: database.settings    # Subchart path
        parent: settings             # Parent path

# Subchart values.yaml:
database:
  settings:
    maxConnections: 100
    timeout: 30
    ssl: true

# Esto hace disponibles estos valores en parent como:
# .Values.settings.maxConnections
# .Values.settings.timeout
# .Values.settings.ssl
----

==== Repositorios de Helm: Hosting y Distribución

Un repositorio de Helm es un servidor HTTP que aloja Chart empaquetados. Permite distribuir Charts a otros usuarios.

**Estructura de un repositorio:**

[source,text]
----
https://charts.example.com/
├── index.yaml           # Índice de Charts (generado automáticamente)
├── myapp-1.0.0.tgz      # Charts empaquetados
├── myapp-1.1.0.tgz
├── other-app-2.0.0.tgz
└── other-app-2.1.0.tgz
----

**Empaquetar un Chart:**

[source,bash]
----
# Crear archivo .tgz del Chart
helm package ./myapp

# Salida:
# Successfully packaged chart and saved it to: /home/user/myapp-1.0.0.tgz

# Ver contenido del archivo empaquetado
tar -tzf myapp-1.0.0.tgz | head -20

# Empaquetar múltiples Charts
helm package ./myapp ./other-app

# Versionar automáticamente (incrementar version en Chart.yaml)
helm package ./myapp --version 1.1.0
----

**Crear repositorio local:**

[source,bash]
----
# Crear directorio para repositorio
mkdir my-helm-repo
cd my-helm-repo

# Empaquetar Charts
helm package ../myapp
helm package ../other-app

# Generar índice (archivo index.yaml)
helm repo index .

# Ver contenido del índice
cat index.yaml

# Salida:
# apiVersion: v1
# entries:
#   myapp:
#   - apiVersion: v2
#     appVersion: "1.0"
#     created: 2024-01-15T10:30:00.123Z
#     description: My awesome application
#     digest: sha256:abc123...
#     name: myapp
#     urls:
#     - myapp-1.0.0.tgz
#     version: 1.0.0
#   other-app:
#   - ...

# Actualizar índice (después de agregar nuevos Charts)
helm repo index . --merge ./index.yaml
----

**Servir repositorio localmente (desarrollo):**

[source,bash]
----
# Opción 1: Usando Python
cd my-helm-repo
python3 -m http.server 8080

# Opción 2: Usando nginx
docker run -p 8080:80 -v $(pwd):/usr/share/nginx/html:ro nginx

# Opción 3: Usando helm serve (deprecado)
helm serve --repo-path ./

# Usar el repositorio local
helm repo add myrepo http://localhost:8080
helm repo update
helm search repo myrepo
----

**Publicar repositorio en GitHub Pages:**

[source,bash]
----
# Crear rama gh-pages
git checkout --orphan gh-pages

# Copiar Charts empaquetados y index.yaml
mkdir -p charts
helm package ../myapp -d charts/
helm package ../other-app -d charts/
helm repo index charts/ --url https://username.github.io/helm-charts

# Commitear y pushear
git add .
git commit -m "Add Helm Charts"
git push origin gh-pages

# Agregar repositorio
helm repo add myrepo https://username.github.io/helm-charts
helm repo update
----

**Publicar en Helm Hub (hub.helm.sh):**

[source,bash]
----
# 1. Crear repositorio público en GitHub
# 2. Asegurar que Chart está bien documentado (Chart.yaml completo)
# 3. Crear Chart.yaml con todos los metadatos necesarios
# 4. Submeter a Helm Hub en: https://github.com/helm/hub/issues

# Requisitos:
# - Chart.yaml bien formado
# - README.md
# - Licencia (LICENSE file)
# - Mantenedores listados
# - Repository público en GitHub
# - Helm 3 compatible (apiVersion: v2)
----

**Repositorio privado con autenticación:**

[source,bash]
----
# Repositorio con autenticación HTTP básica
helm repo add myrepo https://charts.mycompany.com \
  --username myuser \
  --password mypassword

# O guardar credenciales en ~/.netrc (menos seguro)
machine charts.mycompany.com
  login myuser
  password mypassword

# Repositorio con certificados (mTLS)
helm repo add myrepo https://charts.mycompany.com \
  --ca-file ./ca.crt \
  --cert-file ./client.crt \
  --key-file ./client.key

# Ver y actualizar repositorio privado
helm repo update myrepo
helm search repo myrepo
----

**Ejemplo: Servidor Helm con Chartmuseum:**

[source,bash]
----
# Instalar Chartmuseum (servidor Helm completo)
helm repo add chartmuseum https://chartmuseum.github.io/charts
helm install chartmuseum chartmuseum/chartmuseum \
  --set persistence.enabled=true \
  --set persistence.size=10Gi

# Usar Chartmuseum
helm repo add mycharts http://chartmuseum-service:8080
helm push ./myapp mycharts

# O usar curl
curl --data-binary "@myapp-1.0.0.tgz" \
  http://localhost:8080/api/charts

# Descargar desde Chartmuseum
helm repo update
helm install myapp mycharts/myapp
----

==== Best Practices para Helm Avanzado

1. *Hooks seguros*
   - Usar hook-delete-policy apropiadamente
   - Implementar reintentos en Jobs críticos
   - Loguear salida de Hooks para debugging
   - Probar Hooks en staging primero
   - No usar Hooks para lógica crítica (usar operadores)

2. *Dependencias declaradas explícitamente*
   - Especificar versiones exactas de dependencias
   - Documentar por qué se necesita cada dependencia
   - Usar conditions para habilitar opcionalmente
   - Mantener dependencias actualizadas
   - Probar compatibilidad entre versiones

3. *Subcharts bien modulados*
   - Cada subchart debe ser independiente
   - Proporcionar valores por defecto útiles
   - Documentar interface (qué valores espera)
   - Usar alias para múltiples instancias
   - Mantener versionamiento semántico

4. *Repositorios seguros*
   - Usar HTTPS para todos los repositorios
   - Implementar autenticación en repos privados
   - Verificar integridad de Charts (checksums)
   - Auditar cambios en repositorio
   - Mantener backups de Charts críticos

5. *Gestión de versiones*
   - Usar Semantic Versioning estrictamente
   - Documentar cambios en CHANGELOG.md
   - No reutilizar versiones
   - Mantener histórico de versiones antiguas
   - Considerar compatibilidad hacia atrás

6. *Testing en cada nivel*
   - Tests unitarios para templates
   - Tests de integración en staging
   - Tests de seguridad (kubeval, kubesec)
   - Probar upgrades y rollbacks
   - Validar dependencias funcionan

7. *Documentación exhaustiva*
   - README.md con instrucciones claras
   - Ejemplos de valores para casos comunes
   - Documentar breaking changes
   - Guías de troubleshooting
   - Comentarios en templates complejos

8. *Seguridad en producción*
   - No incluir secrets en values.yaml
   - Usar referencias a Secrets externos
   - Implementar RBAC restrictivo
   - Validar inputs de usuarios
   - Auditar acceso a repositorios privados

== Módulo 11: CI/CD con Kubernetes

=== Estrategias de Deployment

**Blue-Green Deployment**

El patrón Blue-Green proporciona cero downtime al mantener dos ambientes idénticos.

* Blue (ambiente actual): versión en producción
* Green (ambiente nuevo): versión candidata

El tráfico se conmuta completamente de Blue a Green mediante el servicio.

Ventajas:
* Rollback instantáneo cambiando el selector del servicio
* Testing completo del ambiente antes de conmutación
* Bajo riesgo de fallos parciales

Desventajas:
* Requiere el doble de recursos en cluster
* Sincronización de datos complicada si hay estado

Ejemplo de implementación:

```yaml
# Blue - Versión actual en producción
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      version: blue
  template:
    metadata:
      labels:
        app: myapp
        version: blue
    spec:
      containers:
      - name: app
        image: myapp:v1.0.0
        ports:
        - containerPort: 8080

---
# Green - Versión candidata
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-green
spec:
  replicas: 3
  selector:
    matchLabels:
      version: green
  template:
    metadata:
      labels:
        app: myapp
        version: green
    spec:
      containers:
      - name: app
        image: myapp:v2.0.0
        ports:
        - containerPort: 8080

---
# Servicio que apunta actualmente a Blue
apiVersion: v1
kind: Service
metadata:
  name: myapp
spec:
  selector:
    app: myapp
    version: blue  # Cambiar a "green" para conmutar
  ports:
  - port: 80
    targetPort: 8080
  type: LoadBalancer
```

Conmutación del tráfico:
```bash
# Verificar que Green está listo
kubectl get pods -l version=green

# Actualizar selector del servicio
kubectl patch service myapp -p '{"spec":{"selector":{"version":"green"}}}'

# Rollback si es necesario
kubectl patch service myapp -p '{"spec":{"selector":{"version":"blue"}}}'

# Eliminar Blue una vez que Green es estable
kubectl delete deployment app-blue
```

**Canary Deployment**

Despliega la nueva versión gradualmente a un pequeño porcentaje de usuarios.

* Reduce riesgo al exponer cambios a pocos usuarios primero
* Permite validación en producción con tráfico real
* Facilita detección de problemas antes de rollout completo

Estrategia:
1. Desplegar nueva versión con 5-10% del tráfico
2. Monitorear métricas de error y latencia
3. Aumentar gradualmente el porcentaje
4. Completar rollout o rollback basado en resultados

```yaml
# Canary Deployment usando Istio VirtualService
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp
spec:
  hosts:
  - myapp
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        host: myapp
        subset: v1
      weight: 90
    - destination:
        host: myapp
        subset: v2
      weight: 10
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp
spec:
  host: myapp
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

Monitoreo durante canary:

```bash
# Monitorear tasa de errores en v2 (canary)
kubectl exec -it <prometheus-pod> -- \
  promtool query instant 'rate(requests_errors_total{version="v2"}[5m])'

# Si los errores son bajos, aumentar tráfico
kubectl apply -f - <<EOF
# Actualizar weights en VirtualService: v1: 80, v2: 20
EOF

# Completar rollout
kubectl apply -f - <<EOF
# Actualizar weights a v1: 0, v2: 100
EOF
```

**Rolling Updates Avanzados**

Kubernetes realiza rolling updates de forma nativa, pero se pueden optimizar con parámetros avanzados.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 3          # Máximo 3 pods adicionales durante update
      maxUnavailable: 1    # Máximo 1 pod unavailable
  minReadySeconds: 30      # Esperar 30s antes de considerarlo ready
  progressDeadlineSeconds: 600
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: myapp:v2
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
```

Control del rolling update:

```bash
# Ver estado del update
kubectl rollout status deployment/app

# Pausar update si hay problemas
kubectl rollout pause deployment/app

# Resumir update
kubectl rollout resume deployment/app

# Ver historial de cambios
kubectl rollout history deployment/app

# Rollback a versión anterior
kubectl rollout undo deployment/app

# Rollback a revisión específica
kubectl rollout undo deployment/app --to-revision=2

# Ver detalles de una revisión
kubectl rollout history deployment/app --revision=2
```

**A/B Testing**

Divide el tráfico basado en características o criterios para validar cambios con usuarios reales.

Casos de uso:
* Pruebas de interfaz de usuario (UI/UX)
* Validación de nuevas features
* Comparación de algoritmos

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp-ab
spec:
  hosts:
  - myapp
  http:
  # Tráfico de usuario ID par a A
  - match:
    - headers:
        user-id:
          regex: '[0-9]*[02468]$'
    route:
    - destination:
        host: myapp
        subset: version-a
  # Resto del tráfico a B
  - route:
    - destination:
        host: myapp
        subset: version-b
---
apiVersion: v1
kind: Service
metadata:
  name: myapp
spec:
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 8080
```

Análisis de resultados:

```bash
# Comparar métricas entre versiones
kubectl exec -it prometheus-pod -- \
  promtool query instant \
  'avg by (version) (response_time_seconds{app="myapp"})'

# Estadísticas de conversión por versión
kubectl exec -it prometheus-pod -- \
  promtool query instant \
  'rate(conversions_total[1h]) / rate(requests_total[1h]) * 100'
```

=== GitOps

**Principios de GitOps**

GitOps es una metodología que utiliza Git como fuente única de verdad para el estado deseado de la infraestructura y aplicaciones.

Principios fundamentales:

1. **Declarativo**: El estado deseado se describe declarativamente (típicamente en YAML)
2. **Versionado en Git**: Todo el código de configuración reside en Git
3. **Reconciliación automática**: Un operador asegura que el estado real coincida con Git
4. **Observabilidad**: Todas las operaciones están auditadas y visibles

Ventajas:
* Control de cambios mediante Git (PR, reviews, audit trail)
* Rollback fácil: revertir commits en Git revierte cambios en cluster
* Infraestructura como código: mismos procesos que el código fuente
* Automatización: cambios en Git disparan despliegues automáticos
* Seguridad: no requiere acceso directo al cluster para cambios

Flujo de trabajo típico:

```
Desarrollador
    ↓
  Git Push
    ↓
  Pull Request
    ↓
  Review & Merge
    ↓
GitOps Operator (ArgoCD/Flux)
    ↓
  Detecta cambio en Git
    ↓
  Obtiene manifiestos actualizados
    ↓
Sincroniza con Kubernetes Cluster
```

**ArgoCD**

ArgoCD es un controlador de despliegue declarativo para Kubernetes que implementa GitOps.

Instalación:

```bash
# Crear namespace para ArgoCD
kubectl create namespace argocd

# Instalar ArgoCD
kubectl apply -n argocd -f \
  https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# Exponer UI de ArgoCD
kubectl port-forward svc/argocd-server -n argocd 8080:443

# Obtener contraseña inicial
kubectl -n argocd get secret argocd-initial-admin-secret \
  -o jsonpath="{.data.password}" | base64 -d
```

Componentes de ArgoCD:

* **API Server**: Expone UI y API REST
* **Repository Server**: Obtiene manifiestos del repositorio Git
* **Application Controller**: Monitorea aplicaciones y sincroniza estado

Definición de una Application:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: myapp
  namespace: argocd
spec:
  project: default

  source:
    repoURL: https://github.com/myorg/myapp-config
    targetRevision: main
    path: k8s/                    # Ruta a manifiestos en Git

  destination:
    server: https://kubernetes.default.svc
    namespace: default

  syncPolicy:
    automated:
      prune: true                 # Eliminar recursos no en Git
      selfHeal: true              # Resincronizar si cluster diverge
    syncOptions:
    - CreateNamespace=true
```

Monitoreo y sincronización:

```bash
# Listar aplicaciones
argocd app list

# Ver estado detallado de aplicación
argocd app get myapp

# Sincronizar manualmente
argocd app sync myapp

# Esperar a que sync complete
kubectl wait --for=condition=SyncStatusCode=Synced \
  application.argoproj.io/myapp -n argocd

# Ver logs de sincronización
argocd app logs myapp

# Obtener eventos de aplicación
kubectl describe application myapp -n argocd
```

**Flux**

Flux es otro operador GitOps que sincroniza cambios de Git con Kubernetes.

Instalación:

```bash
# Requisitos
export GITHUB_USER=myuser
export GITHUB_TOKEN=ghp_...

# Instalar Flux CLI
curl -s https://fluxcd.io/install.sh | sudo bash

# Crear buckets para Flux
flux bootstrap github \
  --owner=$GITHUB_USER \
  --repo=fleet \
  --branch=main \
  --path=./clusters/my-cluster \
  --personal
```

Estructura de Flux:

```
fleet/
├── clusters/my-cluster/
│   └── flux-system/
│       ├── gotk-components.yaml
│       ├── gotk-sync.yaml
│       └── kustomization.yaml
└── repos/
    ├── myapp-repo.yaml
    └── myapp-source.yaml
```

Definición de fuente de Git:

```yaml
apiVersion: source.toolkit.fluxcd.io/v1
kind: GitRepository
metadata:
  name: myapp
  namespace: flux-system
spec:
  interval: 1m0s
  url: https://github.com/myorg/myapp-config
  ref:
    branch: main
  secretRef:
    name: git-credentials
```

Definición de Kustomization (qué sincronizar):

```yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: myapp
  namespace: flux-system
spec:
  interval: 10m0s
  path: ./k8s/

  sourceRef:
    kind: GitRepository
    name: myapp

  prune: true
  wait: true

  postBuild:
    substitute:
      version: "1.0.0"
```

Comandos útiles de Flux:

```bash
# Ver estado de sincronización
flux get source git myapp

# Forzar resincronización
flux reconcile source git myapp

# Ver eventos de Kustomization
flux get kustomization

# Logs de reconciliación
flux logs --all-namespaces --follow
```

**Reconciliation Loops**

Los loops de reconciliación son el mecanismo central que mantiene sincronizado el estado.

Funcionamiento:

1. Operador verifica periódicamente el estado deseado (Git)
2. Compara con estado actual (cluster)
3. Si hay diferencias, realiza cambios para alcanzar estado deseado
4. Repite continuamente (típicamente cada 1-5 minutos)

Ventajas:

* **Auto-healing**: Si alguien elimina un recurso manualmente, se restaura
* **Recuperación de desastres**: Si cluster se corrompe, se restaura desde Git
* **Auditoría**: Cada cambio queda registrado en Git
* **Predictibilidad**: El estado es siempre reproducible desde Git

Configuración de frequency:

```yaml
# Reconciliación cada 30 segundos (más agresivo)
apiVersion: source.toolkit.fluxcd.io/v1
kind: GitRepository
metadata:
  name: myapp
spec:
  interval: 30s

---
# Reconciliación cada 5 minutos (default)
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: myapp
spec:
  interval: 5m0s
```

Monitoreo de reconciliación:

```bash
# ArgoCD: ver estado de sync
argocd app get myapp --refresh

# Flux: ver historial de reconciliación
kubectl describe kustomization myapp -n flux-system

# Ver logs de reconciliación fallida
kubectl logs -n flux-system deploy/kustomize-controller -f \
  | grep myapp
```

Mejores prácticas de GitOps:

1. **Separar por ambientes**: desarrollo, staging, producción en ramas o directorios
2. **Pull request workflow**: cambios deben ser aprobados antes de merge
3. **Automatizar tests**: validar YAML antes de desplegar
4. **Semantic versioning**: usar tags de Git para versiones
5. **Segregación de responsabilidades**: diferentes repos para infra vs apps
6. **Secretos seguros**: usar SealedSecrets, External Secrets o similares

=== CI/CD Pipelines

**Jenkins con Kubernetes**

Jenkins ejecuta pipelines en Kubernetes con el plugin Kubernetes.

Instalación de Jenkins en K8s:

```bash
# Agregar repositorio Helm de Jenkins
helm repo add jenkins https://charts.jenkins.io
helm repo update

# Instalar Jenkins
helm install jenkins jenkins/jenkins \
  --namespace jenkins \
  --create-namespace \
  --set controller.serviceType=LoadBalancer

# Acceder a Jenkins
kubectl port-forward svc/jenkins -n jenkins 8080:8080

# Obtener contraseña inicial
kubectl exec -n jenkins jenkins-0 -- \
  cat /run/secrets/chart-admin-password
```

Configuración de Kubernetes Plugin:

```groovy
pipeline {
  agent {
    kubernetes {
      yaml '''
        apiVersion: v1
        kind: Pod
        metadata:
          labels:
            jenkins: agent
        spec:
          serviceAccountName: jenkins
          containers:
          - name: docker
            image: docker:latest
            command:
            - sleep
            args:
            - 99999
            volumeMounts:
            - name: docker-sock
              mountPath: /var/run/docker.sock
          volumes:
          - name: docker-sock
            hostPath:
              path: /var/run/docker.sock
      '''
    }
  }

  stages {
    stage('Build') {
      steps {
        container('docker') {
          sh '''
            docker build -t myapp:${BUILD_NUMBER} .
            docker push registry.example.com/myapp:${BUILD_NUMBER}
          '''
        }
      }
    }

    stage('Deploy') {
      steps {
        sh '''
          kubectl set image deployment/myapp \
            myapp=registry.example.com/myapp:${BUILD_NUMBER}
        '''
      }
    }
  }
}
```

Ventajas:
* Escalado automático: crea pods bajo demanda
* Aislamiento: cada job en su propio pod
* Recursos eficientes: pods se eliminan después de ejecutarse

**GitLab CI/CD**

GitLab ofrece CI/CD nativo que funciona bien con Kubernetes.

Definición en `.gitlab-ci.yml`:

```yaml
stages:
  - build
  - test
  - deploy

variables:
  DOCKER_DRIVER: overlay2
  DOCKER_REGISTRY: registry.gitlab.com
  IMAGE_NAME: $DOCKER_REGISTRY/$CI_PROJECT_PATH

build:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $DOCKER_REGISTRY
    - docker build -t $IMAGE_NAME:$CI_COMMIT_SHA .
    - docker tag $IMAGE_NAME:$CI_COMMIT_SHA $IMAGE_NAME:latest
    - docker push $IMAGE_NAME:$CI_COMMIT_SHA
    - docker push $IMAGE_NAME:latest

test:
  stage: test
  image: $IMAGE_NAME:$CI_COMMIT_SHA
  script:
    - npm test
    - npm run lint

deploy_staging:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context mygroup/myproject:k8s-agent
    - kubectl set image deployment/myapp myapp=$IMAGE_NAME:$CI_COMMIT_SHA -n staging
  environment:
    name: staging
  only:
    - develop

deploy_production:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context mygroup/myproject:k8s-agent
    - kubectl set image deployment/myapp myapp=$IMAGE_NAME:$CI_COMMIT_SHA -n production
  environment:
    name: production
  when: manual
  only:
    - main
```

GitLab Runner en Kubernetes:

```bash
# Instalar runner de GitLab
helm repo add gitlab https://charts.gitlab.io
helm install gitlab-runner gitlab/gitlab-runner \
  --set gitlabUrl=https://gitlab.example.com/ \
  --set gitlabRunnerRegistrationToken=<token> \
  --set runners.image=ubuntu:20.04 \
  --set runners.privileged=true
```

**GitHub Actions**

GitHub Actions ejecuta CI/CD directamente en GitHub.

Definición en `.github/workflows/build.yml`:

```yaml
name: Build and Deploy

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Log in to Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=semver,pattern={{version}}
            type=sha,prefix={{branch}}-

      - name: Build and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure kubectl
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > $HOME/.kube/config
          chmod 600 $HOME/.kube/config

      - name: Deploy to Kubernetes
        run: |
          kubectl set image deployment/myapp \
            myapp=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }} \
            -n production
          kubectl rollout status deployment/myapp -n production
```

Secrets en GitHub Actions:

```bash
# Agregar secret (vía UI o CLI)
gh secret set KUBE_CONFIG --body "$(cat ~/.kube/config | base64)"
gh secret set DOCKER_USERNAME --body "myusername"
gh secret set DOCKER_PASSWORD --body "mytoken"
```

**Tekton Pipelines**

Tekton es un framework agnóstico de CI/CD nativo de Kubernetes.

Instalación:

```bash
# Instalar Tekton
kubectl apply --filename \
  https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml

# Instalar Tekton Triggers
kubectl apply --filename \
  https://storage.googleapis.com/tekton-releases/triggers/latest/release.yaml

# Instalar Tekton Dashboard
kubectl apply --filename \
  https://storage.googleapis.com/tekton-releases/dashboard/latest/release-full.yaml
```

Definición de Task:

```yaml
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: build-docker-image
spec:
  params:
  - name: image-url
    type: string
  - name: image-tag
    type: string
  steps:
  - name: build
    image: docker:latest
    env:
    - name: DOCKER_HOST
      value: tcp://docker:2375
    script: |
      docker build -t $(params.image-url):$(params.image-tag) .
      docker push $(params.image-url):$(params.image-tag)
```

Definición de Pipeline:

```yaml
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: build-and-deploy
spec:
  params:
  - name: git-url
    type: string
  - name: image-url
    type: string

  tasks:
  - name: clone-repo
    taskRef:
      name: git-clone
    params:
    - name: url
      value: $(params.git-url)
    workspaces:
    - name: output
      workspace: source

  - name: build-image
    runAfter:
    - clone-repo
    taskRef:
      name: build-docker-image
    params:
    - name: image-url
      value: $(params.image-url)
    - name: image-tag
      value: latest
    workspaces:
    - name: source
      workspace: source

  - name: deploy
    runAfter:
    - build-image
    taskRef:
      name: deploy-kubernetes
    params:
    - name: image-url
      value: $(params.image-url)

  workspaces:
  - name: source
```

Ejecución de Pipeline:

```bash
# Crear PipelineRun
kubectl apply -f - <<EOF
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: build-and-deploy-run
spec:
  pipelineRef:
    name: build-and-deploy
  params:
  - name: git-url
    value: https://github.com/myorg/myapp
  - name: image-url
    value: registry.example.com/myapp
  workspaces:
  - name: source
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
EOF

# Ver estado de PipelineRun
tkn pipelinerun list
tkn pipelinerun describe build-and-deploy-run

# Logs de ejecución
tkn pipelinerun logs build-and-deploy-run -f
```

**Comparación de Soluciones**

|Característica|Jenkins|GitLab CI|GitHub Actions|Tekton|
|---|---|---|---|---|
|Instalación|Compleja|Integrada|SaaS|Nativa K8s|
|Escalado|Plugin Kubernetes|Runners|Hosts GitHub|Nativo|
|YAML config|Jenkinsfile|.gitlab-ci.yml|.github/workflows|Pipeline CR|
|Curva aprendizaje|Media|Baja|Baja|Alta|
|Costo|Gratuito|Freemium|Gratuito/Pagado|Gratuito|
|Mejor para|Entornos complejos|Desarrollo rápido|Proyectos GitHub|K8s puro|

=== Image Management

**Container Registries**

Los registros de contenedores almacenan y distribuyen imágenes Docker.

Registros públicos:
* **Docker Hub**: registry.hub.docker.com - registro público principal
* **GitHub Container Registry (GHCR)**: ghcr.io - integrado con GitHub
* **Google Container Registry (GCR)**: gcr.io - integrado con GCP
* **Amazon ECR**: public.ecr.aws - integrado con AWS

Registros privados:
* **Harbor**: registro self-hosted con scanning y replicación
* **Nexus**: gestor de artefactos con soporte Docker
* **Quay**: registro enterpise (RedHat)

Configuración de acceso privado en Kubernetes:

```bash
# Crear secret para autenticación
kubectl create secret docker-registry regcred \
  --docker-server=registry.example.com \
  --docker-username=myuser \
  --docker-password=mypass \
  --docker-email=myemail@example.com

# Listar secrets
kubectl get secrets

# Ver contenido del secret
kubectl get secret regcred --output="jsonpath={.data.\.dockerconfigjson}" | base64 -d
```

Uso en Deployment:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      imagePullSecrets:
      - name: regcred              # Referencia al secret de credenciales
      containers:
      - name: app
        image: registry.example.com/myapp:v1.0.0
        ports:
        - containerPort: 8080
```

**Image Pull Policies**

Las políticas de pull controlan cómo Kubernetes obtiene imágenes.

Políticas disponibles:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-pull-policy
spec:
  containers:
  - name: app
    image: myapp:latest
    imagePullPolicy: Always        # Siempre pull (default para :latest)

---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-never-pull
spec:
  containers:
  - name: app
    image: myapp:v1.0.0
    imagePullPolicy: Never         # Solo usar imagen local, fallar si no existe

---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-if-not-present
spec:
  containers:
  - name: app
    image: myapp:v1.0.0
    imagePullPolicy: IfNotPresent  # Pull solo si no existe localmente
```

Comportamiento por defecto:
* `latest`: Always (siempre pull)
* Versión específica (v1.0.0): IfNotPresent (pull solo si falta)
* Sin tag: IfNotPresent

Implicaciones de rendimiento:

```yaml
# BUENA PRÁCTICA: usar tags específicos
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  template:
    spec:
      containers:
      - image: myapp:v1.2.3        # Tag específico, IfNotPresent
        imagePullPolicy: IfNotPresent

---
# EVITAR: usar latest sin especificar policy
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-bad
spec:
  template:
    spec:
      containers:
      - image: myapp:latest         # Always pull, más lento
```

**Image Scanning**

El scanning detecta vulnerabilidades en imágenes.

Herramientas de scanning:
* **Trivy**: scanner rápido y completo (OSS)
* **Clair**: análisis de vulnerabilidades (OSS)
* **Anchore**: scanning empresarial
* **Snyk**: análisis de dependencias y vulnerabilidades

Instalación y uso de Trivy:

```bash
# Instalar Trivy
wget https://github.com/aquasecurity/trivy/releases/download/v0.45.0/trivy_0.45.0_Linux-64bit.tar.gz
tar zxvf trivy_0.45.0_Linux-64bit.tar.gz
sudo mv trivy /usr/local/bin/

# Escanear imagen local
trivy image myapp:v1.0.0

# Escanear desde registry remoto
trivy image registry.example.com/myapp:v1.0.0

# Generar reporte en JSON
trivy image --format json --output report.json myapp:v1.0.0

# Escanear filesystem
trivy fs /path/to/app

# Severidad mínima para salir con error
trivy image --severity HIGH,CRITICAL myapp:v1.0.0
```

Integración en CI/CD:

```yaml
# GitHub Actions
- name: Scan image with Trivy
  uses: aquasecurity/trivy-action@master
  with:
    image-ref: myapp:${{ github.sha }}
    format: 'sarif'
    output: 'trivy-results.sarif'

- name: Upload results to GitHub Security
  uses: github/codeql-action/upload-sarif@v2
  with:
    sarif_file: 'trivy-results.sarif'
```

```bash
# GitLab CI
scan_image:
  stage: scan
  image: aquasec/trivy:latest
  script:
    - trivy image --exit-code 0 --severity MEDIUM,HIGH,CRITICAL
      --format template --template '@/contrib/html.tpl'
      -o report.html $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  artifacts:
    reports:
      container_scanning: container-scanning-report.json
    paths:
      - report.html
```

**Vulnerability Management**

Gestión sistémica de vulnerabilidades detectadas.

Estrategias de remediación:

```yaml
# 1. Actualizar imagen base a versión sin vulnerabilidades
FROM ubuntu:22.04  # En lugar de ubuntu:20.04

# 2. Instalar parches de seguridad
RUN apt-get update && \
    apt-get install -y --only-upgrade openssl && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# 3. Usar imágenes minimalistas
FROM alpine:3.18  # En lugar de ubuntu

# 4. Eliminar paquetes no necesarios
RUN apk del apk-tools && \
    rm -rf /var/cache/apk/*
```

Políticas de admission para bloquear imágenes vulnerables:

```yaml
# Usando Kyverno para bloquear imágenes no escaneadas
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-image-scan
spec:
  validationFailureAction: enforce
  rules:
  - name: check-image-scan
    match:
      resources:
        kinds:
        - Pod
    validate:
      message: "Image must have been scanned"
      pattern:
        spec:
          containers:
          - image: "*//*"
            '(image)': "?*"
```

Monitoreo continuo con admission webhooks:

```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: image-vulnerability-check
webhooks:
- name: image-scan.example.com
  clientConfig:
    service:
      name: image-scan-service
      namespace: security
      path: "/validate"
    caBundle: LS0tLS1CRUdJ...
  rules:
  - operations: ["CREATE", "UPDATE"]
    apiGroups: [""]
    apiVersions: ["v1"]
    resources: ["pods"]
  admissionReviewVersions: ["v1"]
  sideEffects: None
```

Control de versiones y etiquetado:

```bash
# Etiquetado semántico
docker build -t myapp:v1.2.3 .
docker build -t myapp:latest .
docker build -t myapp:1.2 .
docker build -t myapp:1 .

# Etiquetado con información de build
docker build \
  -t myapp:v1.2.3-build.123 \
  -t myapp:v1.2.3-sha.abc1234 \
  .

# Push a múltiples registros
docker push registry1.example.com/myapp:v1.2.3
docker push registry2.example.com/myapp:v1.2.3
```

Best practices de gestión de imágenes:

1. **Usar tags específicos**: no depender de `latest` en producción
2. **Escanear regularmente**: no solo en CI, sino también imágenes en uso
3. **Segregar imágenes**: development != staging != production
4. **Mantener base images actualizadas**: patches de seguridad
5. **Documentar dependencias**: lista de paquetes y versiones
6. **Auditar acceso al registry**: quién push/pull qué
7. **Implementar políticas de retención**: limpiar imágenes viejas

Ejemplo de política de retención en Harbor:

```bash
# Via API
curl -X PUT https://harbor.example.com/api/v2.0/projects/1/retention/rules \
  -H "Content-Type: application/json" \
  -d '{
    "rules": [
      {
        "priority": 1,
        "template_id": "always",
        "tag_selectors": [{"kind": "label", "decoration": "matches", "pattern": "keep"}],
        "repo_selectors": [{"kind": "doublestar", "decoration": "matches", "pattern": "**"}],
        "retain": {
          "num": 10
        }
      }
    ],
    "algorithm": "or"
  }'
```

== Módulo 12: Service Mesh

=== Introducción a Service Mesh

**Problemas que resuelve**

En arquitecturas de microservicios complejas, la comunicación entre servicios presenta múltiples desafíos:

Problemas tradicionales sin Service Mesh:

1. **Resiliencia**:
   - Sin reintentos automáticos, timeouts o circuit breakers
   - Un servicio lento puede afectar toda la cadena
   - Falta de control sobre comportamiento de fallos

2. **Observabilidad**:
   - Difícil rastrear requests entre múltiples servicios
   - Métricas dispersas en diferentes herramientas
   - Correlación de errores complicada

3. **Seguridad**:
   - Comunicación sin encriptación entre servicios
   - Falta de autenticación/autorización entre servicios
   - Difícil auditoria de quién habla con quién

4. **Tráfico**:
   - Cambios de versiones requieren cambios de código
   - Difícil implementar canary deploys o A/B testing
   - Load balancing manual y complejo

5. **Gestión**:
   - Cada servicio implementa lógica de resiliencia
   - Duplicación de código entre microservicios
   - Difícil actualizar comportamiento globalmente

Ejemplo del problema:

```
Cliente → ServiceA → ServiceB → ServiceC
              ↓          ↓          ↓
           Retry?    Timeout?   Circuit?
          Logging?  Metrics?   Security?
```

Sin Service Mesh, cada servicio debe manejar:
- Reintentos con backoff exponencial
- Timeouts y circuit breakers
- Logging y tracing
- Encriptación TLS
- Autorización

**Service Mesh como solución**

Un Service Mesh es una capa de infraestructura dedicada que maneja la comunicación entre servicios (service-to-service communication).

Arquitectura típica:

```
┌─────────────────────────────────────────────────┐
│                  Application Layer              │
│  ServiceA    ServiceB    ServiceC    ServiceD   │
└────┬─────────────┬──────────────┬──────────┬───┘
     │             │              │          │
┌────▼─────────────▼──────────────▼──────────▼───┐
│           Service Mesh (Data Plane)            │
│  Envoy  Envoy  Envoy  Envoy  Envoy  Envoy     │
│ (sidecar proxies)                              │
└────────────────────────────────────────────────┘
     ▲                                        ▲
     └────────────────────────────────────────┘
            Control Plane (Istio/Linkerd)
```

Ventajas:

- **Transparencia**: Los servicios no necesitan cambiar código
- **Centralización**: Políticas definidas en un lugar
- **Consistencia**: Comportamiento uniforme entre todos los servicios
- **Dinamismo**: Cambios sin redeploy de aplicaciones
- **Observabilidad**: Métricas y trazas automáticas

**Arquitectura de Service Mesh**

Componentes principales:

1. **Data Plane**:
   - Proxies sidecar (uno por pod)
   - Interceptan todo el tráfico
   - Típicamente Envoy

2. **Control Plane**:
   - Configura los proxies
   - Mantiene estado global
   - Gestiona certificados

3. **APIs**:
   - Custom Resource Definitions (CRDs)
   - Definen comportamiento de tráfico
   - Políticas de seguridad

Flujo de una request:

```
1. Cliente env́a request → Envoy sidecar del cliente
2. Envoy cliente:
   - Aplica políticas de retry, timeout
   - Selecciona destino (load balancing)
   - Encripta con mTLS
3. Request llega → Envoy sidecar del servidor
4. Envoy servidor:
   - Valida certificado mTLS
   - Verifica authorization policy
   - Registra métricas
5. Request llega → Servidor real
```

Configuración básica en Kubernetes:

```yaml
# Inyectar sidecar automáticamente
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    istio-injection: enabled  # Inyectar Envoy sidecar

---
# Política de tráfico
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myservice
spec:
  hosts:
  - myservice
  http:
  - route:
    - destination:
        host: myservice
        port:
          number: 8080

---
# Destino con load balancing
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myservice
spec:
  host: myservice
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 2
  outlierDetection:
    consecutive5xxErrors: 5
    interval: 30s
    baseEjectionTime: 30s
```

**Comparación de soluciones Service Mesh**

| Aspecto | Istio | Linkerd | Consul | AWS App Mesh |
|--------|-------|---------|--------|-------------|
| **Proxy** | Envoy | Linkerd2-proxy | Envoy | Envoy |
| **Lenguaje CP** | Go/C++ | Rust | Go | Proprietary |
| **Curva Aprendizaje** | Alta | Media | Media | Baja |
| **Recursos** | Altos | Bajos | Medios | Managed |
| **Madurez** | Muy madura | Madura | Muy madura | En desarrollo |
| **Características** | Completas | Ligeras | Completas | Limitadas |
| **Multicluster** | Nativo | Nativo | Nativo | No (solo AWS) |
| **Costo** | Gratuito | Gratuito | Gratuito | Por hora |

**Linkerd - Alternativa ligera**

Instalación simple:

```bash
# Agregar repositorio
helm repo add linkerd https://helm.linkerd.io
helm repo update

# Instalar Linkerd
helm install linkerd2 linkerd/linkerd2 \
  --namespace linkerd \
  --create-namespace

# Inyectar automáticamente
kubectl annotate namespace default \
  linkerd.io/inject=enabled
```

**Consul Service Mesh**

Integración con HashiCorp Consul:

```bash
# Instalar Consul con Helm
helm install consul hashicorp/consul \
  --set connectInject.enabled=true \
  --set server.replicas=3
```

Definición de tráfico:

```hcl
Kind = "ServiceDefaults"
Name = "api"
Protocol = "http"

---

Kind = "ServiceRouter"
Name = "api"
Routes = [
  {
    Match = {
      HTTP = {
        PathPrefix = "/v2"
      }
    }
    Destination = {
      Service = "api"
      ServiceSubset = "v2"
    }
  }
]
```

**AWS App Mesh**

Integración con AWS:

```bash
# Crear mesh
aws appmesh create-mesh --mesh-name production

# Crear virtual node
aws appmesh create-virtual-node \
  --mesh-name production \
  --virtual-node-name api
```

=== Istio

Istio es el Service Mesh más popular y completo para Kubernetes.

**Componentes de Istio**

Instalación:

```bash
# Descargar Istio
curl -L https://istio.io/downloadIstio | sh -
cd istio-1.17.0

# Instalar Istio
./bin/istioctl install --set profile=demo -y

# Verificar instalación
kubectl get namespaces --show-labels | grep istio
kubectl get pods -n istio-system
```

Componentes del Control Plane:

```yaml
# istiod: Componente central
# - Detecta cambios en servicios
# - Genera configuración para Envoy
# - Maneja certificados mTLS

# Instalado en namespace istio-system
# Pods: istiod-xxxxx

# Ver estado
kubectl get pods -n istio-system
kubectl logs -n istio-system deploy/istiod | tail -20
```

Componentes del Data Plane:

```yaml
# Envoy sidecars:
# - Inyectados automáticamente en cada pod
# - Interceptan tráfico (inbound/outbound)
# - Aplican políticas de tráfico

# Ver sidecars inyectados
kubectl get pods -o jsonpath='{.items[].spec.containers[*].name}'

# Conectarse al sidecar
kubectl exec -it <pod> -c istio-proxy -- bash
```

**Virtual Services**

VirtualService define cómo encaminar el tráfico a destinos reales.

Ejemplo básico:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
  - reviews              # Nombre del servicio
  http:
  - route:
    - destination:
        host: reviews
        port:
          number: 9080
        subset: v1       # Versión específica
```

Routing avanzado:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp
spec:
  hosts:
  - myapp.example.com

  http:
  # Ruta 1: Traffic de usuarios autenticados a v2
  - match:
    - headers:
        user-type:
          exact: premium
    route:
    - destination:
        host: myapp
        subset: v2
      weight: 100
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s

  # Ruta 2: Traffic de API clients a v3
  - match:
    - headers:
        user-agent:
          prefix: "API-Client"
    route:
    - destination:
        host: myapp
        subset: v3

  # Ruta 3: Path-based routing
  - match:
    - uri:
        prefix: "/api/v1"
    route:
    - destination:
        host: myapp
        subset: v1

  # Ruta 4: Default fallback
  - route:
    - destination:
        host: myapp
        subset: v1
      weight: 80
    - destination:
        host: myapp
        subset: v2
      weight: 20
```

Gestión de conexiones:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: bookings
spec:
  hosts:
  - bookings
  http:
  - route:
    - destination:
        host: bookings
        port:
          number: 8080
    # Configuración de conexión
    timeout: 30s           # Timeout total
    retries:
      attempts: 5          # Reintentos
      perTryTimeout: 10s   # Timeout por intento
```

**Destination Rules**

DestinationRule define cómo se conecta a un host específico (load balancing, circuitbreaker, etc.).

Ejemplo básico:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp
spec:
  host: myapp              # Debe coincidir con VirtualService
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        http2MaxRequests: 1000
        maxRequestsPerConnection: 2
    outlierDetection:
      consecutive5xxErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
      minRequestVolume: 5
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

Load Balancing con round-robin:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp-lb
spec:
  host: myapp
  trafficPolicy:
    loadBalancer:
      simple: ROUND_ROBIN   # Opciones: ROUND_ROBIN, LEAST_REQUEST, RANDOM, PASSTHROUGH

---
# LEAST_REQUEST: Envía tráfico al endpoint con menos solicitudes
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp-least
spec:
  host: myapp
  trafficPolicy:
    loadBalancer:
      consistentHash:
        httpHeaderName: "user-id"    # Hash basado en header
```

Circuit Breaker:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: failover-example
spec:
  host: db
  trafficPolicy:
    outlierDetection:
      consecutive5xxErrors: 5        # Expulsar después de 5 errores
      interval: 30s                  # Revisar cada 30s
      baseEjectionTime: 30s          # Mantener fuera 30s
      maxEjectionPercent: 100        # Sacar todos si es necesario
      minRequestVolume: 5            # Mínimo requests antes de expulsar
```

**Gateways**

Gateway configura un load balancer para tráfico de entrada (Ingress).

Ejemplo básico:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: main-gateway
spec:
  selector:
    istio: ingressgateway    # Selector del Ingress Controller
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "example.com"
    - "*.example.com"

---
# Vincular Gateway con VirtualService
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp-routes
spec:
  hosts:
  - example.com
  gateways:
  - main-gateway           # Usar este gateway
  http:
  - match:
    - uri:
        prefix: "/api"
    route:
    - destination:
        host: api
        port:
          number: 8080
  - route:
    - destination:
        host: website
        port:
          number: 80
```

HTTPS/TLS:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: secure-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: mycert-secret  # Secret con cert y key
    hosts:
    - "example.com"
  # Redirigir HTTP a HTTPS
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "example.com"
```

**Traffic Management**

Control granular del tráfico entre servicios.

Canary Deployment:

```yaml
# Desplegar dos versiones
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: v1
  template:
    metadata:
      labels:
        app: myapp
        version: v1
    spec:
      containers:
      - name: myapp
        image: myapp:v1.0.0

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-v2
spec:
  replicas: 1              # Solo 1 pod de canary
  selector:
    matchLabels:
      app: myapp
      version: v2
  template:
    metadata:
      labels:
        app: myapp
        version: v2
    spec:
      containers:
      - name: myapp
        image: myapp:v2.0.0

---
# DestinationRule con subsets
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp
spec:
  host: myapp
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2

---
# VirtualService: 95% a v1, 5% a v2 (canary)
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp
spec:
  hosts:
  - myapp
  http:
  - route:
    - destination:
        host: myapp
        subset: v1
      weight: 95
    - destination:
        host: myapp
        subset: v2
      weight: 5
```

Mirror Traffic (shadow):

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp-shadow
spec:
  hosts:
  - myapp
  http:
  - route:
    - destination:
        host: myapp
        subset: v1
      weight: 100
    mirror:
      host: myapp
      subset: v2            # Copiar tráfico a v2 sin afectar respuesta
    mirrorPercent: 100      # Mirror 100% del tráfico
```

Rate Limiting con VirtualService:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: api-limiter
spec:
  hosts:
  - api.example.com
  http:
  - route:
    - destination:
        host: api
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s
```

Monitoreo de tráfico:

```bash
# Ver configuración aplicada
kubectl get virtualservice
kubectl get destinationrule
kubectl get gateway

# Validar configuración
istioctl analyze

# Ver estado detallado
kubectl describe vs myapp
kubectl describe dr myapp

# Logs de Envoy
kubectl logs <pod> -c istio-proxy | tail -50

# Ver estadísticas de Envoy
kubectl exec -it <pod> -c istio-proxy -- \
  curl localhost:15000/stats/prometheus | grep myapp
```

=== Observabilidad con Service Mesh

La observabilidad es una ventaja clave de Service Mesh, proporcionando visibilidad automática.

**Distributed Tracing**

El distributed tracing rastrea requests a través de múltiples servicios.

Instalación de Jaeger en Istio:

```bash
# Instalar Jaeger con el profile
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.17/samples/addons/jaeger.yaml

# Verificar instalación
kubectl get pods -n istio-system | grep jaeger

# Acceder a Jaeger
kubectl port-forward svc/jaeger -n istio-system 16686:16686
# Navegar a http://localhost:16686
```

Configuración de tracing en Istio:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: istio
  namespace: istio-system
data:
  meshConfig: |
    enableTracing: true
    defaultConfig:
      tracing:
        sampling: 100.0         # % de requests a trackear
        zipkin:
          address: zipkin:9411  # Endpoint de Zipkin/Jaeger
```

Propagación de contexto en aplicaciones:

```bash
# Headers de propagación necesarios para tracing:
# jaeger:
#   - uber-trace-id
# zipkin:
#   - x-b3-trace-id
#   - x-b3-span-id
#   - x-b3-parent-span-id
#   - x-b3-sampled

# Ejemplo: curl con headers de tracing
curl -H "x-b3-trace-id: 80f198ee56343ba8" \
     -H "x-b3-span-id: e457b5a2e3d6a0e6" \
     -H "x-b3-sampled: 1" \
     http://myapp:8080/api
```

Instrumentación de aplicaciones:

```python
# Ejemplo en Python con Jaeger client
from jaeger_client import Config

def init_tracer(service_name):
    config = Config(
        config={
            'sampler': {
                'type': 'const',
                'param': 1,
            },
            'logging': True,
            'local_agent': {
                'reporting_host': 'localhost',
                'reporting_port': 6831,
            }
        },
        service_name=service_name,
        validate=True,
    )
    return config.initialize_tracer()

tracer = init_tracer('myservice')

with tracer.start_span('my-operation') as span:
    span.set_tag('http.method', 'GET')
    span.set_tag('http.url', '/api/users')
    # operación
```

Ejemplo en Node.js:

```javascript
const initTracer = require('jaeger-client').initTracer;

const config = {
  serviceName: 'myservice',
  sampler: {
    type: 'const',
    param: 1,
  },
  reporter_loggers: true,
};

const tracer = initTracer(config);

const span = tracer.startSpan('my-operation');
span.setTag('http.method', 'GET');
span.setTag('http.url', '/api/users');
// operación
span.finish();
```

**Kiali**

Kiali es un panel visual para Service Mesh que muestra la topología de servicios.

Instalación:

```bash
# Instalar Kiali
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.17/samples/addons/kiali.yaml

# Verificar instalación
kubectl get pods -n istio-system | grep kiali

# Acceder a Kiali
kubectl port-forward svc/kiali -n istio-system 20000:20000
# Navegar a http://localhost:20000
```

Configuración de Kiali:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kiali
  namespace: istio-system
data:
  kiali.yaml: |
    auth:
      strategy: anonymous          # Sin autenticación (cambiar en producción)
    external_services:
      prometheus:
        url: http://prometheus:9090
      tracing:
        enabled: true
        url: http://jaeger:16686
    service_type: ClusterIP
    web_root: /kiali
```

Features de Kiali:

- Visualización de topología de servicios
- Métricas en tiempo real (latencia, tasa de errores)
- Distributed tracing integrado
- Validación de configuración
- Health checks
- Traffic direction y tasa de error

Acceso programático a Kiali:

```bash
# API de Kiali
kubectl port-forward svc/kiali -n istio-system 20000:20000

# Obtener topología de servicios
curl http://localhost:20000/api/namespaces/default/services

# Obtener métricas de un servicio
curl http://localhost:20000/api/namespaces/default/services/myapp/metrics

# Obtener tráfico entre servicios
curl http://localhost:20000/api/namespaces/default/services/myapp/traffic
```

**Jaeger**

Jaeger es una plataforma completa para distributed tracing.

Componentes:

```
┌────────────┐
│  Cliente   │ → Envía spans
└────────────┘

      ↓

┌────────────────────────┐
│  Jaeger Agent          │ (localhost:6831 UDP)
│  Recibe spans locales  │
└────────────────────────┘

      ↓

┌────────────────────────┐
│  Jaeger Collector      │
│  Recibe spans remotos  │
└────────────────────────┘

      ↓

┌────────────────────────┐
│  Jaeger Query          │
│  UI y API              │
└────────────────────────┘
```

Instalación completa:

```bash
# Usando Jaeger Operator
helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
helm repo update

# Instalar Jaeger
helm install jaeger jaegertracing/jaeger \
  --namespace observability \
  --create-namespace

# Verificar
kubectl get pods -n observability
```

Configuración de muestreo:

```yaml
# Muestreo adaptativo: sample 10% de requests
apiVersion: v1
kind: ConfigMap
metadata:
  name: jaeger-configuration
data:
  sampling.json: |
    {
      "default_strategy": {
        "type": "probabilistic",
        "param": 0.1
      }
    }
```

Consultas en Jaeger:

```bash
# Encontrar traces por servicio
# En UI: Service dropdown → Select myapp

# Filtrar por tags
# En UI: Add tag filter: error=true

# Buscar trazas lentas
# En UI: Min Duration: 1000ms

# API
curl 'http://localhost:16686/api/traces?service=myapp&limit=10'
```

**Métricas de Servicios**

Istio proporciona automáticamente métricas de Prometheus.

Instalación de Prometheus:

```bash
# Instalar Prometheus
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.17/samples/addons/prometheus.yaml

# Verificar
kubectl get pods -n istio-system | grep prometheus

# Acceder
kubectl port-forward svc/prometheus -n istio-system 9090:9090
# Navegar a http://localhost:9090
```

Métricas disponibles:

```promql
# Tasa de requests por segundo
rate(istio_requests_total[1m])

# Tasa de errores (5xx)
rate(istio_requests_total{response_code=~"5.."}[1m])

# Latencia P95
histogram_quantile(0.95, rate(istio_request_duration_milliseconds_bucket[1m]))

# Tráfico por destino
sum(rate(istio_requests_total[1m])) by (destination_service_name)

# Errores por par de servicios
sum(rate(istio_requests_total{response_code=~"5.."}[1m])) by (source_app, destination_app)
```

Instalación de Grafana:

```bash
# Instalar Grafana
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.17/samples/addons/grafana.yaml

# Acceder
kubectl port-forward svc/grafana -n istio-system 3000:3000
# Navegar a http://localhost:3000 (admin/admin)
```

Dashboards pre-configurados:

- **Mesh Dashboard**: Visión general del mesh
- **Service Dashboard**: Métricas por servicio
- **Workload Dashboard**: Métricas por workload
- **Performance Dashboard**: Performance y latencia

Alertas con Prometheus:

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: istio-alerts
spec:
  groups:
  - name: istio
    rules:
    # Alerta: Error rate > 5%
    - alert: HighErrorRate
      expr: |
        rate(istio_requests_total{response_code=~"5.."}[5m]) /
        rate(istio_requests_total[5m]) > 0.05
      for: 5m
      annotations:
        summary: "High error rate for {{ $labels.destination_service }}"

    # Alerta: Latencia P95 > 1s
    - alert: HighLatency
      expr: |
        histogram_quantile(0.95, rate(istio_request_duration_milliseconds_bucket[5m])) > 1000
      for: 5m
      annotations:
        summary: "High latency for {{ $labels.destination_service }}"
```

Integración con herramientas externas:

```bash
# Exportar métricas a Stackdriver (GCP)
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.17/samples/addons/stackdriver.yaml

# Exportar a Datadog
# Requiere Datadog agent en cluster

# Exportar a New Relic
# Requiere New Relic Kubernetes integration
```

=== Seguridad en Service Mesh

Service Mesh proporciona seguridad automática para comunicación entre servicios.

**mTLS Automático**

mTLS (Mutual TLS) encripta y autentica la comunicación entre servicios automáticamente.

Configuración de mTLS en Istio:

```yaml
# Habilitar mTLS para todo el namespace
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: production
spec:
  mtls:
    mode: STRICT              # STRICT, PERMISSIVE, DISABLE, UNSET
```

Modos de mTLS:

- **STRICT**: Solo tráfico mTLS permitido (recomendado)
- **PERMISSIVE**: Permite mTLS y plaintext (para migración)
- **DISABLE**: Sin mTLS (solo para debugging)
- **UNSET**: Hereda configuración del namespace

Ejemplo: Migración gradual a mTLS

```yaml
# Fase 1: PERMISSIVE (permite ambos)
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
spec:
  mtls:
    mode: PERMISSIVE

---
# Fase 2: STRICT (solo mTLS)
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
spec:
  mtls:
    mode: STRICT
```

Configuración por workload:

```yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: api-mtls
spec:
  selector:
    matchLabels:
      app: api
  mtls:
    mode: STRICT

---
# Pero permitir plaintext para health checks
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: allow-plaintext-health
spec:
  selector:
    matchLabels:
      app: api
  portLevelMtls:
    8080:              # Puerto de health checks
      mode: DISABLE
    8081:              # Puerto de aplicación
      mode: STRICT
```

Validación de certificados:

```bash
# Verificar certificados mTLS instalados
kubectl exec <pod> -c istio-proxy -- \
  ls -la /etc/certs/workload-cert-chain.pem

# Ver detalles del certificado
kubectl exec <pod> -c istio-proxy -- \
  openssl x509 -in /etc/certs/workload-cert-chain.pem -text -noout

# Probar conexión mTLS
kubectl exec <client-pod> -- \
  curl --cacert /etc/certs/root-cert.pem \
       --cert /etc/certs/cert-chain.pem \
       --key /etc/certs/key.pem \
       https://server:8080/api
```

**Authorization Policies**

Control de acceso basado en atributos (ABAC) entre servicios.

Política básica (permitir todo):

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-all
  namespace: default
spec:
  rules:
  - {}  # Regla vacía = permitir todo
```

Denegar todo, permitir excepciones:

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: default-deny
spec:
  {} # Nada aquí = denegar por defecto

---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-frontend
spec:
  selector:
    matchLabels:
      app: backend
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/default/sa/frontend"]
    to:
    - operation:
        methods: ["GET"]
        paths: ["/api/v1/*"]
```

Control granular por métodos y paths:

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: api-policy
spec:
  selector:
    matchLabels:
      app: api
  rules:
  # Regla 1: GET /products - público
  - to:
    - operation:
        methods: ["GET"]
        paths: ["/products"]

  # Regla 2: POST /orders - requiere usuario autenticado
  - from:
    - source:
        requestPrincipals: ["*"]
    to:
    - operation:
        methods: ["POST"]
        paths: ["/orders"]

  # Regla 3: DELETE - solo admin
  - from:
    - source:
        principals: ["cluster.local/ns/default/sa/admin"]
    to:
    - operation:
        methods: ["DELETE"]
        paths: ["/admin/*"]

  # Regla 4: Entre servicios
  - from:
    - source:
        principals:
        - "cluster.local/ns/default/sa/frontend"
        - "cluster.local/ns/default/sa/mobile-app"
    to:
    - operation:
        ports: ["8080"]
```

Denegación explícita:

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: deny-legacy-app
spec:
  selector:
    matchLabels:
      app: sensitive-data
  rules:
  - {}  # Permitir por defecto

---
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: deny-specific
spec:
  selector:
    matchLabels:
      app: sensitive-data
  action: DENY            # DENY en lugar de ALLOW
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/default/sa/legacy-app"]
    to:
    - operation:
        paths: ["/sensitive/*"]
```

**JWT Validation**

Validar tokens JWT para autorización.

Configuración básica:

```yaml
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-auth
spec:
  jwtRules:
  - issuer: "https://auth.example.com"
    jwksUri: "https://auth.example.com/.well-known/jwks.json"
    audiences: ["api.example.com"]
```

Validación con cabecera personalizada:

```yaml
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-custom-header
spec:
  jwtRules:
  - issuer: "https://auth.example.com"
    jwksUri: "https://auth.example.com/.well-known/jwks.json"
    outputPayloadToHeader: "x-jwt-payload"  # Enviar JWT decodificado aquí
```

Múltiples issuers:

```yaml
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-multi-issuer
spec:
  jwtRules:
  - issuer: "https://auth.example.com"
    jwksUri: "https://auth.example.com/.well-known/jwks.json"
  - issuer: "https://oauth.github.com"
    jwksUri: "https://github.com/login/oauth/access_token"
```

Combinación con AuthorizationPolicy:

```yaml
# 1. Autenticar JWT
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-auth
spec:
  selector:
    matchLabels:
      app: api
  jwtRules:
  - issuer: "https://auth.example.com"
    jwksUri: "https://auth.example.com/.well-known/jwks.json"

---
# 2. Autorizar basado en claims JWT
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: jwt-authz
spec:
  selector:
    matchLabels:
      app: api
  rules:
  # Admin: cualquier método
  - from:
    - source:
        requestPrincipals: ["https://auth.example.com/admin"]
    to:
    - operation:
        paths: ["/admin/*"]

  # Usuario normal: solo GET
  - from:
    - source:
        requestPrincipals: ["https://auth.example.com/user"]
    to:
    - operation:
        methods: ["GET"]

  # Custom claim: premium users
  - from:
    - source:
        requestPrincipals: ["*"]
    when:
    - key: request.auth.claims[tier]
      values: ["premium"]
    to:
    - operation:
        paths: ["/api/premium/*"]
```

Debugging de seguridad:

```bash
# Ver policies aplicadas
kubectl get authorizationpolicies -A

# Validar policy
istioctl analyze -A

# Ver logs de denegación
kubectl logs <pod> -c istio-proxy | grep DENIED

# Monitoreo de tráfico denegado
kubectl exec -it <pod> -c istio-proxy -- \
  curl localhost:15000/stats/prometheus | grep denied
```

Best practices de seguridad:

1. **Habilitar mTLS STRICT**: Encripción automática de tráfico
2. **Default deny, allow exceptions**: Principio de mínimos privilegios
3. **Usar service accounts**: Identificar servicios automáticamente
4. **Validar JWT**: Tokens deben ser validados
5. **Registrar accesos denegados**: Auditoría de intentos fallidos
6. **Certificados rotados**: Istio lo hace automáticamente
7. **Segregación por namespace**: Aislar por entorno/equipo

Ejemplo completo de seguridad:

```yaml
# 1. Habilitar mTLS STRICT
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
spec:
  mtls:
    mode: STRICT

---
# 2. Denegar todo por defecto
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: default-deny
spec:
  {}

---
# 3. Permitir frontend → backend
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-frontend-to-backend
spec:
  selector:
    matchLabels:
      app: backend
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/default/sa/frontend"]

---
# 4. Autenticar JWT para API
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: api-jwt
spec:
  selector:
    matchLabels:
      app: api
  jwtRules:
  - issuer: "https://auth.example.com"
    jwksUri: "https://auth.example.com/.well-known/jwks.json"

---
# 5. Autorizar acceso a API
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: api-authz
spec:
  selector:
    matchLabels:
      app: api
  rules:
  - from:
    - source:
        requestPrincipals: ["*"]
    to:
    - operation:
        methods: ["GET"]
```

== Módulo 13: Operaciones Avanzadas

=== Cluster Management

**Actualizaciones de Cluster**

Actualizar Kubernetes sin downtime requiere planificación cuidadosa.

Estrategia de actualización:

```
1. Planificación
   - Revisar notas de cambio (release notes)
   - Verificar compatibilidad de aplicaciones
   - Backup previo

2. Actualización del Control Plane
   - Actualizar API Server
   - Actualizar Controller Manager
   - Actualizar Scheduler
   - Actualizar etcd

3. Actualización de Nodos
   - Cordon (marcar como no schedulable)
   - Drain (evacuar pods)
   - Actualizar kubelet
   - Uncordon (volver a schedulable)

4. Validación
   - Verificar salud del cluster
   - Revisar logs
   - Pruebas de funcionalidad
```

Actualización con kubeadm:

```bash
# 1. Verificar qué versión está disponible
kubeadm upgrade plan

# 2. Actualizar kubeadm en el nodo control plane
apt-get update && apt-get install -y kubeadm=1.27.0-00

# 3. Planificar actualización
kubeadm upgrade plan v1.27.0

# 4. Aplicar actualización
kubeadm upgrade apply v1.27.0

# 5. Actualizar kubelet en control plane
apt-get install -y kubelet=1.27.0-00 kubectl=1.27.0-00
systemctl daemon-reload
systemctl restart kubelet

# 6. Actualizar nodos worker
for node in worker1 worker2 worker3; do
  kubectl cordon $node
  kubectl drain $node --ignore-daemonsets --delete-emptydir-data
  # SSH al nodo y ejecutar:
  # apt-get install -y kubelet=1.27.0-00
  # systemctl restart kubelet
  kubectl uncordon $node
done
```

Actualización en cluster managed (EKS/GKE/AKS):

```bash
# AWS EKS
aws eks update-cluster-version \
  --name my-cluster \
  --kubernetes-version 1.27

# Google GKE
gcloud container clusters upgrade my-cluster \
  --master-pool-name default-pool \
  --cluster-version 1.27

# Azure AKS
az aks upgrade \
  --resource-group myResourceGroup \
  --name myAKSCluster \
  --kubernetes-version 1.27
```

Verificación post-actualización:

```bash
# Verificar versión
kubectl version

# Verificar componentes
kubectl get nodes
kubectl get cs
kubectl get componentstatus

# Verificar pods del sistema
kubectl get pods -n kube-system

# Revisar logs
journalctl -u kubelet -n 50
```

**Backup y Restore**

Proteger datos críticos del cluster.

Herramientas de backup:

1. **Velero**: Solución completa de backup
2. **etcd backup**: Backup manual del almacenamiento
3. **Application-level**: Backups dentro de la aplicación

Backup de etcd:

```bash
# Snapshot de etcd
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /backup/etcd-snapshot.db

# Verificar snapshot
ETCDCTL_API=3 etcdctl \
  snapshot status /backup/etcd-snapshot.db

# Restaurar snapshot
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot restore /backup/etcd-snapshot.db \
  --data-dir=/var/lib/etcd-backup
```

Instalación de Velero:

```bash
# Descargar Velero
wget https://github.com/vmware-tanzu/velero/releases/download/v1.11.0/velero-v1.11.0-linux-amd64.tar.gz
tar -xzf velero-v1.11.0-linux-amd64.tar.gz
sudo mv velero-v1.11.0-linux-amd64/velero /usr/local/bin/

# Instalar Velero en cluster (con AWS S3)
velero install \
  --provider aws \
  --bucket velero-backups \
  --secret-file ./credentials-velero \
  --plugins velero/velero-plugin-for-aws:v1.7.0
```

Crear backups con Velero:

```bash
# Backup completo
velero backup create full-backup

# Backup de namespace específico
velero backup create prod-backup \
  --include-namespaces production

# Backup con schedule
velero schedule create daily-backup \
  --schedule="0 2 * * *" \
  --include-namespaces production

# Ver backups
velero backup get
velero backup describe full-backup
velero backup logs full-backup
```

Restaurar desde backup:

```bash
# Restaurar backup completo
velero restore create \
  --from-backup full-backup

# Restaurar namespace específico
velero restore create \
  --from-backup prod-backup \
  --include-namespaces production

# Ver restauraciones
velero restore get
velero restore logs full-backup-20231115123456
```

**Disaster Recovery**

Plan de recuperación ante fallos.

RTO (Recovery Time Objective) y RPO (Recovery Point Objective):

```
RTO: Tiempo máximo para recuperación (e.g., 4 horas)
RPO: Máximo tiempo de pérdida de datos (e.g., 1 hora)

Estrategia:
- RTO bajo → Replicación activa, multi-región
- RPO bajo → Backups frecuentes
- Ambos bajos → Caro, requiere múltiples regiones
```

DR con múltiples clusters:

```yaml
# Cluster primario - namespace
apiVersion: v1
kind: Namespace
metadata:
  name: production

---
# Backup automático en cluster secundario
apiVersion: velero.io/v1
kind: Backup
metadata:
  name: auto-backup
spec:
  schedule: "0 */6 * * *"  # Cada 6 horas
  includedNamespaces:
  - production
  storageLocation: aws-s3
```

Testing de DR:

```bash
# 1. Crear backup de cluster primario
velero backup create pre-disaster-backup

# 2. Simular fallo - restaurar en cluster secundario
# Apuntar kubeconfig al cluster secundario
kubectl config use-context secondary-cluster

# 3. Restaurar aplicación
velero restore create \
  --from-backup pre-disaster-backup

# 4. Validar que funciona
kubectl get all -n production
# Pruebas de funcionalidad

# 5. Replicar tráfico a secundario
# Cambiar DNS/Load balancer
```

**Multi-cluster Management**

Gestionar múltiples clusters Kubernetes.

Herramientas:

1. **Helm**: Desplegar aplicaciones en múltiples clusters
2. **Flux/ArgoCD**: GitOps en múltiples clusters
3. **Karmada**: Control plane multi-cluster nativo
4. **Rancher**: Platform multi-cluster

Deplyment multi-cluster con Helm:

```bash
# Agregar contextos
kubectl config use-context cluster1
helm install myapp ./chart --namespace production

kubectl config use-context cluster2
helm install myapp ./chart --namespace production

kubectl config use-context cluster3
helm install myapp ./chart --namespace production
```

GitOps multi-cluster con ArgoCD:

```yaml
# Cluster 1 (primario)
apiVersion: v1
kind: Namespace
metadata:
  name: argocd

---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: myapp-cluster1
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/myorg/config
    targetRevision: main
    path: clusters/cluster1
  destination:
    server: https://kubernetes.default.svc
    namespace: production

---
# Cluster 2 (secundario)
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: myapp-cluster2
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/myorg/config
    targetRevision: main
    path: clusters/cluster2
  destination:
    server: https://cluster2-api.example.com:6443
    namespace: production
```

Karmada - Control plane unificado:

```bash
# Instalar Karmada
git clone https://github.com/karmada-io/karmada.git
cd karmada
hack/local-up-karmada.sh

# Registrar clusters
karmadactl join cluster1 --cluster-kubeconfig=/path/to/cluster1.conf
karmadactl join cluster2 --cluster-kubeconfig=/path/to/cluster2.conf

# Crear PropagationPolicy (distribuir a múltiples clusters)
cat <<EOF | kubectl apply -f -
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: multi-cluster-deployment
spec:
  resourceSelectors:
  - apiVersion: apps/v1
    kind: Deployment
    name: myapp
  placement:
    clusterAffinity:
      clusterNames:
      - cluster1
      - cluster2
      - cluster3
EOF
```

Monitoreo multi-cluster:

```bash
# Prometheus federado - scrapar de múltiples clusters
global:
  scrape_interval: 15s

scrape_configs:
- job_name: 'cluster1'
  kubernetes_sd_configs:
  - api_server: https://cluster1-api.example.com:6443
    tls_config:
      ca_file: /etc/prometheus/cluster1-ca.crt

- job_name: 'cluster2'
  kubernetes_sd_configs:
  - api_server: https://cluster2-api.example.com:6443
    tls_config:
      ca_file: /etc/prometheus/cluster2-ca.crt
```

=== Custom Resources (CRD)

Los Custom Resource Definitions (CRDs) permiten extender la API de Kubernetes con nuevos tipos de recursos.

**Definición de CRDs**

Crear un tipo de recurso personalizado:

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: websites.mycompany.io      # nombre debe ser <plural>.<group>
spec:
  group: mycompany.io              # API group
  scope: Namespaced                # Namespaced o Cluster
  names:
    plural: websites
    singular: website
    kind: Website                  # PascalCase
    shortNames:
    - ws                          # kubectl get ws
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            required:
            - domain
            - owner
            properties:
              domain:
                type: string
                description: "Domain name of website"
                pattern: '^[a-z0-9\-]+\.[a-z]+$'
              owner:
                type: string
              replicas:
                type: integer
                minimum: 1
                maximum: 10
              ssl:
                type: boolean
                default: true
```

Usar el CRD:

```bash
# Crear instancia del CRD
cat <<EOF | kubectl apply -f -
apiVersion: mycompany.io/v1
kind: Website
metadata:
  name: my-blog
spec:
  domain: myblog.com
  owner: john@example.com
  replicas: 3
  ssl: true
EOF

# Listar recursos personalizados
kubectl get websites
kubectl get ws

# Ver detalles
kubectl describe website my-blog

# Editar
kubectl edit website my-blog

# Eliminar
kubectl delete website my-blog
```

Validación de CRDs:

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: databases.example.io
spec:
  group: example.io
  scope: Namespaced
  names:
    plural: databases
    kind: Database
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            required:
            - engine
            - size
            properties:
              engine:
                type: string
                enum:
                - postgresql
                - mysql
                - mongodb
              size:
                type: string
                pattern: '^[0-9]+Gi$'
              backup:
                type: object
                properties:
                  enabled:
                    type: boolean
                  frequency:
                    type: string
                    enum:
                    - hourly
                    - daily
                    - weekly
```

**Custom Controllers**

Un controller observa recursos y realiza acciones.

Ejemplo simple de controller en Python:

```python
#!/usr/bin/env python3
from kubernetes import client, config, watch
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cargar configuración del cluster
config.load_incluster_config()

# Crear cliente de Kubernetes
v1 = client.CustomObjectsApi()

# Definir CRD
GROUP = "mycompany.io"
VERSION = "v1"
NAMESPACE = "default"
PLURAL = "websites"

def create_deployment_for_website(website):
    """Crear deployment cuando se crea un Website"""
    name = website['metadata']['name']
    domain = website['spec']['domain']
    replicas = website['spec'].get('replicas', 1)

    deployment = {
        'apiVersion': 'apps/v1',
        'kind': 'Deployment',
        'metadata': {
            'name': f'{name}-deployment',
            'namespace': NAMESPACE,
            'ownerReferences': [{
                'apiVersion': 'mycompany.io/v1',
                'kind': 'Website',
                'name': name,
                'uid': website['metadata']['uid']
            }]
        },
        'spec': {
            'replicas': replicas,
            'selector': {'matchLabels': {'app': name}},
            'template': {
                'metadata': {'labels': {'app': name}},
                'spec': {
                    'containers': [{
                        'name': 'nginx',
                        'image': 'nginx:latest',
                        'env': [{
                            'name': 'DOMAIN',
                            'value': domain
                        }]
                    }]
                }
            }
        }
    }

    return deployment

def watch_websites():
    """Observar cambios en Website resources"""
    w = watch.Watch()

    for event in w.stream(
        v1.list_namespaced_custom_object,
        GROUP, VERSION, NAMESPACE, PLURAL,
        _preload_content=False
    ):
        website = event['object']
        event_type = event['type']
        name = website['metadata']['name']

        logger.info(f"Event: {event_type} on {name}")

        if event_type == 'ADDED':
            # Crear deployment
            deployment = create_deployment_for_website(website)
            logger.info(f"Creating deployment for {name}")
            # Enviar deployment a API

        elif event_type == 'MODIFIED':
            logger.info(f"Updated {name}")

        elif event_type == 'DELETED':
            logger.info(f"Deleted {name}")

if __name__ == '__main__':
    watch_websites()
```

**Operators**

Un Operator combina CRDs + Controllers para automatizar tareas complejas.

Estructura de un Operator:

```
my-operator/
├── config/
│   ├── crd/
│   │   └── database_crd.yaml
│   ├── rbac/
│   │   ├── role.yaml
│   │   ├── rolebinding.yaml
│   │   └── serviceaccount.yaml
│   └── manager/
│       └── manager.yaml
├── api/
│   └── v1/
│       ├── database_types.go
│       └── groupversion_info.go
├── controllers/
│   └── database_controller.go
├── main.go
└── Dockerfile
```

Permisos RBAC para Operator:

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-operator

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-operator-role
rules:
# Permisos para el CRD
- apiGroups: ["mycompany.io"]
  resources: ["databases"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

# Permisos para Deployments
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

# Permisos para Services
- apiGroups: [""]
  resources: ["services"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

# Permisos para StatefulSets
- apiGroups: ["apps"]
  resources: ["statefulsets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

# Permisos para Secrets
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-operator-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-operator-role
subjects:
- kind: ServiceAccount
  name: my-operator
  namespace: operators
```

Desplegar Operator:

```bash
# Crear namespace
kubectl create namespace operators

# Instalar CRD
kubectl apply -f config/crd/database_crd.yaml

# Crear RBAC
kubectl apply -f config/rbac/ -n operators

# Desplegar Operator
kubectl apply -f config/manager/manager.yaml -n operators

# Verificar
kubectl get pods -n operators
kubectl logs -f deployment/my-operator-controller -n operators
```

**Operator Framework**

Kubebuilder simplifica la creación de Operators.

Crear proyecto Kubebuilder:

```bash
# Instalar Kubebuilder
curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH)
chmod +x kubebuilder
sudo mv kubebuilder /usr/local/bin/

# Crear proyecto
kubebuilder init --domain mycompany.io --repo github.com/mycompany/my-operator

# Crear API (CRD + Controller)
kubebuilder create api \
  --group databases \
  --version v1 \
  --kind Database \
  --resource --controller
```

Estructura generada:

```
config/
├── crd/
├── rbac/
├── manager/
└── samples/

api/v1/
├── database_types.go
└── groupversion_info.go

controllers/
└── database_controller.go

main.go
Dockerfile
```

Definir el tipo de recurso (api/v1/database_types.go):

```go
package v1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

type DatabaseSpec struct {
	Engine   string `json:"engine"`
	Size     string `json:"size"`
	Backup   bool   `json:"backup,omitempty"`
	Replicas int32  `json:"replicas,omitempty"`
}

type DatabaseStatus struct {
	Phase     string `json:"phase,omitempty"`
	Ready     bool   `json:"ready,omitempty"`
	Endpoint  string `json:"endpoint,omitempty"`
}

// +kubebuilder:object:root=true
// +kubebuilder:subresource:status
type Database struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	Spec   DatabaseSpec   `json:"spec,omitempty"`
	Status DatabaseStatus `json:"status,omitempty"`
}

// +kubebuilder:object:root=true
type DatabaseList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []Database `json:"items"`
}

func init() {
	SchemeBuilder.Register(&Database{}, &DatabaseList{})
}
```

Implementar el Controller (controllers/database_controller.go):

```go
package controllers

import (
	"context"
	"github.com/mycompany/my-operator/api/v1"
	corev1 "k8s.io/api/core/v1"
	appsv1 "k8s.io/api/apps/v1"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type DatabaseReconciler struct {
	client.Client
}

// +kubebuilder:rbac:groups=databases.mycompany.io,resources=databases,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=apps,resources=deployments,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups="",resources=services,verbs=get;list;watch;create;update;patch;delete

func (r *DatabaseReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	// Obtener Database
	var db v1.Database
	if err := r.Get(ctx, req.NamespacedName, &db); err != nil {
		return ctrl.Result{}, client.IgnoreNotFound(err)
	}

	// Crear/actualizar Deployment
	deployment := &appsv1.Deployment{}
	if err := r.Get(ctx, req.NamespacedName, deployment); err != nil {
		// No existe, crear nuevo
		deployment = r.constructDeployment(&db)
		if err := r.Create(ctx, deployment); err != nil {
			return ctrl.Result{}, err
		}
	}

	// Actualizar status
	db.Status.Phase = "Running"
	db.Status.Ready = true
	db.Status.Endpoint = db.Name + ".default.svc.cluster.local"

	if err := r.Status().Update(ctx, &db); err != nil {
		return ctrl.Result{}, err
	}

	return ctrl.Result{}, nil
}

func (r *DatabaseReconciler) constructDeployment(db *v1.Database) *appsv1.Deployment {
	// Lógica para crear Deployment basado en Database spec
	replicas := int32(1)
	if db.Spec.Replicas > 0 {
		replicas = db.Spec.Replicas
	}

	deployment := &appsv1.Deployment{}
	deployment.Name = db.Name
	deployment.Namespace = db.Namespace
	deployment.Spec.Replicas = &replicas
	// ... más configuración

	return deployment
}

func (r *DatabaseReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&v1.Database{}).
		Owns(&appsv1.Deployment{}).
		Complete(r)
}
```

Compilar y ejecutar:

```bash
# Hacer test
make test

# Generar manifests
make manifests

# Compilar imagen Docker
make docker-build docker-push IMG=myregistry/my-operator:v0.1.0

# Desplegar
make deploy IMG=myregistry/my-operator:v0.1.0

# Crear instancia
kubectl apply -f config/samples/databases_v1_database.yaml

# Ver en acción
kubectl logs -f deployment/my-operator-controller -n my-operator-system
```

=== Admission Controllers

Los Admission Controllers interceptan y validan/modifican requests a la API de Kubernetes.

**Validating Webhooks**

Validan requests y rechazan si no cumplen políticas.

Crear validating webhook:

```go
package main

import (
	"encoding/json"
	"fmt"
	"io"
	"net/http"

	admissionv1 "k8s.io/api/admission/v1"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func handleValidate(w http.ResponseWriter, r *http.Request) {
	body, _ := io.ReadAll(r.Body)
	admissionReview := admissionv1.AdmissionReview{}
	json.Unmarshal(body, &admissionReview)

	var pod corev1.Pod
	json.Unmarshal(admissionReview.Request.Object.Raw, &pod)

	allowed := true
	message := ""

	// Validación: Pod debe tener resource limits
	for _, container := range pod.Spec.Containers {
		if container.Resources.Limits == nil {
			allowed = false
			message = "Container must have resource limits"
			break
		}
	}

	// Validación: Pod debe tener probes
	for _, container := range pod.Spec.Containers {
		if container.LivenessProbe == nil {
			allowed = false
			message = "Container must have liveness probe"
			break
		}
	}

	admissionResponse := &admissionv1.AdmissionResponse{
		UID:     admissionReview.Request.UID,
		Allowed: allowed,
		Result: &metav1.Status{
			Message: message,
		},
	}

	responseReview := admissionv1.AdmissionReview{
		TypeMeta: metav1.TypeMeta{
			APIVersion: "admission.k8s.io/v1",
			Kind:       "AdmissionReview",
		},
		Response: admissionResponse,
	}

	respBytes, _ := json.Marshal(responseReview)
	w.Header().Set("Content-Type", "application/json")
	w.Write(respBytes)
}

func main() {
	http.HandleFunc("/validate", handleValidate)
	http.ListenAndServeTLS(":8443", "/etc/webhook/certs/tls.crt", "/etc/webhook/certs/tls.key", nil)
}
```

Registrar validating webhook:

```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: pod-validator
webhooks:
- name: pod-validator.example.com
  clientConfig:
    service:
      name: webhook-service
      namespace: default
      path: "/validate"
    caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN...
  rules:
  - operations: ["CREATE", "UPDATE"]
    apiGroups: [""]
    apiVersions: ["v1"]
    resources: ["pods"]
  admissionReviewVersions: ["v1"]
  sideEffects: None
  failurePolicy: Fail       # Fail = rechazar si webhook no responde
```

**Mutating Webhooks**

Modifican requests antes de guardarlos.

Ejemplo: Inyectar sidecar automáticamente

```go
package main

import (
	"encoding/json"
	"io"
	"net/http"

	admissionv1 "k8s.io/api/admission/v1"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func handleMutate(w http.ResponseWriter, r *http.Request) {
	body, _ := io.ReadAll(r.Body)
	admissionReview := admissionv1.AdmissionReview{}
	json.Unmarshal(body, &admissionReview)

	var pod corev1.Pod
	json.Unmarshal(admissionReview.Request.Object.Raw, &pod)

	// Agregar sidecar
	sidecar := corev1.Container{
		Name:  "istio-proxy",
		Image: "istio/proxyv2:latest",
	}

	// Crear patch
	patch := []map[string]interface{}{
		{
			"op":    "add",
			"path":  "/spec/containers/-",
			"value": sidecar,
		},
	}

	patchBytes, _ := json.Marshal(patch)

	admissionResponse := &admissionv1.AdmissionResponse{
		UID:     admissionReview.Request.UID,
		Allowed: true,
		Patch:   patchBytes,
		PatchType: func() *admissionv1.PatchType {
			pt := admissionv1.PatchTypeJSONPatch
			return &pt
		}(),
	}

	responseReview := admissionv1.AdmissionReview{
		TypeMeta: metav1.TypeMeta{
			APIVersion: "admission.k8s.io/v1",
			Kind:       "AdmissionReview",
		},
		Response: admissionResponse,
	}

	respBytes, _ := json.Marshal(responseReview)
	w.Header().Set("Content-Type", "application/json")
	w.Write(respBytes)
}

func main() {
	http.HandleFunc("/mutate", handleMutate)
	http.ListenAndServeTLS(":8443", "/etc/webhook/certs/tls.crt", "/etc/webhook/certs/tls.key", nil)
}
```

Registrar mutating webhook:

```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: pod-injector
webhooks:
- name: pod-injector.example.com
  clientConfig:
    service:
      name: webhook-service
      namespace: default
      path: "/mutate"
    caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN...
  rules:
  - operations: ["CREATE"]
    apiGroups: [""]
    apiVersions: ["v1"]
    resources: ["pods"]
  admissionReviewVersions: ["v1"]
  sideEffects: None
  failurePolicy: Ignore    # Ignore = permitir si webhook falla
```

**OPA (Open Policy Agent)**

Motor de políticas agnóstico que funciona con Kubernetes.

Instalación:

```bash
# Instalar OPA
curl https://openpolicyagent.org/downloads/latest/opa_linux_amd64 -Lo opa
chmod +x opa

# Iniciar OPA como servidor
opa run -s
```

Política OPA (Rego):

```rego
# policies/pod_policy.rego

package kubernetes.admission

deny[msg] {
    input.request.kind.kind == "Pod"
    container := input.request.object.spec.containers[_]
    not container.resources.limits
    msg := sprintf("Container '%v' must have resource limits", [container.name])
}

deny[msg] {
    input.request.kind.kind == "Pod"
    container := input.request.object.spec.containers[_]
    not container.livenessProbe
    msg := sprintf("Container '%v' must have liveness probe", [container.name])
}

deny[msg] {
    input.request.kind.kind == "Pod"
    image := input.request.object.spec.containers[_].image
    not startswith(image, "gcr.io/")
    msg := sprintf("Image '%v' must be from approved registry (gcr.io)", [image])
}

allow {
    count(deny) == 0
}
```

Integración de OPA con Kubernetes:

```bash
# Compilar política
opa build -b policies/

# Registrar en cluster
kubectl create configmap opa-policy \
  --from-file=pod_policy.rego \
  -n opa

# Crear ValidatingWebhookConfiguration que use OPA
kubectl apply -f - <<EOF
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: opa-validator
webhooks:
- name: opa.example.com
  clientConfig:
    url: https://opa:8443/v1/data/kubernetes/admission
    caBundle: ...
  rules:
  - operations: ["CREATE", "UPDATE"]
    apiGroups: [""]
    apiVersions: ["v1"]
    resources: ["pods"]
  admissionReviewVersions: ["v1"]
  sideEffects: None
  failurePolicy: Fail
EOF
```

**Kyverno**

Política de Kubernetes nativa (alternativa más simple a OPA).

Instalación:

```bash
# Instalar Kyverno
helm repo add kyverno https://kyverno.github.io/kyverno/
helm install kyverno kyverno/kyverno \
  --namespace kyverno \
  --create-namespace
```

Política: Validar que los pods tengan resource limits

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-requests-limits
spec:
  validationFailureAction: audit  # audit o enforce
  rules:
  - name: check-container-limits
    match:
      resources:
        kinds:
        - Pod
    validate:
      message: "CPU and memory limits required"
      pattern:
        spec:
          containers:
          - resources:
              limits:
                memory: "?*"
                cpu: "?*"
```

Política: Requerir imágenes de registro específico

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-registry
spec:
  validationFailureAction: enforce
  rules:
  - name: validate-image-registry
    match:
      resources:
        kinds:
        - Pod
    validate:
      message: "Image must be from approved registries"
      pattern:
        spec:
          containers:
          - image: "gcr.io/* | docker.io/* | quay.io/*"
```

Política: Mutar (agregar labels)

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: add-labels
spec:
  rules:
  - name: add-app-label
    match:
      resources:
        kinds:
        - Pod
    mutate:
      patchStrategicMerge:
        metadata:
          labels:
            managed-by: kyverno
            policy-version: "1.0"
```

Política: Denegar pods privilegiados

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: restrict-privileged-containers
spec:
  validationFailureAction: enforce
  rules:
  - name: privileged
    match:
      resources:
        kinds:
        - Pod
    validate:
      message: "Privileged containers not allowed"
      pattern:
        spec:
          containers:
          - securityContext:
              privileged: false

  - name: capabilities
    match:
      resources:
        kinds:
        - Pod
    validate:
      message: "Dangerous capabilities not allowed"
      pattern:
        spec:
          containers:
          - securityContext:
                capabilities:
                  drop:
                  - ALL
```

Monitoreo de políticas:

```bash
# Ver políticas
kubectl get clusterpolicies
kubectl get policies -A

# Auditar violaciones (en modo audit)
kubectl get policyreport -A

# Detalle de violación
kubectl describe policyreport -n default

# Logs de Kyverno
kubectl logs -n kyverno deploy/kyverno -f
```

Comparación: OPA vs Kyverno

| Aspecto | OPA | Kyverno |
|---------|-----|---------|
| **Lenguaje** | Rego | YAML |
| **Curva aprendizaje** | Alta | Baja |
| **Complejidad** | Alta | Media |
| **Agnóstico** | Sí | Solo K8s |
| **Rendimiento** | Muy rápido | Rápido |
| **Ecosistema** | Grande | Creciente |
| **Mejor para** | Políticas complejas | Políticas simples |

=== API Server

El API Server es el centro de Kubernetes. Entender su gestión es crítico para operaciones avanzadas.

**API Versioning**

Kubernetes mantiene múltiples versiones de la API para compatibilidad.

Tipos de versiones:

```
Alpha:  v1alpha1, v1alpha2     - Experimental, cambios frecuentes
Beta:   v1beta1, v1beta2       - Casi estable, cambios menores
Stable: v1, v2, etc            - Versión de producción
```

Ejemplo de CRD con múltiples versiones:

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: databases.example.io
spec:
  group: example.io
  scope: Namespaced
  names:
    plural: databases
    kind: Database
  versions:
  # Versión antigua (beta)
  - name: v1beta1
    served: true
    storage: false
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              engine:
                type: string
              replicas:
                type: integer
  # Versión nueva (stable)
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              engine:
                type: string
              replicas:
                type: integer
              backup:           # Campo nuevo en v1
                type: object
  # Conversión entre versiones
  conversion:
    strategy: Webhook
    webhook:
      clientConfig:
        service:
          name: conversion-webhook
          namespace: default
          path: /convert
      conversionReviewVersions: ["v1"]
```

Estrategia de versiones:

```
Paso 1: Introducir versión nueva, mantener vieja
        - v1beta1 (served: true, storage: true)
        - v2beta1 (served: true, storage: false)

Paso 2: Cambiar storage a nueva versión
        - v1beta1 (served: true, storage: false)
        - v2beta1 (served: true, storage: true)

Paso 3: Eliminar versión vieja
        - v2beta1 (served: true, storage: true)
```

Ver versiones soportadas:

```bash
# Ver versiones API
kubectl api-versions

# Ver recursos en una versión
kubectl api-resources --api-group apps

# Información detallada
kubectl get apiservice
```

**API Deprecation**

Remover versiones de forma segura.

Anunciar deprecation:

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: databases.example.io
spec:
  group: example.io
  names:
    plural: databases
    kind: Database
  versions:
  # Versión deprecada
  - name: v1alpha1
    served: true
    storage: false
    deprecated: true           # Marcar como deprecada
    deprecationWarning: "v1alpha1 is deprecated, use v1 instead"
    schema:
      openAPIV3Schema:
        type: object
  # Versión recomendada
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
```

Kubernetes mantiene un horario de deprecation:

```
Kubernetes 1.18: Funcionalidad marcada como deprecated
Kubernetes 1.19: Última versión que sirve versión deprecated
Kubernetes 1.20: Versión deprecated removida
```

Checar deprecations:

```bash
# Ver políticas de deprecación
kubectl api-resources --deprecated

# Logs de advertencia al usar recurso deprecated
kubectl apply -f old-api.yaml
# Warning: Detected deprecated API object: <kind> ...
```

Herramientas de migración:

```bash
# kubepug: analiza manifests en busca de APIs deprecated
wget https://github.com/rikatz/kubepug/releases/download/v1.5.1/kubepug_linux_amd64
chmod +x kubepug_linux_amd64

./kubepug_linux_amd64 --input-file=manifest.yaml --k8s-version v1.22.0

# Resultado: advertencias sobre APIs deprecated
```

**Custom API Aggregation**

Extender la API de Kubernetes con servicios customizados.

Caso de uso: Crear una API personalizada que vive dentro de Kubernetes.

Crear servicio de API customizada:

```go
package main

import (
	"encoding/json"
	"net/http"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apiserver/pkg/endpoints/handlers"
	"k8s.io/apiserver/pkg/endpoints/handlers/responsewriters"
)

type Widget struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	Spec              WidgetSpec `json:"spec,omitempty"`
}

type WidgetSpec struct {
	Name  string `json:"name"`
	Color string `json:"color"`
}

type WidgetList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []Widget `json:"items"`
}

var widgets = []Widget{
	{
		ObjectMeta: metav1.ObjectMeta{Name: "widget1"},
		Spec: WidgetSpec{Name: "Widget 1", Color: "red"},
	},
}

func handleGetWidgets(w http.ResponseWriter, r *http.Request) {
	widgetList := &WidgetList{
		Items: widgets,
	}
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(widgetList)
}

func handleGetWidget(w http.ResponseWriter, r *http.Request) {
	name := r.URL.Query().Get("name")
	for _, widget := range widgets {
		if widget.Name == name {
			w.Header().Set("Content-Type", "application/json")
			json.NewEncoder(w).Encode(widget)
			return
		}
	}
	http.NotFound(w, r)
}

func main() {
	http.HandleFunc("/apis/example.io/v1/widgets", handleGetWidgets)
	http.HandleFunc("/apis/example.io/v1/widgets/", handleGetWidget)
	http.ListenAndServe(":8443", nil)
}
```

Registrar API aggregada:

```yaml
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  name: v1.example.io
spec:
  # Versión que se sirve
  version: v1
  # Grupo de la API
  group: example.io
  # Dónde encontrar esta API
  service:
    name: my-api-service
    namespace: default
    port: 443
  # Certificado CA del servicio
  caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN...
  # Insecuro en desarrollo
  insecureSkipTLSVerify: true
```

Usar API agregada:

```bash
# Una vez registrada, aparece en api-versions
kubectl api-versions | grep example.io
example.io/v1

# Acceder a través de kubectl
kubectl get widgets

# A través de API REST
curl https://kubernetes:6443/apis/example.io/v1/widgets \
  --cert client.crt \
  --key client.key \
  --cacert ca.crt
```

Ejemplo completo con deployment:

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: custom-api

---
apiVersion: v1
kind: Service
metadata:
  name: my-api-service
  namespace: custom-api
spec:
  ports:
  - port: 443
    targetPort: 8443
  selector:
    app: my-api

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-api
  namespace: custom-api
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-api
  template:
    metadata:
      labels:
        app: my-api
    spec:
      containers:
      - name: api
        image: my-api-server:1.0
        ports:
        - containerPort: 8443
        volumeMounts:
        - name: tls
          mountPath: /etc/tls
      volumes:
      - name: tls
        secret:
          secretName: my-api-tls

---
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  name: v1.example.io
spec:
  version: v1
  group: example.io
  service:
    name: my-api-service
    namespace: custom-api
    port: 443
  caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0t...
```

Debugging de API Server:

```bash
# Ver logs del API Server
kubectl logs -n kube-system kube-apiserver-<node>

# Ver solicitudes a la API
kubectl logs -n kube-system kube-apiserver | grep "audit"

# Metricas del API Server
kubectl get --raw /metrics | grep apiserver_

# Ver hooks de admission registrados
kubectl get validatingwebhookconfigurations
kubectl get mutatingwebhookconfigurations

# Ver API aggregados
kubectl get apiservice
```

Performance tuning del API Server:

```yaml
# Configuración de API Server (en kube-apiserver)
--max-requests-inflight=400           # Max requests concurrentes sin autenticación
--max-mutating-requests-inflight=200  # Max requests de modificación
--request-timeout=5m                  # Timeout de requests
--etcd-request-timeout=1m             # Timeout al hablar con etcd
```

Control de rate limiting:

```bash
# Ver límites actuales
kubectl get --raw /metrics | grep "apiserver_client_requests_total" | head -5

# Configurar limites por usuario
kubectl apply -f - <<EOF
apiVersion: apiserver.config.k8s.io/v1
kind: FlowSchema
metadata:
  name: default-limits
spec:
  distinguisherMethod:
    type: ByUser
  matchingPrecedence: 1000
  priorityLevelConfiguration:
    name: default-pl
  rules:
  - nonResourceRules:
    - nonResourceURLs:
      - /api
      - /api/*
EOF
```

== Módulo 14: Kubernetes en Producción

=== Arquitectura de Producción

**High Availability**

Diseñar clusters Kubernetes para máxima disponibilidad.

Arquitectura de HA:

```
┌─────────────────────────────────────────────────────┐
│                  Load Balancer                       │
└──────────┬──────────────────────────┬────────────────┘
           │                          │
    ┌──────▼──────┐          ┌────────▼──────┐
    │  API Server │          │  API Server    │
    │  (Replica1) │          │  (Replica2)    │
    └──────┬──────┘          └────────┬───────┘
           │                          │
    ┌──────▼──────────────────────────▼───────┐
    │         etcd Cluster (3+ nodos)         │
    └────────────────────────────────────────┘
           ▲              ▲              ▲
       Control1      Control2      Control3
```

Configuración de HA:

```yaml
# 1. API Server con múltiples réplicas
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  replicas: 3
  selector:
    matchLabels:
      component: kube-apiserver
  template:
    metadata:
      labels:
        component: kube-apiserver
    spec:
      priorityClassName: system-cluster-critical
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: component
                operator: In
                values:
                - kube-apiserver
            topologyKey: kubernetes.io/hostname
      containers:
      - name: kube-apiserver
        image: k8s.gcr.io/kube-apiserver:v1.27.0
        ports:
        - containerPort: 6443
        env:
        - name: ETCD_SERVERS
          value: "https://etcd1:2379,https://etcd2:2379,https://etcd3:2379"

---
# 2. Service para API Server
apiVersion: v1
kind: Service
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  type: LoadBalancer
  selector:
    component: kube-apiserver
  ports:
  - port: 6443
    targetPort: 6443

---
# 3. etcd con múltiples réplicas
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: etcd
  namespace: kube-system
spec:
  serviceName: etcd
  replicas: 3
  selector:
    matchLabels:
      component: etcd
  template:
    metadata:
      labels:
        component: etcd
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: component
                operator: In
                values:
                - etcd
            topologyKey: kubernetes.io/hostname
      containers:
      - name: etcd
        image: quay.io/coreos/etcd:v3.5.0
        ports:
        - containerPort: 2379
          name: client
        - containerPort: 2380
          name: peer
        env:
        - name: ETCD_LISTEN_CLIENT_URLS
          value: "https://0.0.0.0:2379"
        - name: ETCD_INITIAL_CLUSTER
          value: "etcd-0=https://etcd-0.etcd:2380,etcd-1=https://etcd-1.etcd:2380,etcd-2=https://etcd-2.etcd:2380"
        volumeMounts:
        - name: data
          mountPath: /var/lib/etcd
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 100Gi
```

Pod Disruption Budgets para HA:

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: apiserver-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      component: kube-apiserver

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: etcd-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      component: etcd

---
# Para aplicaciones críticas
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-app-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: critical-app
```

**Multi-region Deployments**

Desplegar aplicaciones en múltiples regiones.

Arquitectura multi-región:

```
┌──────────────────┐      ┌──────────────────┐
│   Region US-East │      │   Region EU-West │
├──────────────────┤      ├──────────────────┤
│   Cluster A      │      │   Cluster B      │
│   3 nodos        │      │   3 nodos        │
│   DB Primaria    │      │   DB Replica     │
└──────────────────┘      └──────────────────┘
         ▲                         ▲
         └─────────────────────────┘
          Replicación de datos
```

Configuración multi-región:

```yaml
# Cluster A (US-East) - Primario
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    region: us-east
    tier: primary

---
# Deployment primario
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  namespace: production
spec:
  replicas: 3
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: region
            operator: In
            values:
            - us-east
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: myapp:v1.0.0
        env:
        - name: DB_ENDPOINT
          value: "db-primary.us-east.example.com"

---
# Cluster B (EU-West) - Réplica
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    region: eu-west
    tier: replica

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  namespace: production
spec:
  replicas: 3
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: region
            operator: In
            values:
            - eu-west
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: myapp:v1.0.0
        env:
        - name: DB_ENDPOINT
          value: "db-replica.eu-west.example.com"
```

Replicación de datos entre regiones:

```bash
# Configurar replicación de BD (ejemplo PostgreSQL)
# Cluster primario US-East
PGUSER=postgres PGPASSWORD=password psql -h db-primary.us-east \
  -c "CREATE PUBLICATION all_tables FOR ALL TABLES;"

# Cluster secundario EU-West
PGUSER=postgres PGPASSWORD=password psql -h db-replica.eu-west \
  -c "CREATE SUBSCRIPTION all_from_primary CONNECTION 'host=db-primary.us-east user=postgres' \
      PUBLICATION all_tables WITH (copy_data = true);"
```

**Load Balancing**

Distribuir tráfico entre clusters.

Global Load Balancer:

```yaml
# DNS con failover automático
apiVersion: v1
kind: Endpoints
metadata:
  name: myapp-global
  namespace: production
subsets:
- addresses:
  - ip: 10.0.0.10  # VIP de cluster US-East
    targetRef:
      kind: Node
      name: us-east-cluster
  - ip: 10.0.1.10  # VIP de cluster EU-West
    targetRef:
      kind: Node
      name: eu-west-cluster
  ports:
  - port: 80
    protocol: TCP

---
# Service que usa esos endpoints
apiVersion: v1
kind: Service
metadata:
  name: myapp-global
  namespace: production
spec:
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: myapp
```

Ingress con múltiples clusters:

```yaml
# Ingress que balancea entre clusters
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-global
  namespace: production
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - myapp.example.com
    secretName: myapp-tls
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: myapp-us-east
            port:
              number: 80
      - path: /eu
        pathType: Prefix
        backend:
          service:
            name: myapp-eu-west
            port:
              number: 80
```

**Capacity Planning**

Planificar recursos para escalar.

Análisis de capacidad:

```bash
# 1. Analizar uso actual
kubectl top nodes
kubectl top pods -A

# 2. Proyectar crecimiento (ejemplo: 50% crecimiento anual)
# Node actual: 32GB RAM
# Promedio por pod: 512MB
# Pods actuales: 40
# RAM usada: 20GB

# Crecimiento esperado en 12 meses:
# Pods esperados: 40 * 1.5 = 60
# RAM esperada: 60 * 512MB = 30.72GB
# Con headroom (20%): 30.72GB * 1.2 = 36.86GB
# Nuevas máquinas necesarias: ceil((36.86 - 32) / 32) = 1 nodo

# 3. Monitorear tendencias
kubectl apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: capacity-planning
data:
  script.sh: |
    #!/bin/bash
    while true; do
      echo "$(date): $(kubectl top nodes | awk 'NR>1 {sum+=\$3} END {print sum}')" >> /tmp/cpu.log
      sleep 3600
    done
EOF
```

Autoscaling de cluster:

```yaml
# Cluster Autoscaler (para cloud platforms)
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-priority-expander
  namespace: kube-system
data:
  priorities: |
    10:
      - .*spot.*
    50:
      - .*on-demand.*

---
# Monitoring para capacity planning
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-capacity-planning
data:
  capacity.yml: |
    groups:
    - name: capacity
      rules:
      - record: cluster:node:count
        expr: count(kube_node_status_allocatable)
      - record: cluster:cpu:allocatable
        expr: sum(kube_node_status_allocatable{resource="cpu"})
      - record: cluster:memory:allocatable
        expr: sum(kube_node_status_allocatable{resource="memory"})
      - record: cluster:cpu:usage
        expr: sum(node_cpu_seconds_total) / cluster:cpu:allocatable
      - record: cluster:memory:usage
        expr: sum(node_memory_MemAvailable_bytes) / cluster:memory:allocatable
```

Alertas de capacidad:

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: capacity-alerts
spec:
  groups:
  - name: capacity
    rules:
    - alert: ClusterCPUUtilizationHigh
      expr: cluster:cpu:usage > 0.75
      for: 5m
      annotations:
        summary: "Cluster CPU utilization is {{ $value }}"

    - alert: ClusterMemoryUtilizationHigh
      expr: cluster:memory:usage > 0.80
      for: 5m
      annotations:
        summary: "Cluster memory utilization is {{ $value }}"

    - alert: NotEnoughNodesAvailable
      expr: count(kube_node_status_condition{condition="Ready",status="true"}) < 3
      for: 5m
      annotations:
        summary: "Only {{ $value }} nodes available, minimum is 3"
```

=== Hardening

Fortalecer la seguridad de Kubernetes siguiendo mejores prácticas.

**Security Best Practices**

1. **Pod Security Standards**:

```yaml
# Pod Security Policy (deprecated, usar PSS)
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'MustRunAs'
    seLinuxOptions:
      level: "s0:c123,c456"
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  readOnlyRootFilesystem: true

---
# Pod Security Standards (reemplazo de PSP)
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

2. **Seguridad de contenedores**:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 2000
    seccompProfile:
      type: RuntimeDefault
  containers:
  - name: app
    image: myapp:v1
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 50m
        memory: 64Mi
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 30
      periodSeconds: 10
```

3. **Network Policies**:

```yaml
# Denegar todo tráfico por defecto
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress

---
# Permitir solo tráfico específico
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-web
spec:
  podSelector:
    matchLabels:
      role: web
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: production
    ports:
    - protocol: TCP
      port: 8080
```

4. **RBAC (Role-Based Access Control)**:

```yaml
# Role con permisos mínimos
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: default
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get"]

---
# RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pod-reader
subjects:
- kind: ServiceAccount
  name: default
  namespace: default
```

**CIS Kubernetes Benchmark**

El CIS Benchmark proporciona recomendaciones de configuración.

Checklist de CIS (resumen):

```
1. Control Plane Configuration
   ✓ API Server: --basic-auth-file no debe estar configurado
   ✓ API Server: --token-auth-file no debe estar configurado
   ✓ API Server: --enable-admin-admission-controller
   ✓ Kubelet: --protect-kernel-defaults=true

2. ETCD
   ✓ Usar certificados para peer communication
   ✓ client-cert-auth: true
   ✓ Encriptar datos en reposo

3. General Configuration
   ✓ Usar Red Hat Enterprise Linux / CentOS como OS
   ✓ kernel version >= 4.15
   ✓ Configurar SELinux en enforcing
```

Ejecutar kube-bench para validar:

```bash
# Instalar kube-bench
kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job.yaml

# Ejecutar y obtener resultados
kubectl logs -f job/kube-bench

# Resultados mostrará:
# [PASS] 1.1.1 Ensure that the API server executable permissions are set to 644 or more restrictive
# [FAIL] 1.2.1 Ensure that the --basic-auth-file argument is not set
```

**Compliance**

Mantener compliance con regulaciones (GDPR, HIPAA, SOC2, PCI-DSS).

Implementar auditoría:

```yaml
# Configurar API Server Audit Log
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
# Log de todos los requests a escalada de privilegios
- level: RequestResponse
  verbs: ["create", "update", "patch"]
  resources:
  - group: ""
    resources: ["pods"]
  omitStages:
  - RequestReceived

# Log de todos los requests fallidos
- level: Metadata
  omitStages:
  - RequestReceived
  userGroups:
  - "system:authenticated"

# Log por defecto
- level: None
  omitStages:
  - RequestReceived
```

Monitoreo de compliance:

```bash
# Validar policies están activas
kubectl get networkpolicies -A

# Verificar RBAC está configurado
kubectl get roles -A
kubectl get rolebindings -A

# Auditar access logs
kubectl logs -n kube-system kube-apiserver | grep audit
```

**Auditing**

Registrar y monitorear todas las operaciones.

Configuración avanzada de auditoría:

```yaml
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
# 1. Log de cambios en secrets
- level: RequestResponse
  verbs: ["create", "update", "patch", "delete"]
  resources:
  - group: ""
    resources: ["secrets"]
  omitStages:
  - RequestReceived

# 2. Log de cambios en RBAC
- level: RequestResponse
  verbs: ["create", "update", "patch", "delete"]
  resources:
  - group: "rbac.authorization.k8s.io"
    resources: ["roles", "rolebindings", "clusterroles", "clusterrolebindings"]
  omitStages:
  - RequestReceived

# 3. Log de cambios en control plane
- level: RequestResponse
  verbs: ["create", "update", "patch", "delete"]
  resources:
  - group: "apps"
    resources: ["deployments", "daemonsets", "statefulsets"]
  omitStages:
  - RequestReceived

# 4. Log de exec en pods
- level: RequestResponse
  verbs: ["create"]
  resources:
  - group: ""
    resources: ["pods/exec", "pods/portforward"]
  omitStages:
  - RequestReceived

# 5. Log todos los demás requests (pero en nivel Metadata)
- level: Metadata
  omitStages:
  - RequestReceived
```

Analizar audit logs:

```bash
# Ver audit logs
tail -f /var/log/kubernetes/audit.log | jq '.'

# Filtrar por usuario
tail -f /var/log/kubernetes/audit.log | jq 'select(.user.username == "admin")'

# Filtrar por recurso
tail -f /var/log/kubernetes/audit.log | jq 'select(.objectRef.resource == "pods")'

# Filtrar por verbo
tail -f /var/log/kubernetes/audit.log | jq 'select(.verb == "delete")'

# Contar eventos por tipo
jq -r '.verb' /var/log/kubernetes/audit.log | sort | uniq -c

# Detectar actividad sospechosa
jq 'select(.verb == "create" and .objectRef.resource == "clusterrolebindings")' /var/log/kubernetes/audit.log
```

Alertas en base a audit logs:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: audit-alerts
data:
  rules.yml: |
    groups:
    - name: kubernetes-audit
      rules:
      - alert: UnauthorizedAPIAccess
        expr: |
          rate(apiserver_audit_event_total{user_agent != "kubelet"}[5m]) > 10
        for: 5m
        annotations:
          summary: "Unauthorized API access detected"

      - alert: PrivilegeEscalation
        expr: |
          apiserver_audit_event_total{verb="create", objectRef_resource="clusterrolebindings"} > 0
        annotations:
          summary: "Potential privilege escalation attempt"

      - alert: SecretAccessViolation
        expr: |
          apiserver_audit_event_total{objectRef_resource="secrets", verb="get"} > 100
        for: 5m
        annotations:
          summary: "Unusual secret access pattern detected"
```

Enviar audit logs a sistema externo:

```bash
# Configurar webhook para audit logs
# En API Server startup flags:
# --audit-webhook-config-file=/etc/kubernetes/audit-webhook.yaml

# audit-webhook.yaml content:
# apiVersion: v1
# kind: Config
# clusters:
# - name: falco
#   cluster:
#     server: http://falco-service:5555/
# contexts:
# - context:
#     cluster: falco
#     user: ""
#   name: default-context
# current-context: default-context
# preferences: {}
# users: []
```

=== Disaster Recovery

**Backup Strategies**

Estrategias de backup para diferentes datos en Kubernetes.

Tipos de backups:

```
1. Cluster Backup:
   - Estado del cluster (etcd)
   - Configuración de nodos
   - Definiciones de recursos

2. Application Backup:
   - Datos persistentes (PVs)
   - Configuración de aplicación
   - Secrets y ConfigMaps

3. Database Backup:
   - Snapshots de base de datos
   - Backups transaccionales
   - Replicación

4. Infrastructure Backup:
   - Snapshots de volúmenes
   - Snapshots de máquinas
   - Configuración de red
```

Plan de backup:

```yaml
# Ejemplo de política de backup diaria
cronSchedule: "0 2 * * *"     # 2 AM diariamente

# Retención
retention:
  ttl: "720h"                  # 30 días

# Locations
locations:
- name: aws-s3
  provider: aws
  bucket: my-backups
  prefix: daily

# Volumenes a respaldar
volumeSnapshotLocation:
  name: aws-ebs
  provider: aws
```

**Velero**

Ya cubierto en Módulo 13.1, pero resumen práctico:

```bash
# Crear schedules de backup
velero schedule create weekly \
  --schedule="0 2 * * 0" \
  --include-namespaces production \
  --ttl 168h

# Backup incremental
velero backup create prod-backup-$(date +%s) \
  --include-namespaces production \
  --selector app=myapp

# Listar y monitorear
velero backup logs <backup-name>
velero backup describe <backup-name> --details

# Restaurar en nuevo cluster
# En cluster destino:
velero restore create --from-backup prod-backup-123
```

**etcd Backup y Restore**

Backup manual de etcd:

```bash
# 1. Hacer snapshot
ETCDCTL_API=3 etcdctl --endpoints=127.0.0.1:2379 \
  snapshot save backup.db

# 2. Verificar integridad
ETCDCTL_API=3 etcdctl snapshot status backup.db

# 3. Restaurar en nuevo cluster
ETCDCTL_API=3 etcdctl snapshot restore backup.db \
  --data-dir /var/lib/etcd-backup

# 4. Reiniciar etcd con nuevo data-dir
systemctl restart etcd
```

Automatizar backups de etcd:

```yaml
apiVersion: v1
kind: CronJob
metadata:
  name: etcd-backup
  namespace: kube-system
spec:
  schedule: "0 * * * *"  # Cada hora
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: etcd-backup
          containers:
          - name: backup
            image: bitnami/etcd:latest
            command:
            - /bin/sh
            - -c
            - |
              etcdctl --endpoints=http://etcd:2379 \
                snapshot save /backups/etcd-$(date +%s).db
            volumeMounts:
            - name: backup-dir
              mountPath: /backups
          volumes:
          - name: backup-dir
            hostPath:
              path: /mnt/etcd-backups
          restartPolicy: OnFailure
```

**Testing de Recuperación**

Validar que los backups funcionan:

```bash
# 1. Crear backup
velero backup create test-backup

# 2. Esperar a que complete
velero backup describe test-backup --wait

# 3. Simular desastre - restaurar en namespace de test
velero restore create --from-backup test-backup \
  --namespace-mappings production:test-restore

# 4. Validar
kubectl get all -n test-restore

# 5. Limpiar si todo OK
velero restore delete <restore-name>
```

DR Testing checklist:

```
□ Crear backup
□ Verificar backup está completo
□ Restaurar en ambiente diferente
□ Validar datos están completos
□ Validar aplicaciones funcionan
□ Validar conectividad de BD
□ Pruebas de funcionalidad básicas
□ Documentar tiempo de restauración
□ Limpiar ambiente de test
```

=== Cost Optimization

**Resource Optimization**

Optimizar uso de recursos del cluster.

Análisis de uso:

```bash
# Ver uso actual por namespace
kubectl top pods -A | awk '{print $1}' | sort | uniq -c | sort -rn

# Identificar pods sin requests/limits
kubectl get pods -A -o json | jq '.items[] | select(.spec.containers[].resources.requests == null) | .metadata.namespace + "/" + .metadata.name'

# Ver pods que usan más recursos
kubectl top pods -A --sort-by=memory | tail -20
```

Establecer requests y limits:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: optimized-app
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: myapp:v1
        resources:
          requests:
            cpu: 100m          # Mínimo garantizado
            memory: 128Mi
          limits:
            cpu: 500m          # Máximo permitido
            memory: 512Mi       # Evita OOMKill
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]  # Grace period
```

Usar Vertical Pod Autoscaler (VPA):

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: vpa-auto-sizing
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: myapp
  updatePolicy:
    updateMode: "auto"  # auto, off, initial, recreate
  resourcePolicy:
    containerPolicies:
    - containerName: "*"
      minAllowed:
        cpu: 50m
        memory: 64Mi
      maxAllowed:
        cpu: 1
        memory: 2Gi
      recommendedResources: true
```

**Spot Instances**

Usar instancias spot (preemptibles) para reducir costos.

Affinidad para Spot:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cost-optimized-app
spec:
  replicas: 3
  template:
    spec:
      affinity:
        nodeAffinity:
          # Preferir spot, permitir on-demand
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                - spot
      tolerations:
      # Tolerar interrupciones de spot
      - key: cloud.google.com/gke-preemptible
        operator: Equal
        value: "true"
        effect: NoSchedule
      containers:
      - name: app
        image: myapp:v1
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 30"]  # Tiempo para graceful shutdown
```

**Cluster Efficiency**

Maximizar eficiencia del cluster.

Bin packing agresivo:

```yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
systemReserved:
  cpu: 100m
  memory: 100Mi
kubeReserved:
  cpu: 100m
  memory: 100Mi
evictionHard:
  memory.available: "100Mi"
  nodefs.available: "2%"
  nodefs.inodesFree: "5%"
```

Priority Classes para importancia:

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: critical
value: 1000
globalDefault: false
description: "For critical production workloads"

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: batch
value: 100
description: "For batch processing jobs"

---
# Usar en Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-app
spec:
  template:
    spec:
      priorityClassName: critical
      containers:
      - name: app
        image: myapp:v1
```

**Cost Monitoring**

Monitorear y reportar costos.

Kubecost - Herramienta de visualización:

```bash
# Instalar Kubecost
helm repo add kubecost https://kubecost.github.io/cost-analyzer/
helm install kubecost kubecost/cost-analyzer \
  --namespace kubecost \
  --create-namespace
```

Métricas de costo:

```bash
# Acceder a dashboard
kubectl port-forward -n kubecost svc/kubecost 9090:9090

# API para costos por namespace
curl http://localhost:9090/api/v1/allocations

# Costos por pod
curl http://localhost:9090/api/v1/Assets/pods
```

Alertas de costo:

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cost-alerts
spec:
  groups:
  - name: kubecost
    rules:
    - alert: HighClusterCost
      expr: kubecost_cluster_hourly_cost > 50
      for: 1h
      annotations:
        summary: "Cluster cost is ${{ $value }}/hour"

    - alert: HighNamespaceCost
      expr: kubecost_namespace_hourly_cost > 10
      for: 1h
      annotations:
        summary: "Namespace {{ $labels.namespace }} cost is ${{ $value }}/hour"
```

=== Troubleshooting Avanzado

**Debugging de Problemas de Red**

Diagnosticar problemas de conectividad.

Verificar CNI:

```bash
# Ver plugin CNI
kubectl get daemonset -n kube-system -l k8s-app=flannel

# Verificar IP assignment
kubectl get pods -o wide
kubectl describe pod <pod> | grep IP

# Test conectividad
kubectl run debug --image=busybox --rm -it -- sh
# Desde dentro:
wget http://service-name:port
nslookup service-name
traceroute 10.x.x.x
```

Debug avanzado con tcpdump:

```bash
# En nodo: capturar tráfico
tcpdump -i any -w /tmp/capture.pcap port 8080

# Analizar con Wireshark
# scp capture.pcap local-machine:
```

Usar Network Policy debugging:

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-debug
spec:
  podSelector:
    matchLabels:
      debug: "true"
  ingress:
  - from:
    - podSelector: {}
    ports:
    - port: 8080
```

**Performance Issues**

Identificar y resolver problemas de performance.

Analizar latencia:

```bash
# Ver requests lentos
kubectl top nodes
kubectl top pods -A --sort-by=cpu

# Analizar API Server latency
kubectl get --raw /metrics | grep apiserver_request_duration_seconds

# Ver logs de latencia
kubectl logs -n kube-system kube-apiserver | grep "duration"
```

Tuning de Kubelet:

```yaml
# /etc/kubernetes/kubelet-config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
maxPods: 110                    # Max pods por nodo
registryPullQPS: 10            # Rate limit para pulls
registryBurst: 20
serializeImagePulls: false      # Pull paralelo
cpuManager:
  policyName: static            # CPU pinning
topologyManager:
  topologyPolicyName: best-effort  # NUMA awareness
```

**Resource Contention**

Gestionar competencia por recursos.

CPU throttling:

```bash
# Detectar pods siendo throttled
kubectl get --raw /metrics | grep 'container_cpu_cfs_throttled_seconds_total'

# Solución: aumentar limits o replicas
kubectl set resources deployment myapp \
  --limits=cpu=500m,memory=512Mi \
  --requests=cpu=250m,memory=256Mi
```

Memory pressure:

```bash
# Detectar pods bajo memory pressure
kubectl describe node <node> | grep -A5 "Conditions"

# Ver eviciones
kubectl get events -A --field-selector reason=Evicted

# Solución: añadir nodos o optimizar requests
```

**Análisis de Crash Loops**

Diagnosticar pods que crashean continuamente.

Obtener logs:

```bash
# Ver últimos logs antes del crash
kubectl logs <pod> --previous

# Ver todos los eventos
kubectl describe pod <pod>

# Logs con timestamps
kubectl logs <pod> --timestamps

# Ver exit code
kubectl get pod <pod> -o jsonpath='{.status.containerStatuses[0].lastState.terminated.exitCode}'
```

Debugging de crashloops:

```yaml
# Crear pod de debug con mismo image
apiVersion: v1
kind: Pod
metadata:
  name: debug-crash
spec:
  containers:
  - name: debug
    image: myapp:broken
    command: ["/bin/sh"]
    args: ["-c", "sleep 3600"]  # No ejecutar comando que causa crash
    stdin: true
    tty: true
```

Monitoreo de crash loops:

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: crash-loop-alerts
spec:
  groups:
  - name: kubernetes
    rules:
    - alert: CrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0.1
      for: 5m
      annotations:
        summary: "Pod {{ $labels.pod }} is crash looping"

    - alert: ContainerNotReady
      expr: min_over_time(kube_pod_status_ready{condition="true"}[1m]) < 1
      for: 5m
      annotations:
        summary: "Pod {{ $labels.pod }} is not ready"
```

== Módulo 15: Casos de Uso y Patrones

=== Microservicios

Los microservicios representan uno de los casos de uso más comunes y exitosos de Kubernetes. Esta arquitectura divide las aplicaciones en servicios pequeños, independientes y desacoplados que se comunican entre sí.

==== Arquitectura de Microservicios en Kubernetes

La arquitectura de microservicios aprovecha las capacidades de Kubernetes de:

* **Aislamiento**: Cada microservicio corre en su propio contenedor
* **Escalabilidad independiente**: Escalar servicios específicos según la demanda
* **Despliegues independientes**: Actualizar servicios sin afectar otros
* **Resiliencia**: Fallos en un servicio no derriban toda la aplicación

**Ejemplo de arquitectura típica:**

----
┌─────────────────────────────────────────────────────────┐
│                    Cliente / API Gateway                │
└──────────────────┬──────────────────────────────────────┘
                   │
    ┌──────────────┼──────────────┐
    ▼              ▼              ▼
┌────────┐   ┌────────┐   ┌────────────┐
│ Auth   │   │ Order  │   │ Payment    │
│Service │   │Service │   │Service     │
└────────┘   └────────┘   └────────────┘
    │              │              │
    └──────────────┼──────────────┘
                   │
          ┌────────┴────────┐
          ▼                 ▼
    ┌─────────────┐   ┌──────────────┐
    │  Database   │   │ Cache (Redis)│
    └─────────────┘   └──────────────┘
----

==== Service Discovery

En Kubernetes, el service discovery ocurre automáticamente gracias al DNS interno:

[source,yaml]
----
# Cada servicio obtiene un DNS automático
# Formato: <nombre-servicio>.<namespace>.svc.cluster.local

# Ejemplo: Si tenemos un servicio "api-service" en namespace "production"
# Los pods pueden acceder con: http://api-service.production.svc.cluster.local
----

**Implementación con Kubernetes Services:**

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: order-service
  namespace: production
spec:
  type: ClusterIP
  selector:
    app: order-api
  ports:
  - port: 80
    targetPort: 8080
    name: http
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-api
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: order-api
  template:
    metadata:
      labels:
        app: order-api
    spec:
      containers:
      - name: order-api
        image: myregistry/order-api:v1.0
        ports:
        - containerPort: 8080
----

Los pods pueden descubrir servicios automáticamente mediante:

- **DNS**: `order-service.production.svc.cluster.local`
- **Variables de entorno**: Kubernetes inyecta variables como `ORDER_SERVICE_PORT`
- **Headless Services**: Para aplicaciones que necesitan IPs específicas de pods

**Headless Service para control fino:**

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: database
spec:
  clusterIP: None  # Headless service
  selector:
    app: postgres
  ports:
  - port: 5432
----

==== Patrones API Gateway

El API Gateway actúa como punto de entrada único para todos los clientes:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-gateway
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - api.example.com
    secretName: api-tls
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /auth
        pathType: Prefix
        backend:
          service:
            name: auth-service
            port:
              number: 80
      - path: /orders
        pathType: Prefix
        backend:
          service:
            name: order-service
            port:
              number: 80
      - path: /payments
        pathType: Prefix
        backend:
          service:
            name: payment-service
            port:
              number: 80
----

**Ventajas del API Gateway:**

- Punto de entrada único
- Autenticación y autorización centralizadas
- Rate limiting y throttling
- Enrutamiento inteligente
- Logging y monitoreo centralizado

==== Comunicación Entre Servicios

Las estrategias de comunicación incluyen:

**Sincrónica (REST/gRPC):**

[source,yaml]
----
# Service A llama a Service B
apiVersion: v1
kind: ConfigMap
metadata:
  name: service-urls
data:
  ORDER_SERVICE_URL: "http://order-service.production.svc.cluster.local"
  PAYMENT_SERVICE_URL: "http://payment-service.production.svc.cluster.local"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-consumer
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:v1
        envFrom:
        - configMapRef:
            name: service-urls
----

**Asincrónica (Mensaje Queue):**

[source,yaml]
----
# Usar RabbitMQ o Kafka para desacoplamiento
apiVersion: v1
kind: Service
metadata:
  name: rabbitmq
spec:
  ports:
  - port: 5672
    targetPort: 5672
  selector:
    app: rabbitmq
---
# Publicar eventos en lugar de llamadas directas
# Event: order.created -> payment-service la escucha
----

**Patrones de Resilencia:**

- **Circuit Breaker**: Fallar rápido si un servicio está caído
- **Retry con backoff exponencial**: Reintentar con esperas crecientes
- **Timeout**: Establecer límites de tiempo de espera
- **Bulkhead**: Aislar grupos de recursos

[source,yaml]
----
# Usar NetworkPolicy para controlar comunicación
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-order-to-payment
spec:
  podSelector:
    matchLabels:
      app: order-api
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: payment-api
    ports:
    - protocol: TCP
      port: 8080
----

=== Bases de Datos

Kubernetes puede ejecutar bases de datos, aunque requiere consideraciones especiales para persistencia, backup y replicación. Las aplicaciones stateful como las bases de datos tienen requisitos diferentes a las aplicaciones stateless.

==== Despliegue de Bases de Datos

**Opciones de despliegue:**

1. **Managed Services (Recomendado para Producción)**
   - Usar servicios gestionados: AWS RDS, Google Cloud SQL, Azure Database
   - Menor complejidad operacional
   - Backups automáticos
   - Alta disponibilidad

2. **Kubernetes-Native (Para desarrollo y casos específicos)**
   - Aplicable si necesitas control total
   - Requiere expertise en operaciones

**Arquitectura típica para despliegue en Kubernetes:**

----
┌──────────────────────────────────────────┐
│    Ingress / LoadBalancer                │
│  (Acceso a la base de datos)             │
└──────────────────┬───────────────────────┘
                   │
        ┌──────────┴──────────┐
        ▼                     ▼
   ┌─────────┐           ┌─────────┐
   │ Primary │  Replica  │Replica 2│
   │   DB    │ ------→   │   DB    │
   └────┬────┘           └─────────┘
        │
   ┌────┴─────┐
   ▼          ▼
┌────────┐ ┌────────┐
│PVC: DB │ │PVC: Log│
└────────┘ └────────┘
----

==== StatefulSets para Bases de Datos

Los StatefulSets son cruciales para aplicaciones stateful que requieren:

- Identidades estables
- Almacenamiento persistente
- Orden garantizado de despliegue

**Ejemplo: PostgreSQL con StatefulSet**

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: fast-ssd
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      terminationGracePeriodSeconds: 30
      containers:
      - name: postgres
        image: postgres:15-alpine
        ports:
        - containerPort: 5432
          name: postgres
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres
          initialDelaySeconds: 10
          periodSeconds: 5
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes:
        - ReadWriteOnce
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
spec:
  clusterIP: None  # Headless service para StatefulSet
  ports:
  - port: 5432
    targetPort: 5432
  selector:
    app: postgres
----

**Características importantes:**

- **serviceName**: Necesario para DNS estable
- **volumeClaimTemplates**: Crea un PVC por réplica
- **terminationGracePeriodSeconds**: Tiempo para shutdown limpio
- **Headless Service**: Acceso directo a pods específicos

==== Operators de Bases de Datos

Los Operators automatizan tareas complejas de bases de datos mediante lógica encapsulada en el cluster:

**Operators populares:**

- **PostgreSQL Operator**: Automatiza backup, replicación y escalado
- **MySQL Operator**: Gestión automática de MySQL
- **MongoDB Operator**: Manejo de clusters MongoDB
- **Elasticsearch Operator**: Gestión de clusters Elasticsearch

**Ejemplo usando PostgreSQL Operator:**

[source,yaml]
----
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: my-database
spec:
  instances: 3
  primaryUpdateStrategy: unsupervised
  postgresql:
    parameters:
      max_connections: "200"
      shared_buffers: "256MB"
  bootstrap:
    initdb:
      database: myapp
      owner: app_user
  storage:
    size: 10Gi
    storageClass: fast-ssd
  monitoring:
    enabled: true
  backup:
    barmanObjectStore:
      destinationPath: s3://my-bucket/backups
      s3Credentials:
        accessKeyId:
          name: s3-creds
          key: access_key
        secretAccessKey:
          name: s3-creds
          key: secret_key
----

El Operator se encarga de:

- Crear la estructura del cluster
- Configurar replicación
- Gestionar failover automático
- Realizar backups programados
- Restauración de datos

==== Backup y Replicación

**Estrategia de Backup:**

[source,yaml]
----
# Backup programado con CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: database-backup
spec:
  schedule: "0 2 * * *"  # Diariamente a las 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: postgres:15-alpine
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-credentials
                  key: password
            command:
            - /bin/bash
            - -c
            - |
              pg_dump -h postgres -U postgres mydb > \
              /backups/db-backup-$(date +%Y%m%d-%H%M%S).sql
            volumeMounts:
            - name: backup-storage
              mountPath: /backups
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc
          restartPolicy: OnFailure
----

**Replicación:**

[source,yaml]
----
# Configurar replicación streaming
# En el StatefulSet del Primary:
- name: STREAMING_REPLICATION
  value: "true"
- name: REPLICATION_SLOTS
  value: "replication"

# Replicas se conectan como:
# Host: postgres-0.postgres.default.svc.cluster.local
# Replication user: replicator
----

**Verificación de estado:**

[source,bash]
----
# Ver replicas conectadas en PostgreSQL
kubectl exec -it postgres-0 -- \
  psql -U postgres -c "SELECT usename, application_name, state \
  FROM pg_stat_replication;"

# Ver retraso de replicación (lag)
kubectl exec -it postgres-0 -- \
  psql -U postgres -c "SELECT now() - pg_last_xact_replay_timestamp() \
  AS replication_lag;"
----

**Best Practices:**

- Usar StorageClass con replicación en el storage (RAID)
- Mantener backups en ubicación diferente (S3, GCS, etc.)
- Implementar automáticas health checks
- Testear restauración de backups regularmente
- Usar Operators para automatización
- Separar base de datos de aplicación en casos de producción
- Implementar monitoring y alertas
- Documentar procedimientos de disaster recovery

=== Machine Learning

Kubernetes se ha convertido en la plataforma estándar para entrenar, servir e implementar modelos de machine learning a escala. Permite gestionar los recursos computacionales intensivos y distribuir trabajos de ML complejos.

==== Kubeflow

Kubeflow es una plataforma de ML nativa de Kubernetes que proporciona componentes para el ciclo de vida completo del ML.

**Instalación de Kubeflow:**

[source,bash]
----
# Instalar Kubeflow (requiere kubectl configurado)
cd ~
git clone https://github.com/kubeflow/manifests.git
cd manifests

# Instalar componentes base
kustomize build example | kubectl apply -f -

# Verificar instalación
kubectl get pods -n kubeflow
----

**Componentes principales:**

- **Jupyter Notebooks**: Desarrollo interactivo
- **Training Operators**: Para entrenar modelos (TFJob, PyTorchJob)
- **KServe**: Servir modelos en producción
- **Pipelines**: Orquestar flujos de ML complejos
- **AutoML**: Automatizar búsqueda de hyperparámetros

**Ejemplo: Entrenar modelo con PyTorchJob**

[source,yaml]
----
apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: pytorch-training
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      template:
        spec:
          containers:
          - name: pytorch
            image: pytorch/pytorch:latest
            command:
            - python
            - /app/train.py
            resources:
              requests:
                memory: "4Gi"
                cpu: "2"
              limits:
                memory: "8Gi"
                cpu: "4"
            volumeMounts:
            - name: training-data
              mountPath: /data
          volumes:
          - name: training-data
            persistentVolumeClaim:
              claimName: ml-data-pvc
    Worker:
      replicas: 3
      template:
        spec:
          containers:
          - name: pytorch
            image: pytorch/pytorch:latest
            command:
            - python
            - /app/train.py
            resources:
              requests:
                memory: "4Gi"
                cpu: "2"
              limits:
                memory: "8Gi"
                cpu: "4"
            volumeMounts:
            - name: training-data
              mountPath: /data
          volumes:
          - name: training-data
            persistentVolumeClaim:
              claimName: ml-data-pvc
----

==== GPU Scheduling

Kubernetes puede gestionar GPUs como recursos programables. Esto es crítico para entrenar modelos de deep learning.

**Configuración de nodos con GPU:**

[source,yaml]
----
# Ver GPUs disponibles en el cluster
kubectl describe nodes | grep -A5 "nvidia.com/gpu"

# Resultado esperado:
# nvidia.com/gpu: 8    (8 GPUs disponibles)
----

**Solicitar GPUs en pods:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: gpu-training
spec:
  containers:
  - name: training
    image: tensorflow/tensorflow:latest-gpu
    resources:
      requests:
        nvidia.com/gpu: 2    # Solicitar 2 GPUs
      limits:
        nvidia.com/gpu: 2
    env:
    - name: CUDA_VISIBLE_DEVICES
      value: "0,1"
----

**Estatuas de GPU en el cluster:**

[source,bash]
----
# Ver distribución de GPUs
kubectl get nodes -o custom-columns=\
NAME:.metadata.name,\
GPU:.status.allocatable.nvidia\\.com/gpu,\
GPU_USED:.status.allocated_resources.nvidia\\.com/gpu

# Monitorear uso de GPU
kubectl top nodes --containers

# Logs de GPU en un pod
kubectl logs pod-name | grep CUDA
----

**Affinity para seleccionar tipo de GPU:**

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-inference
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: accelerator
                operator: In
                values:
                - nvidia-tesla-v100  # Seleccionar GPU V100
      containers:
      - name: inference
        image: tensorflow/tensorflow:latest-gpu
        resources:
          requests:
            nvidia.com/gpu: 1
          limits:
            nvidia.com/gpu: 1
----

==== Model Serving

Servir modelos en producción requiere alta disponibilidad, escalabilidad y bajo latencia.

**KServe para serving de modelos:**

[source,yaml]
----
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: iris-classifier
spec:
  predictor:
    sklearn:
      storageUri: s3://my-bucket/models/iris-classifier
      resources:
        requests:
          memory: "1Gi"
          cpu: "500m"
        limits:
          memory: "2Gi"
          cpu: "1"
  transformer:
    containers:
    - name: transformer
      image: my-registry/iris-transformer:v1
      env:
      - name: MODEL_NAME
        value: iris
  explainer:
    containers:
    - name: explainer
      image: my-registry/iris-explainer:v1
----

**Características:**

- **Automatic scaling**: Escala automática según carga
- **Traffic splitting**: Canary deployments A/B testing
- **Model monitoring**: Detección de data drift
- **Inference logging**: Auditoría y debugging

**Ejemplo con canary deployment:**

[source,yaml]
----
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: model-v2
spec:
  predictor:
    canaryTrafficPercent: 20  # 20% tráfico al nuevo modelo
    containers:
    - name: kserve-container
      image: model-v2:latest
  predictor:
    canaryTrafficPercent: 80  # 80% tráfico al modelo anterior
    containers:
    - name: kserve-container
      image: model-v1:latest
----

==== MLOps Pipelines

Orquestar flujos complejos de ML con reproducibilidad y auditoría.

**Kubeflow Pipelines:**

[source,python]
----
import kfp
from kfp import dsl
from kfp.components import create_component_from_func

@create_component_from_func
def prepare_data(input_path: str, output_path: str):
    # Código de preparación de datos
    pass

@create_component_from_func
def train_model(data_path: str, model_path: str):
    # Código de entrenamiento
    pass

@create_component_from_func
def evaluate_model(model_path: str, data_path: str) -> float:
    # Código de evaluación
    return accuracy

@dsl.pipeline(
    name='ML Pipeline',
    description='End-to-end ML pipeline'
)
def ml_pipeline(input_data: str):
    prepare = prepare_data(input_path=input_data,
                          output_path='/data/processed')
    train = train_model(data_path=prepare.outputs['output_path'],
                       model_path='/models/trained')
    evaluate = evaluate_model(model_path=train.outputs['model_path'],
                             data_path=prepare.outputs['output_path'])

# Compilar y ejecutar
kfp.compiler.Compiler().compile(ml_pipeline, 'pipeline.zip')
----

**Ventajas de MLOps Pipelines:**

- Reproducibilidad: Volver a ejecutar con mismos datos y parámetros
- Auditoría: Registro de qué código/datos creó cada modelo
- Escalabilidad: Distribuir componentes en múltiples nodos
- Experimentación: Tracking automático de experimentos
- Orquestación: Dependencias entre tareas

**Ejemplo YAML Pipeline:**

[source,yaml]
----
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ml-pipeline-
spec:
  entrypoint: main
  templates:
  - name: main
    dag:
      tasks:
      - name: prepare-data
        template: prepare
      - name: train
        template: train
        dependencies: prepare-data
      - name: evaluate
        template: evaluate
        dependencies: train

  - name: prepare
    container:
      image: ml-image:v1
      command: [python, prepare.py]

  - name: train
    container:
      image: ml-image:v1
      command: [python, train.py]
      resources:
        requests:
          nvidia.com/gpu: 1

  - name: evaluate
    container:
      image: ml-image:v1
      command: [python, evaluate.py]
----

**Best Practices para ML en Kubernetes:**

- Usar namespaces separados para desarrollo y producción
- Implementar resource quotas para limitar consumo
- Usar network policies para aislamiento
- Versionear datos y modelos en registries
- Implementar monitoring de data drift
- Automated retraining cuando metrics degradan
- Documentar preprocessing para reproducibilidad
- Usar GitOps para manage de configuraciones

=== Serverless

La computación serverless en Kubernetes permite ejecutar funciones y aplicaciones sin gestionar infraestructura. Los usuarios solo escriben código y especifican eventos; la plataforma maneja el escalado automático y la facturación por uso.

==== Knative

Knative es la plataforma serverless de Kubernetes más madura y respaldada por Google. Proporciona abstracciones para aplicaciones y funciones.

**Componentes principales:**

- **Knative Serving**: Ejecutar aplicaciones sin servidor
- **Knative Eventing**: Enrutamiento de eventos asincrónico
- **Knative Functions**: SDK para crear funciones rápidamente

**Instalación de Knative:**

[source,bash]
----
# Instalar Knative Serving
kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.8.0/serving-crds.yaml
kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.8.0/serving-core.yaml

# Instalar Knative Eventing
kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.8.0/eventing-crds.yaml
kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.8.0/eventing-core.yaml

# Verificar instalación
kubectl get pods -n knative-serving
kubectl get pods -n knative-eventing
----

**Crear un servicio Knative (Serverless):**

[source,yaml]
----
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: hello-world
spec:
  template:
    spec:
      containers:
      - image: gcr.io/knative-samples/helloworld-go
        env:
        - name: PORT
          value: "8080"
      timeoutSeconds: 300
    metadata:
      annotations:
        autoscaling.knative.dev/minScale: "0"        # Scale a 0
        autoscaling.knative.dev/maxScale: "100"      # Máx 100 instancias
        autoscaling.knative.dev/target: "70"         # CPU target
  traffic:
  - percent: 100
    latestRevision: true
----

**Características de Knative Serving:**

- **Scale-to-zero**: Reducir a 0 cuando no hay tráfico
- **Automatic scaling**: Basado en tráfico (RPS, CPU, custom metrics)
- **Revisiones**: Cada despliegue crea una revisión inmutable
- **Traffic splitting**: Canary deployments y A/B testing
- **Cold start optimization**: Inicialización rápida de funciones

**Ejemplo: Knative Function con Python**

[source,yaml]
----
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: sentiment-analysis
spec:
  template:
    spec:
      containers:
      - image: my-registry/sentiment-analyzer:v1
        ports:
        - containerPort: 8080
        env:
        - name: MODEL_PATH
          value: /models/sentiment
        volumeMounts:
        - name: models
          mountPath: /models
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: ml-models-pvc
      terminationGracePeriodSeconds: 60
    metadata:
      annotations:
        autoscaling.knative.dev/target: "100"  # 100 RPS por instancia
        autoscaling.knative.dev/minScale: "1"

# Traffic splitting: 80% tráfico a v1, 20% a v2
  traffic:
  - revisionName: sentiment-analysis-v1
    percent: 80
  - revisionName: sentiment-analysis-v2
    percent: 20
----

==== Knative Eventing

Arquitetura event-driven asincrónica para comunicación entre servicios.

**Conceptos principales:**

- **EventSource**: Origen de eventos (Kafka, CloudEvents, etc.)
- **Broker**: Enrutador central de eventos
- **Trigger**: Suscripción a eventos con filtros
- **Sink**: Destino de los eventos (servicio, función)

**Ejemplo: Event-driven pipeline**

[source,yaml]
----
# EventSource: Kafka como origen de eventos
apiVersion: sources.knative.dev/v1beta1
kind: KafkaSource
metadata:
  name: order-events
spec:
  bootstrapServers:
  - kafka-broker.kafka:9092
  topics:
  - orders
  consumerGroup: order-processing
  sink:
    ref:
      apiVersion: eventing.knative.dev/v1
      kind: Broker
      name: default
---
# Broker: Enrutador central
apiVersion: eventing.knative.dev/v1
kind: Broker
metadata:
  name: default
---
# Trigger 1: Procesar órdenes
apiVersion: eventing.knative.dev/v1
kind: Trigger
metadata:
  name: process-order
spec:
  broker: default
  filter:
    attributes:
      type: order.created
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: order-processor
---
# Trigger 2: Enviar notificaciones
apiVersion: eventing.knative.dev/v1
kind: Trigger
metadata:
  name: send-notification
spec:
  broker: default
  filter:
    attributes:
      type: order.completed
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: notification-service
----

==== OpenFaaS

Framework serverless más simple y flexible que Knative, con soporte para Docker.

**Instalación de OpenFaaS:**

[source,bash]
----
# Usar Helm para instalar OpenFaaS
helm repo add openfaas https://openfaas.github.io/faas-netes/
helm install openfaas openfaas/openfaas \
  --namespace openfaas --create-namespace

# Obtener contraseña de admin
kubectl get secret -n openfaas basic-auth -o jsonpath="{.data.basic-auth-password}" | base64 --decode
----

**Crear una función OpenFaaS:**

[source,yaml]
----
version: 1.0
provider:
  name: kubernetes
  gateway: http://localhost:8080
functions:
  sentiment-analysis:
    lang: python3
    handler: ./sentiment
    image: username/sentiment-analysis:latest
    environment:
      MODEL: sentiment-v2
    requests:
      memory: 128Mi
      cpu: 100m
    limits:
      memory: 256Mi
      cpu: 500m
    labels:
      com.openfaas.scale.min: 1
      com.openfaas.scale.max: 50
    annotations:
      prometheus: "true"
----

**Estructura de función OpenFaaS (Python):**

[source,python]
----
# handler.py
def handle(req):
    """
    Función serverless OpenFaaS
    req: objeto de request
    """
    import json

    try:
        data = json.loads(req)
        text = data.get('text', '')

        # Análisis de sentimiento
        from textblob import TextBlob
        sentiment = TextBlob(text).sentiment.polarity

        return json.dumps({
            'text': text,
            'sentiment': sentiment,
            'status': 'success'
        })
    except Exception as e:
        return json.dumps({
            'error': str(e),
            'status': 'error'
        })
----

==== Kubeless

Plataforma serverless diseñada específicamente para Kubernetes.

**Instalación de Kubeless:**

[source,bash]
----
# Descargar e instalar Kubeless
export RELEASE=$(curl -s https://api.github.com/repos/kubeless/kubeless/releases/latest | grep tag_name | cut -d '"' -f 4)
kubectl create ns kubeless
kubectl create -f https://github.com/kubeless/kubeless/releases/download/$RELEASE/kubeless-$RELEASE.yaml
----

**Crear función Kubeless:**

[source,yaml]
----
apiVersion: kubeless.io/v1beta1
kind: Function
metadata:
  name: http-trigger
spec:
  handler: index.handler
  runtime: python3.8
  code: |
    def handler(event, context):
        return {
            'statusCode': 200,
            'body': 'Hello from Kubeless!'
        }
  horizontalPodAutoscaler:
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
  limits:
    memory: 256Mi
    cpu: 200m
  requests:
    memory: 128Mi
    cpu: 100m
  service:
    ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
----

==== Arquitecturas Event-Driven

**Patrones comunes:**

**1. Request-Response (Sincrónica):**

[source]
----
Client → API Gateway → Function → Response
----

**2. Pub-Sub (Asincrónica):**

[source]
----
Event Source → Message Broker → Subscribers (múltiples funciones)
----

**3. CQRS con Eventos:**

[source]
----
Command → Write Service → Event → Read Services (actualizar replicas)
----

**Ejemplo completo: Sistema de órdenes event-driven:**

[source,yaml]
----
# Step 1: Order Service recibe orden
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: order-service
spec:
  template:
    spec:
      containers:
      - image: order-service:v1
---
# Step 2: Broker enruta eventos
apiVersion: eventing.knative.dev/v1
kind: Broker
metadata:
  name: orders
---
# Step 3: Payment Service procesa pagos
apiVersion: eventing.knative.dev/v1
kind: Trigger
metadata:
  name: payment-processor
spec:
  broker: orders
  filter:
    attributes:
      type: order.created
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: payment-service
---
# Step 4: Shipping Service prepara envío
apiVersion: eventing.knative.dev/v1
kind: Trigger
metadata:
  name: shipping-handler
spec:
  broker: orders
  filter:
    attributes:
      type: order.paid
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: shipping-service
---
# Step 5: Notification Service notifica
apiVersion: eventing.knative.dev/v1
kind: Trigger
metadata:
  name: notification-handler
spec:
  broker: orders
  filter:
    attributes:
      type: order.shipped
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: notification-service
----

**Ventajas de Serverless en Kubernetes:**

- **Scale-to-zero**: Ahorra costos cuando no hay carga
- **Escalado automático**: Responde a demanda en segundos
- **Enfoque en código**: Menos preocupación por infraestructura
- **Bajo costo**: Pagar solo por uso real
- **Event-driven**: Natural para microservicios

**Best Practices Serverless:**

- Mantener funciones pequeñas y enfocadas
- Implementar health checks
- Usar tracing distribuido para debugging
- Monitorear cold starts
- Implementar retry logic para llamadas fallidas
- Usar timeouts apropiados
- Documentar dependencias de eventos
- Versionear cambios en esquemas de eventos

=== IoT y Edge Computing

IoT y Edge Computing requieren orquestación de contenedores en entornos con recursos limitados. Kubernetes proporciona soluciones específicas para estos casos.

==== K3s: Kubernetes Ligero para Edge

K3s es una distribución ligera de Kubernetes, optimizada para entornos con recursos limitados, IoT y edge computing.

**Características de K3s:**

- **Tamaño reducido**: ~40MB vs 100MB+ de Kubernetes estándar
- **Requisitos bajos**: Funciona con 512MB RAM
- **Binario único**: Fácil instalación y mantenimiento
- **Integración de SQLite**: Base de datos embebida por defecto
- **Soporte completo de Kubernetes**: Compatible con APIs estándar

**Instalación de K3s:**

[source,bash]
----
# Instalación rápida en Linux
curl -sfL https://get.k3s.io | sh -

# Instalar sin systemd
curl -sfL https://get.k3s.io | sh -s - --no-systemd

# Instalar en Raspberry Pi
curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE="644" sh -

# Verificar instalación
sudo k3s kubectl get nodes

# Unir worker nodes al cluster
K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken ./k3s agent
----

**Arquitectura mínima K3s:**

[source]
----
┌─────────────────────────────┐
│   K3s Server Node           │
│  (API, kubelet, controller) │
│  ~40MB, 512MB RAM mínimo    │
└──────────┬──────────────────┘
           │
    ┌──────┴──────┬──────────┐
    ▼             ▼          ▼
┌────────┐  ┌────────┐  ┌────────┐
│Worker 1│  │Worker 2│  │Worker 3│
│IoT Dev │  │IoT Dev │  │IoT Dev │
└────────┘  └────────┘  └────────┘
----

**Configuración de K3s para Edge:**

[source,yaml]
----
# Instalación minimal con recursos limitados
apiVersion: v1
kind: Pod
metadata:
  name: edge-app
spec:
  containers:
  - name: app
    image: myapp:edge
    resources:
      requests:
        memory: "32Mi"    # Mínimo
        cpu: "50m"        # 50 milicores
      limits:
        memory: "64Mi"    # Límite bajo
        cpu: "100m"
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    emptyDir: {}
    sizeLimit: 100Mi      # Limitar tamaño de almacenamiento
  nodeSelector:
    node-type: edge       # Ejecutar solo en nodos edge
----

**Almacenamiento local en K3s:**

[source,yaml]
----
# StorageClass local para edge
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
# PVC para datos de IoT
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sensor-data
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 1Gi
----

==== KubeEdge

KubeEdge extiende Kubernetes a dispositivos edge, permitiendo gestionar miles de nodos con latencia baja y desconexión resiliente.

**Arquitectura de KubeEdge:**

[source]
----
┌─────────────────────────┐
│  Cloud (Cloud Side)     │
│  - Kubernetes Master    │
│  - CloudHub (gRPC)      │
└──────────┬──────────────┘
           │ gRPC Tunnel
    ┌──────┴──────────────────┐
    │  Edge (Edge Side)       │
    │  - EdgeHub (Edge Node)  │
    │  - Device Mapper        │
    │  - MetaManager          │
    └──────┬──────────────────┘
           │
    ┌──────┴──────┬──────────┐
    ▼             ▼          ▼
┌──────────┐ ┌──────────┐ ┌──────────┐
│Raspberry │ │ IoT Dev. │ │ Sensor   │
│   Pi     │ │ Gateway  │ │ Network  │
└──────────┘ └──────────┘ └──────────┘
----

**Instalación de KubeEdge:**

[source,bash]
----
# Clonar repositorio de KubeEdge
git clone https://github.com/kubeedge/kubeedge.git
cd kubeedge

# Instalar CloudCore en el maestro
./keadm init --advertise-address=<cloud-node-ip>

# Obtener token para unir edge nodes
./keadm gettoken

# Instalar EdgeCore en dispositivo edge
./keadm join --cloudcore-ipport=<cloud-ip>:10000 \
             --token=<token-del-maestro> \
             --edgenode-name=<edge-node-name>
----

**Configuración de Device Twin en KubeEdge:**

[source,yaml]
----
apiVersion: devices.kubeedge.io/v1alpha2
kind: Device
metadata:
  name: temp-sensor
  namespace: default
spec:
  deviceModelRef:
    name: temperature-model
  nodeSelector:
    nodeName: edge-node-1
  properties:
  - name: temperature
    value: "25.0"
  - name: humidity
    value: "60"
  protocol:
    protocolName: modbus
    protocolConfig:
      ip: "192.168.1.100"
      port: "502"
  twins:
  - propertyName: temperature
    reportedValue: "25.0"
    desiredValue: "25.0"
  - propertyName: humidity
    reportedValue: "60"
    desiredValue: "60"
----

**Aplicación Edge que consume datos de sensores:**

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: edge-processor
spec:
  replicas: 1
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-role.kubernetes.io/edge
            operator: In
            values:
            - "true"
  template:
    spec:
      containers:
      - name: processor
        image: sensor-processor:v1
        env:
        - name: DEVICE_NAME
          value: temp-sensor
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
        volumeMounts:
        - name: cache
          mountPath: /var/cache
      volumes:
      - name: cache
        emptyDir:
          sizeLimit: 100Mi
----

==== Edge Deployments: Patrones

**Patrón 1: Cloud-Native con Síncrónización Edge**

[source]
----
┌─────────────────────────┐
│  Cloud                  │
│  - Análisis Heavy       │
│  - Machine Learning     │
│  - Almacenamiento       │
└──────────┬──────────────┘
           │ Sincronización
           │ de datos
    ┌──────┴──────┐
    ▼             ▼
┌────────┐  ┌────────┐
│ Edge 1 │  │ Edge 2 │
│ Local  │  │ Local  │
│ Apps   │  │ Apps   │
└────────┘  └────────┘
----

**Patrón 2: Filtrado en Edge**

[source,yaml]
----
# Solo enviar datos importantes a la nube
apiVersion: v1
kind: ConfigMap
metadata:
  name: edge-filter-rules
data:
  rules: |
    # Si temperatura > 30°C, enviar a cloud
    if temperature > 30:
      send_to_cloud()

    # Almacenar datos locales
    store_local()

    # Ejecutar lógica de tiempo real local
    if temperature > 35:
      activate_cooling()
----

**Ejemplo: IoT Smart Home**

[source,yaml]
----
# 1. Define sensor device
apiVersion: devices.kubeedge.io/v1alpha2
kind: Device
metadata:
  name: living-room-sensor
spec:
  deviceModelRef:
    name: climate-sensor
  nodeSelector:
    nodeName: edge-gateway
  protocol:
    protocolName: mqtt
    protocolConfig:
      ip: "192.168.1.50"
      port: "1883"
  twins:
  - propertyName: temperature
    reportedValue: "22"
  - propertyName: humidity
    reportedValue: "45"
---
# 2. Deploy edge app que procesa datos localmente
apiVersion: apps/v1
kind: Deployment
metadata:
  name: climate-controller
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-type
                operator: In
                values:
                - edge
      containers:
      - name: controller
        image: climate-control:v1
        resources:
          requests:
            memory: "32Mi"
            cpu: "50m"
          limits:
            memory: "64Mi"
            cpu: "100m"
        env:
        - name: SENSOR_NAME
          value: living-room-sensor
---
# 3. Send aggregated data to cloud
apiVersion: batch/v1
kind: CronJob
metadata:
  name: sync-to-cloud
spec:
  schedule: "*/5 * * * *"  # Cada 5 minutos
  jobTemplate:
    spec:
      template:
        spec:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: node-type
                    operator: In
                    values:
                    - edge
          containers:
          - name: sync
            image: cloud-sync:v1
            env:
            - name: CLOUD_API
              value: "https://cloud.example.com/api"
            resources:
              requests:
                memory: "16Mi"
                cpu: "50m"
          restartPolicy: OnFailure
----

==== Restricciones de Recursos en Edge

**Optimización para dispositivos con recursos limitados:**

[source,yaml]
----
# Limitar estrictamente consumo de recursos
apiVersion: v1
kind: Pod
metadata:
  name: lightweight-app
spec:
  containers:
  - name: app
    image: myapp:ultra-light  # Imagen base alpina
    resources:
      requests:
        memory: "16Mi"
        cpu: "25m"
      limits:
        memory: "32Mi"
        cpu: "50m"

    # Desabilitar logs verbosos
    env:
    - name: LOG_LEVEL
      value: "ERROR"

    # Usar buffer pequeño
    - name: BUFFER_SIZE
      value: "16384"

    # No mantener conexiones abiertas
    - name: CONNECTION_TIMEOUT
      value: "30"

    livenessProbe:
      initialDelaySeconds: 60  # Mayor delay en edge
      periodSeconds: 30

    readinessProbe:
      initialDelaySeconds: 30
      periodSeconds: 10

  # Evitar overcommit
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - fast-ssd  # Usar almacenamiento rápido
----

**Monitoreo en Edge:**

[source,bash]
----
# Ver consumo de recursos en nodos edge
kubectl top nodes -l node-type=edge

# Ver consumo por pod
kubectl top pods -l app=edge-app --namespace=default

# Obtener estado de conectividad de edge nodes
kubectl get nodes -L kubernetes.io/hostname,node-type

# Ver latencia de sincronización
kubectl logs -n kubeedge -l app=edgehub | grep latency
----

**Best Practices para Edge Computing:**

- **Aplicaciones ligeras**: Usar imágenes base mínimas (Alpine, Distroless)
- **Escalado manual**: Edge nodes tienen recursos limitados
- **Cache local**: Minimizar comunicación con cloud
- **Tolerancia a desconexión**: Diseñar apps que funcionen sin cloud
- **Actualizaciones seguras**: Verificar espacio antes de descargar imágenes
- **Monitoreo local**: No enviar todos los logs a cloud
- **Compresión de datos**: Reducir ancho de banda
- **Particionamiento de datos**: Procesar solo datos relevantes
- **Heartbeat**: Detectar desconexión de edge nodes
- **Sincronización inteligente**: Batching de cambios antes de enviar

== Anexos

=== A. Comandos kubectl Esenciales

==== Gestión de Recursos

**Crear y aplicar manifiestos:**

[source,bash]
----
# Aplicar un archivo YAML
kubectl apply -f deployment.yaml

# Aplicar todos los archivos en un directorio
kubectl apply -f ./manifests/

# Aplicar cambios de un archivo (editar in-place)
kubectl apply -f deployment.yaml --record

# Ver cambios antes de aplicar
kubectl apply -f deployment.yaml --dry-run=client

# Aplicar con output
kubectl apply -f deployment.yaml -o wide
----

**Crear recursos imperativamente:**

[source,bash]
----
# Crear un deployment imperativamente
kubectl create deployment nginx --image=nginx:latest --replicas=3

# Crear un servicio
kubectl expose deployment nginx --port=80 --target-port=80 --type=LoadBalancer

# Crear un namespace
kubectl create namespace production

# Crear un secret
kubectl create secret generic db-secret --from-literal=password=mysecret

# Crear un ConfigMap
kubectl create configmap app-config --from-file=config.yaml
----

**Eliminar recursos:**

[source,bash]
----
# Eliminar un recurso
kubectl delete deployment nginx

# Eliminar múltiples tipos
kubectl delete deployment,service nginx

# Eliminar por selector
kubectl delete pods -l app=nginx

# Eliminar todo en un namespace
kubectl delete all --all -n staging

# Eliminar recurso pero esperar a que finalice
kubectl delete pod nginx --grace-period=30
----

**Actualizar recursos:**

[source,bash]
----
# Actualizar una imagen
kubectl set image deployment/nginx nginx=nginx:1.20 --record

# Escalar manualmente
kubectl scale deployment nginx --replicas=5

# Editar recurso directamente
kubectl edit deployment nginx

# Patch un recurso (cambio mínimo)
kubectl patch deployment nginx -p '{"spec":{"replicas":3}}'

# Rollout a versión anterior
kubectl rollout undo deployment/nginx
----

==== Debugging y Troubleshooting

**Inspeccionar recursos:**

[source,bash]
----
# Ver descripción detallada
kubectl describe pod nginx-xyz

# Ver logs de un pod
kubectl logs nginx-xyz

# Ver logs de contenedor específico en pod multi-contenedor
kubectl logs nginx-xyz -c nginx

# Ver logs en tiempo real
kubectl logs -f nginx-xyz

# Ver logs de pod anterior (si crasheó)
kubectl logs nginx-xyz --previous

# Ver eventos de un pod
kubectl describe pod nginx-xyz | grep Events -A 20
----

**Acceso a pods:**

[source,bash]
----
# Ejecutar comando en pod
kubectl exec nginx-xyz -- ls -la

# Ejecutar comando interactivo en pod
kubectl exec -it nginx-xyz -- /bin/bash

# Copiar archivo desde pod
kubectl cp nginx-xyz:/var/log/access.log ./access.log

# Copiar archivo a pod
kubectl cp ./config.yaml nginx-xyz:/etc/config.yaml

# Port-forward a un pod
kubectl port-forward nginx-xyz 8080:80
----

**Monitoreo y estado:**

[source,bash]
----
# Ver consumo de recursos
kubectl top nodes

# Ver consumo por pod
kubectl top pods

# Ver eventos del cluster
kubectl get events --sort-by='.lastTimestamp'

# Ver estado de rollout
kubectl rollout status deployment/nginx

# Ver historial de cambios
kubectl rollout history deployment/nginx

# Ver cambios específicos de revisión
kubectl rollout history deployment/nginx --revision=2
----

==== Consultas y Filtros

**Listar recursos:**

[source,bash]
----
# Listar todos los pods
kubectl get pods

# Listar con información ampliada
kubectl get pods -o wide

# Listar en namespace específico
kubectl get pods -n production

# Listar de todos los namespaces
kubectl get pods --all-namespaces

# Listar deployments
kubectl get deployments

# Listar services
kubectl get svc

# Listar recursos por selector
kubectl get pods -l app=nginx

# Listar recursos sin labels específico
kubectl get pods -L app,version
----

**Filtros y selecciones:**

[source,bash]
----
# Filtrar por label
kubectl get pods -l tier=frontend

# Filtrar por múltiples labels
kubectl get pods -l app=api,version=v1

# Filtrar por namespace
kubectl get pods -n production

# Filtrar por campo
kubectl get pods --field-selector=status.phase=Running

# Filtrar pods en estado fallido
kubectl get pods --field-selector=status.phase=Failed

# Ver solo nombres de pods
kubectl get pods -o name

# Ver en formato JSON
kubectl get pods -o json

# Ver en formato YAML
kubectl get pods -o yaml

# Ver con columnas personalizadas
kubectl get pods -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,RESTARTS:.status.containerStatuses[0].restartCount
----

**Búsqueda y visualización:**

[source,bash]
----
# Buscar pod por nombre
kubectl get pods | grep nginx

# Ver estructura de un CRD
kubectl explain deployment
kubectl explain deployment.spec

# Ver estructuras anidadas
kubectl explain deployment.spec.template.spec.containers

# Buscar recursos de un tipo específico
kubectl api-resources

# Ver versiones API disponibles
kubectl api-versions
----

==== Shortcuts y Aliases Útiles

**Abreviaturas de kubectl:**

[source,bash]
----
# Crear alias en shell
alias k=kubectl
alias kg='kubectl get'
alias kd='kubectl delete'
alias kl='kubectl logs'
alias ke='kubectl exec'
alias kgp='kubectl get pods'
alias kgs='kubectl get svc'
alias kgd='kubectl get deployment'
alias kdesc='kubectl describe'

# Usar bash completion
source <(kubectl completion bash)

# Agregar permanentemente en ~/.bashrc
echo "source <(kubectl completion bash)" >> ~/.bashrc
echo "alias k=kubectl" >> ~/.bashrc
echo "complete -o default -F __start_kubectl k" >> ~/.bashrc
----

**Comandos útiles frecuentes:**

[source,bash]
----
# Ver todos los recursos en un namespace
kubectl get all -n production

# Ver pods que se están ejecutando
kubectl get pods --field-selector=status.phase=Running

# Ver pods fallidos
kubectl get pods --field-selector=status.phase=Failed

# Listar nodos con etiquetas
kubectl get nodes --show-labels

# Ver nodos y su capacidad
kubectl get nodes -o wide

# Ver eventos recientes
kubectl get events --sort-by='.lastTimestamp' | tail -20

# Limpiar pods completados
kubectl delete pods --field-selector=status.phase=Succeeded

# Forzar eliminación de pod
kubectl delete pod nginx-xyz --force --grace-period=0

# Ver taints en nodos
kubectl describe nodes | grep Taints

# Aplicar taint a nodo
kubectl taint nodes node-1 gpu=true:NoSchedule

# Quitar taint de nodo
kubectl taint nodes node-1 gpu:NoSchedule-
----

**Monitoreo rápido:**

[source,bash]
----
# Watch: actualizar en tiempo real
kubectl get pods -w

# Ver cambios en deployment
kubectl get deployment nginx -w

# Ver status de rollout
kubectl rollout status deployment/nginx -w

# Monitoreo avanzado
watch 'kubectl get pods -o wide'
----

**Exportar y guardar:**

[source,bash]
----
# Exportar configuración de un pod
kubectl get pod nginx-xyz -o yaml > nginx-pod.yaml

# Exportar deployment
kubectl get deployment nginx -o yaml > deployment.yaml

# Exportar todo un namespace
kubectl get all -n production -o yaml > production-backup.yaml

# Exportar sin metadata del sistema
kubectl get pod nginx-xyz -o yaml | kubectl neat
----

=== B. YAML Reference

==== Sintaxis Básica de YAML

YAML es el formato usado para definir recursos en Kubernetes. Es sensible a la indentación y utiliza espacios (no tabulaciones).

**Tipos de datos:**

[source,yaml]
----
# Strings (sin comillas, con comillas, o multiline)
nombre: "Kubernetes"
descripcion: curso de K8s
multiline: |
  Esta es una línea
  Esta es otra línea
  Preserva los saltos de línea

# Números
puerto: 8080
replicas: 3
cpu: 0.5

# Booleanos
habilitado: true
debug: false

# Null
valor_nulo: null
otro_nulo:

# Listas
puertos:
  - 8080
  - 8081
  - 8082

# Objetos (diccionarios)
metadata:
  nombre: mi-app
  version: v1
  labels:
    app: backend
    tier: api

# Alias y referencias
default_labels: &default-labels
  app: myapp
  version: v1

deployment_labels:
  <<: *default-labels  # Hereda las labels
  environment: prod
----

**Comentarios:**

[source,yaml]
----
# Este es un comentario de línea

apiVersion: v1  # Versión de la API
kind: Pod       # Tipo de recurso
----

==== Estructura de Manifiestos Kubernetes

Todo recurso de Kubernetes tiene la misma estructura base:

[source,yaml]
----
# Requerido: Versión de API
apiVersion: v1

# Requerido: Tipo de recurso
kind: Pod

# Requerido: Información del recurso
metadata:
  # Nombre único en el namespace
  name: mi-pod

  # Namespace (default si no especifica)
  namespace: default

  # Labels para seleccionar/agrupar recursos
  labels:
    app: backend
    version: v1
    tier: api

  # Anotaciones para metadatos no identificables
  annotations:
    descripcion: "Pod de ejemplo"
    owner: "team-platform"

# Requerido: Especificación del recurso
spec:
  # Contenido depende del tipo (Pod, Deployment, Service, etc.)
  containers:
  - name: app
    image: nginx:latest
----

==== Versiones de API

Las versiones definen qué campos están disponibles:

[source,yaml]
----
# Versión estable
apiVersion: v1

# Versiones beta (pueden cambiar)
apiVersion: apps/v1beta1

# Versiones estables con grupo
apiVersion: apps/v1
apiVersion: batch/v1

# Versiones avanzadas
apiVersion: networking.k8s.io/v1
apiVersion: storage.k8s.io/v1
apiVersion: rbac.authorization.k8s.io/v1

# Ver todas las versiones disponibles
# kubectl api-versions
----

==== Campos Comunes en Recursos

**Pod/Deployment/StatefulSet spec.template.spec:**

[source,yaml]
----
spec:
  # Tiempo para detener gracefully (shutdown)
  terminationGracePeriodSeconds: 30

  # DNS policy
  dnsPolicy: ClusterFirst

  # Restart policy
  restartPolicy: Always

  # Service account
  serviceAccountName: default

  # Security context (nivel pod)
  securityContext:
    runAsUser: 1000
    runAsNonRoot: true

  # Toleraciones a taints de nodos
  tolerations:
  - key: "gpu"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"

  # Afinidad a nodos
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

  # Volúmenes del pod
  volumes:
  - name: config
    configMap:
      name: app-config
  - name: storage
    persistentVolumeClaim:
      claimName: my-pvc

  # Contenedores
  containers:
  - name: app
    image: myapp:v1.0
    imagePullPolicy: IfNotPresent

    # Puertos
    ports:
    - name: http
      containerPort: 8080
      protocol: TCP

    # Variables de entorno
    env:
    - name: LOG_LEVEL
      value: "info"
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: password

    # Montar volúmenes
    volumeMounts:
    - name: config
      mountPath: /etc/config
    - name: storage
      mountPath: /data

    # Recursos (solicitudes y límites)
    resources:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "256Mi"
        cpu: "500m"

    # Health checks
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3

    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      initialDelaySeconds: 10
      periodSeconds: 5

    # Security context (nivel contenedor)
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true

    # Ciclo de vida
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh", "-c", "echo 'iniciando'"]
      preStop:
        exec:
          command: ["/bin/sh", "-c", "sleep 15"]
----

**Selector y match:**

[source,yaml]
----
# En Deployment/Service/etc
selector:
  matchLabels:
    app: nginx
    version: v1

# O alternativa
selector:
  app: nginx
  version: v1
----

==== Ejemplos de Manifiestos Completos

**Ejemplo 1: Pod simple**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    ports:
    - containerPort: 80
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
----

**Ejemplo 2: Deployment con Service**

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      containers:
      - name: api
        image: mycompany/api:v2.0
        ports:
        - containerPort: 8080
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: url
        resources:
          requests:
            memory: "256Mi"
            cpu: "500m"
          limits:
            memory: "512Mi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: api-server
spec:
  selector:
    app: api
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP
----

**Ejemplo 3: StatefulSet con PVC**

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:13
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
----

**Ejemplo 4: ConfigMap y Secret**

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  app.properties: |
    server.port=8080
    debug=true
  database.yaml: |
    host: postgres.default.svc.cluster.local
    port: 5432
---
apiVersion: v1
kind: Secret
metadata:
  name: db-credentials
type: Opaque
data:
  # Base64 encoded (echo -n 'password' | base64)
  password: cGFzc3dvcmQ=
  username: YWRtaW4=
----

**Validación de sintaxis:**

[source,bash]
----
# Validar YAML sin aplicar
kubectl apply -f deployment.yaml --dry-run=client

# Validar y ver lo que se aplicaría
kubectl apply -f deployment.yaml --dry-run=client -o yaml

# Usar herramientas externas
# kubeval deployment.yaml
# kube-score score deployment.yaml
----

=== C. Glosario

==== Términos Clave

**API Server**:: Servidor central de Kubernetes que expone la API REST y gestiona la comunicación. Todos los componentes interactúan a través del API Server.

**Cluster**:: Conjunto de nodos (máquinas) controlados por Kubernetes para ejecutar cargas de trabajo. Mínimo requiere un nodo master y al menos un nodo worker.

**ConfigMap**:: Recurso Kubernetes para almacenar configuración no sensible (variables de entorno, archivos). Los datos están en texto plano.

**Container Runtime**:: Software que ejecuta contenedores (Docker, containerd, CRI-O). Kubernetes es agnóstico al runtime.

**Control Plane (Master)**:: Conjunto de componentes que toman decisiones sobre el cluster (API Server, Scheduler, Controller Manager, etcd).

**CRD (Custom Resource Definition)**:: Extensión que permite definir tipos de recursos personalizados en Kubernetes.

**Deployment**:: Controlador que gestiona Pods con replicas, actualizaciones y rollbacks automáticos.

**DaemonSet**:: Controlador que asegura que un Pod se ejecute en cada nodo del cluster.

**Etcd**:: Base de datos distribuida que almacena toda la información de estado del cluster Kubernetes.

**Headless Service**:: Service sin IP de cluster (clusterIP: None) que retorna IPs específicas de Pods para aplicaciones que requieren identidades estables.

**Health Check**:: Verificación automatizada del estado de un Pod (liveness probe, readiness probe).

**Helm**:: Gestor de paquetes para Kubernetes que simplifica la instalación y configuración de aplicaciones complejas.

**Image**:: Plantilla inmutable de un contenedor que contiene código, runtime, librerías y dependencias.

**Ingress**:: Recurso que gestiona acceso HTTP/HTTPS externo a servicios dentro del cluster.

**Job**:: Controlador para ejecutar Pods una sola vez hasta completarse (para trabajos batch).

**Kubelet**:: Agente que corre en cada nodo y asegura que los Pods estén corriendo según la especificación.

**Kubectl**:: Herramienta CLI para interactuar con clusters Kubernetes.

**Label**:: Par clave-valor que se asigna a recursos para identificación y selección.

**Namespace**:: Mecanismo de aislamiento lógico para dividir un cluster en múltiples entornos virtuales.

**Node**:: Máquina (física o virtual) que forma parte del cluster Kubernetes.

**Operator**:: Controlador personalizado que automatiza procesos operacionales complejos específicos de aplicaciones.

**Persistent Volume (PV)**:: Almacenamiento de cluster que persiste independientemente de los Pods.

**Persistent Volume Claim (PVC)**:: Solicitud de almacenamiento por parte de un Pod.

**Pod**:: Unidad desplegable más pequeña en Kubernetes, generalmente contiene un contenedor (puede tener varios).

**Probe**:: Verificación periódica del estado de un contenedor (liveness, readiness, startup).

**ReplicaSet**:: Controlador que mantiene un número específico de replicas de un Pod en ejecución.

**Secret**:: Recurso para almacenar datos sensibles (contraseñas, tokens, certificados) codificados en base64.

**Service**:: Abstracción que expone un conjunto de Pods como servicio de red con un nombre DNS.

**Service Mesh**:: Infraestructura de red dedicada que gestiona comunicación entre servicios (Istio, Linkerd).

**StatefulSet**:: Controlador para Pods con identidad estable, nombres predecibles y almacenamiento persistente.

**Storage Class**:: Define clases de almacenamiento disponibles para provisioning dinámico de PVs.

**Taint**:: Marca en un nodo que repele Pods a menos que tengan una toleración correspondiente.

**Tolerations**:: Especificación que permite a un Pod ser programado en nodos con taints específicos.

**Volume**:: Almacenamiento adjunto a un Pod, puede ser efímero o persistente.

==== Acrónimos Comunes

[cols="1,2", options="header"]
|===
|Acrónimo|Significado
|API|Application Programming Interface
|CNCF|Cloud Native Computing Foundation
|CRD|Custom Resource Definition
|ETCD|Distributed Reliable Key-Value Store
|HA|High Availability (Alta Disponibilidad)
|HPA|Horizontal Pod Autoscaler
|RBAC|Role-Based Access Control
|RPS|Requests Per Second
|SLA|Service Level Agreement
|SLI|Service Level Indicator
|SLO|Service Level Objective
|VPA|Vertical Pod Autoscaler
|QoS|Quality of Service
|DaemonSet|Service ejecutado en todos los nodos
|CNI|Container Network Interface
|CSI|Container Storage Interface
|YAML|YAML Ain't Markup Language
|JSON|JavaScript Object Notation
|RFC|Request for Comments
|CR|Custom Resource
|CRI|Container Runtime Interface
|===

==== Conceptos Fundamentales

**Declarativo vs Imperativo**::
- *Imperativo*: Comando directo (kubectl run, kubectl create)
- *Declarativo*: Describir estado deseado en YAML (kubectl apply)
- Kubernetes prefiere enfoque declarativo para reproducibilidad

**Eventual Consistency**::
El estado eventual del cluster coincidirá con el deseado, pero puede tomar tiempo. El Reconciliation Loop verifica y corrige constantemente.

**Idempotencia**::
Aplicar la misma configuración múltiples veces produce el mismo resultado final (seguro aplicar varias veces).

**Reconciliation**::
Proceso continuo donde Kubernetes compara estado deseado vs actual y toma acciones para alinearlos.

**Selectors**::
Mecanismo para seleccionar recursos por labels. Ejemplo: `app=nginx,tier=frontend`

**Affinity**::
Reglas para preferir o requerir que Pods se ejecuten en nodos específicos.

**Pod Disruption Budget (PDB)**::
Especifica cuántos Pods pueden ser interrumpidos simultáneamente durante maintenance.

**Quality of Service (QoS)**::
- *Guaranteed*: Requests = Limits (máxima prioridad)
- *Burstable*: Requests < Limits (prioridad media)
- *BestEffort*: Sin requests/limits (baja prioridad)

**Graceful Shutdown**::
Tiempo permitido para que un Pod finalice transacciones antes de ser forzadamente terminado.

**Blue-Green Deployment**::
Estrategia donde se mantienen dos ambientes idénticos y se cambia tráfico de uno a otro.

**Canary Deployment**::
Desplegar cambios gradualmente a un pequeño porcentaje de usuarios primero.

**Rolling Update**::
Actualizar Pods gradualmente, reemplazando instancias antiguas por nuevas.

=== D. Recursos Adicionales

==== Documentación Oficial

**Kubernetes.io (Fuente Oficial)**::
- *Documentación*: https://kubernetes.io/docs/
- *Conceptos*: https://kubernetes.io/docs/concepts/
- *Tareas*: https://kubernetes.io/docs/tasks/
- *Referencias*: https://kubernetes.io/docs/reference/
- *API oficial*: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/

**API Documentation**::
- Documentación interactiva de API: `kubectl api-resources`
- Explicación de campos: `kubectl explain deployment.spec`
- Referencia JSON: https://kubernetes.io/docs/reference/

**CNCF (Cloud Native Computing Foundation)**::
- Sitio oficial: https://www.cncf.io/
- Proyectos incubados: https://www.cncf.io/projects/
- Landscape: https://landscape.cncf.io/

==== Libros Recomendados

**Kubernetes Essentials**
- Autores: Thaha Kamarudin
- Cobertura: Conceptos fundamentales, instalación, despliegue
- Nivel: Principiante a Intermedio

**Kubernetes in Action (2ª Edición)**
- Autores: Marko Lukša
- Cobertura: Comprehensive guide con ejemplos prácticos
- Nivel: Intermedio a Avanzado

**The Kubernetes Book**
- Autores: Nigel Poulton
- Cobertura: Desde lo básico hasta temas avanzados
- Nivel: Todos los niveles

**Cloud Native DevOps with Kubernetes**
- Autores: John Arundel, Justin Domingus
- Cobertura: DevOps practices en Kubernetes
- Nivel: Intermedio a Avanzado

**Kubernetes Security**
- Autores: Liz Rice, Michael Hausenblas
- Cobertura: Security best practices
- Nivel: Avanzado

**Kubernetes Patterns**
- Autores: Bilgin Ibryam, Roland Huss
- Cobertura: Patrones de diseño y arquitectura
- Nivel: Intermedio a Avanzado

==== Plataformas de Aprendizaje Online

**Linux Academy / A Cloud Guru**::
- Cursos interactivos en video
- Labs prácticos
- Certificación CKA/CKAD

**Udemy**::
- Cursos variados sobre Kubernetes
- Precios accesibles
- Certificados completados

**Pluralsight**::
- Cursos profesionales estructurados
- Rutas de aprendizaje por nivel
- Laboratorios prácticos

**KodeKloud**::
- Especialización en Kubernetes
- Labs prácticos con retroalimentación
- Preparación para CKA/CKAD/CKS

**Linux Foundation**::
- Cursos oficiales LF (LFS258, LFS259)
- Exámenes oficiales CKA/CKAD/CKS
- Entrenamientos patrocinados

==== Comunidades y Foros

**Kubernetes Community**::
- Slack oficial: https://slack.k8s.io/
- Canales por tema: #kubernetes-users, #kubernetes-dev
- GitHub discussions: https://github.com/kubernetes/kubernetes

**Stack Overflow**::
- Tag `kubernetes`: https://stackoverflow.com/questions/tagged/kubernetes
- Miles de preguntas respondidas
- Búsqueda rápida de soluciones

**Reddit**::
- r/kubernetes: Comunidad principal
- r/devops: Includes Kubernetes discussions
- Preguntas casuales bienvenidas

**Discord Servers**::
- Varios servidores comunitarios
- Canales de #help-general
- Networking con profesionales

**CNCF Events**::
- KubeCon (conferencia principal)
- CloudNativeCon (diferentes regiones)
- Meetups locales
- Webinars gratuitos

==== Blogs y Sitios de Referencia

**Kubernetes Blog Oficial**::
https://kubernetes.io/blog/ - Noticias, tutoriales, releases

**Blogs de Expertos**::
- *Kelsey Hightower*: https://kelseyhightower.com/
- *Jérôme Petazzoni*: https://jpetazzo.github.io/
- *Jessie Frazelle*: Blog sobre containers y K8s
- *Cilium Blog*: Networking y security
- *CoreOS Blog*: Container best practices

**Sitios de Referencia**::
- *DevOps.com*: Articulos sobre DevOps y Kubernetes
- *InfoQ*: Entrevistas y reportes sobre tecnología
- *DZone*: Tutoriales y guías técnicas
- *Linuxize*: Guías paso a paso

**YouTube Channels**::
- *Kubernetes*: Canal oficial
- *Linux Academy*: Tutoriales completos
- *TechWorld with Nana*: Tutoriales prácticos
- *Mumshad Mannambeth*: KodeKloud tutoriales

==== Herramientas Útiles

**CLI Tools**::
- `kubectl`: CLI oficial
- `kustomize`: Customización de manifiestos
- `helm`: Package manager
- `kubectx`: Cambiar contextos rápidamente
- `kubens`: Cambiar namespaces rápidamente
- `kubectl-neat`: Limpiar metadata innecesaria

**Debugging**::
- `kubectl-debug`: Debugging avanzado
- `k9s`: Terminal UI para Kubernetes
- `stern`: Multiline log aggregator
- `kubetail`: Tail logs de múltiples pods

**Security**::
- `kubesec`: Análisis de seguridad de manifiestos
- `polaris`: Auditoría de buenas prácticas
- `falco`: Runtime security monitoring
- `kubewarden`: Policy engine

**Monitoring & Observability**::
- `prometheus`: Monitoring (CNCF graduated)
- `grafana`: Visualization
- `loki`: Log aggregation
- `jaeger`: Distributed tracing
- `datadog`: Observability platform

**CI/CD Integration**::
- `GitLab CI`: Pipeline nativo
- `GitHub Actions`: Automatización en GitHub
- `ArgoCD`: GitOps deployment
- `FluxCD`: Declarative GitOps

==== Guías Rápidas (Cheat Sheets)

**kubectl Cheat Sheet**::
https://kubernetes.io/docs/reference/kubectl/cheatsheet/

**YAML Cheat Sheet**::
Disponible en múltiples sitios: yamllint.com, yaml-validator.com

**Kubernetes Architecture Cheat Sheet**::
Diagramas de arquitectura y componentes

**Common Commands Quick Reference**::
----
# Crear recursos
kubectl create -f manifest.yaml
kubectl apply -f manifest.yaml

# Ver recursos
kubectl get pods
kubectl describe pod <name>

# Debugging
kubectl logs <pod>
kubectl exec -it <pod> -- /bin/bash

# Actualizar
kubectl set image deployment/<name> <container>=<image>
kubectl rollout status deployment/<name>

# Eliminar
kubectl delete pod <name>
kubectl delete -f manifest.yaml
----

==== Certificaciones Kubernetes

*Ver sección "E. Certificaciones Kubernetes"*

==== Canales de Noticias

**Docker Official Blog**::
https://www.docker.com/blog/ - Noticias relacionadas con containers

**Hacker News**::
https://news.ycombinator.com/ - Comunidad tech

**Product Hunt**::
https://www.producthunt.com/ - Nuevas herramientas y proyectos

**Twitter/X**::
Seguir hashtags: #kubernetes #k8s #devops #cloudnative

=== E. Certificaciones Kubernetes

Las certificaciones Kubernetes de la Linux Foundation son reconocidas internacionalmente y validan expertise en la plataforma.

==== CKA (Certified Kubernetes Administrator)

**Resumen**::
Certificación para administradores de Kubernetes que valida la capacidad de diseñar, instalar, configurar y administrar clusters Kubernetes en producción.

**Tópicos cubiertos:**

* Cluster Architecture (30%)
  - Cluster management
  - High availability
  - Control plane components
  - Etcd

* Installation, Configuration & Validation (25%)
  - Descargar, instalar y configurar componentes
  - Provisionamiento seguro

* Workloads & Scheduling (15%)
  - Deployments, DaemonSets, StatefulSets
  - Resource limits
  - Scheduling

* Services & Networking (20%)
  - Service types
  - Ingress
  - Network policies

* Storage (10%)
  - PV, PVC
  - Storage classes

**Requisitos de examen:**

[cols="1,1", options="header"]
|===
|Aspecto|Detalle
|Duración|3 horas
|Preguntas|15-20 (prácticas)
|Formato|Examen práctico en CLI
|Passing Score|66%
|Costo|$395 USD
|Validez|3 años
|Herramientas|Solo línea de comandos (vim, cat, etc.)
|===

**Temas importantes a estudiar:**

[source,bash]
----
# Cluster Upgrade
kubectl drain <node>
kubeadm upgrade plan
kubeadm upgrade apply

# Backup & Restore
kubectl get all --all-namespaces -o yaml > backup.yaml
ETCDCTL_API=3 etcdctl snapshot save snapshot.db
ETCDCTL_API=3 etcdctl snapshot restore snapshot.db

# Troubleshooting
kubectl logs <pod>
kubectl describe node <node>
kubectl get events

# RBAC
kubectl create role <role-name> --verb=create --resource=pods
kubectl create rolebinding <binding> --role=<role> --user=<user>

# Network Policy
kubectl apply -f network-policy.yaml

# Resource Limits
kubectl set resources deployment <name> --limits=cpu=1,memory=512Mi
----

**Tiempo de preparación recomendado**:: 2-3 meses con 1-2 horas diarias

**Recursos recomendados**::
- Udemy: CKA course por Mumshad
- KodeKloud: Laboratorios CKA
- Linux Academy: Comprehensive CKA course
- Documentación oficial de Kubernetes
- Practice exams: killer.sh (incluido con el examen)

==== CKAD (Certified Kubernetes Application Developer)

**Resumen**::
Certificación para desarrolladores que valida la capacidad de construir, configurar e implementar aplicaciones en Kubernetes.

**Tópicos cubiertos:**

* Core Concepts (13%)
  - Pods, ReplicaSets, Deployments
  - Namespaces

* Configuration (18%)
  - ConfigMaps
  - Secrets
  - Security Contexts
  - Service Accounts

* Multi-Container Pods (10%)
  - Pod design patterns
  - Init containers
  - Sidecar containers

* Observability (18%)
  - Logs
  - Monitoring
  - Debugging

* Pod Design (20%)
  - Labels and Selectors
  - Deployments and Rollouts
  - Jobs and CronJobs

* Services & Networking (13%)
  - Services
  - Ingress
  - Network policies

* State Persistence (8%)
  - Volumes
  - PV y PVC

**Requisitos de examen:**

[cols="1,1", options="header"]
|===
|Aspecto|Detalle
|Duración|2 horas
|Preguntas|15-20 (prácticas)
|Formato|Examen práctico en CLI
|Passing Score|66%
|Costo|$395 USD
|Validez|3 años
|Herramientas|Solo kubectl y vim
|===

**Temas importantes a estudiar:**

[source,bash]
----
# Crear manifiestos
kubectl create deployment <name> --image=<image> --dry-run=client -o yaml

# Configuración
kubectl create configmap <name> --from-literal=key=value
kubectl create secret generic <name> --from-literal=key=value

# Health checks
kubectl set probe deployment <name> --liveness --initial-delay=30

# Pod design patterns
# - Init containers
# - Sidecar containers
# - Ambassador pattern

# Jobs and CronJobs
kubectl create job <name> --image=<image>
kubectl create cronjob <name> --schedule="*/5 * * * *" --image=<image>

# Rolling updates
kubectl set image deployment/<name> <container>=<image> --record
kubectl rollout undo deployment/<name> --to-revision=1

# Scaling
kubectl scale deployment <name> --replicas=5
----

**Tiempo de preparación recomendado**:: 4-6 semanas con 1-2 horas diarias

**Recursos recomendados**::
- KodeKloud: CKAD course
- Udemy: CKAD course
- Practice labs: kodekloud.com, katacoda.com
- Killer.sh practice exams
- Documentación oficial

==== CKS (Certified Kubernetes Security Specialist)

**Resumen**::
Certificación avanzada para especialistas en seguridad Kubernetes que valida la capacidad de asegurar clusters y aplicaciones Kubernetes.

**Requisito Previo**:: Tener CKA válida (no necesariamente completada, pero requerida para el examen)

**Tópicos cubiertos:**

* Cluster Setup (10%)
  - Secure network policies
  - Ingress with TLS
  - RBAC
  - Etcd encryption

* Cluster Hardening (15%)
  - RBAC
  - Service accounts
  - Admission controllers
  - API server access controls

* System Hardening (15%)
  - Minimize host OS footprint
  - Minimize IAM roles
  - Minimize external access
  - Kernel hardening

* Minimize Microservice Vulnerabilities (20%)
  - Security contexts
  - Pod security policies
  - Network policies
  - Image scanning
  - Secrets management

* Supply Chain Security (20%)
  - Image scanning
  - Image signing
  - Secure supply chain
  - Container registries

* Monitoring, Logging & Runtime Security (20%)
  - Audit logging
  - Falco
  - Runtime security

**Requisitos de examen:**

[cols="1,1", options="header"]
|===
|Aspecto|Detalle
|Duración|2 horas
|Preguntas|15-20 (prácticas)
|Formato|Examen práctico en CLI
|Passing Score|66%
|Costo|$395 USD
|Validez|3 años
|Prerequisito|CKA requerida
|===

**Temas importantes a estudiar:**

[source,bash]
----
# RBAC
kubectl create role developer --verb=create,list,get --resource=pods
kubectl create rolebinding dev-binding --role=developer --user=user@example.com

# Network Policies
# Deny all, then allow specific

# Pod Security Policies
kubectl apply -f restricted-psp.yaml

# Security Contexts
securityContext:
  runAsUser: 1000
  runAsNonRoot: true
  readOnlyRootFilesystem: true

# Admission controllers
- PodSecurityPolicy
- ResourceQuota
- LimitRanger

# ETCD encryption
--encryption-provider-config=/etc/kubernetes/encryption.yaml

# Audit logging
--audit-log-path=/var/log/audit.log
--audit-policy-file=/etc/kubernetes/audit-policy.yaml

# Falco runtime security
helm install falco falcosecurity/falco

# Image scanning
trivy image myimage:latest
aqua trivy scan
----

**Tiempo de preparación recomendado**:: 3-4 meses (después de CKA)

**Recursos recomendados**::
- KodeKloud: CKS course
- Udemy: CKS course
- Killer.sh practice exams
- Linux Foundation: LFS260 (oficial)
- Falco documentation
- OWASP security guidelines

==== Estrategia de Preparación

**Orden recomendado de certificaciones:**
1. Comenzar con CKAD (más accesible)
2. Luego CKA (más administrativo)
3. Finalmente CKS (más avanzado, requiere CKA)

**Plan de estudio genérico (2-3 meses):**

**Semana 1-2: Conceptos Fundamentales**
- Entender arquitectura de Kubernetes
- Estudiar componentes principales
- Practicar comandos básicos de kubectl

**Semana 3-4: Recursos Principales**
- Pods, Deployments, StatefulSets
- Services, Ingress
- ConfigMaps, Secrets

**Semana 5-6: Tópicos Avanzados**
- Storage, Networking
- RBAC, Security
- Monitoring, Logging

**Semana 7-8: Práctica Intensiva**
- Resolver labs
- Tomar exámenes de práctica
- Enfocarse en áreas débiles

**Semana 9-10: Preparación Final**
- Exámenes de simulación
- Revisar commands rápidos
- Descansar antes del examen

**Tips para el examen:**

- Leer la pregunta cuidadosamente
- Usar `--dry-run=client -o yaml` para generar manifiestos
- Practicar con vim/nano antes del examen
- Usar bash aliases para comandos comunes
- No olvidar cambiar de contexto/namespace si es necesario
- Guardar tiempo para preguntas difíciles
- Verificar nombres de recursos y labels

**Recursos para práctica:**

[cols="1,2", options="header"]
|===
|Recurso|URL/Descripción
|killer.sh|Simulador oficial incluido con examen
|KodeKloud|https://kodekloud.com (labs interactivos)
|Katacoda|Escenarios de Kubernetes (ahora parte de O'Reilly)
|Play with Kubernetes|https://labs.play-with-k8s.com
|Linux Academy|Práctica y cursos completos
|Udemy Practice Tests|Exámenes de simulación
|GitHub Repos|Colecciones de práctica
|===

==== Después de la Certificación

**Mantener la certificación actualizada:**
- Las certificaciones son válidas por 3 años
- Renovar tomando el examen nuevamente
- Contribuir a proyectos open source
- Mantener skills con nuevas versiones

**Siguiente paso:**
- Aplicar conocimientos en proyectos reales
- Contribuir a comunidad Kubernetes
- Especialización en tópicos específicos
- Perseguir certificaciones complementarias (Docker, cloud platforms)

**Recursos para continuar aprendiendo:**
- Leer código fuente de Kubernetes
- Contribuir a proyectos CNCF
- Asistir a conferencias (KubeCon)
- Escribir blogs sobre experiencias
- Mentorar a otros estudiantes
