= Ejercicios Prácticos de Kubernetes
:toc:
:numbered:
:source-highlighter: pygments

== Módulo 1: Introducción a Kubernetes

=== Ejercicio 1.1: Crear tu primer Pod con nginx

**Objetivo de aprendizaje:**
Entender cómo crear un Pod simple y interactuar con él usando kubectl. Comprender que un Pod es la unidad mínima desplegable en Kubernetes.

**Descripción:**
En este ejercicio crearás un Pod que ejecuta un servidor nginx. Esto te permitirá entender cómo Kubernetes acepta manifiestos YAML, cómo el API Server procesa la solicitud y cómo kubelet en los nodos ejecuta el contenedor.

**Tareas:**

1. Crea un archivo YAML llamado `nginx-pod.yaml` con el siguiente contenido:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  namespace: default
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    ports:
    - containerPort: 80
      name: http
----

2. Despliega el Pod en el cluster:
+
[source,bash]
----
kubectl apply -f nginx-pod.yaml
----

3. Verifica que el Pod se creó correctamente:
+
[source,bash]
----
kubectl get pods
kubectl describe pod nginx-pod
----

4. Accede a los logs del contenedor:
+
[source,bash]
----
kubectl logs nginx-pod
----

5. Ejecuta un comando dentro del contenedor:
+
[source,bash]
----
kubectl exec -it nginx-pod -- bash
# Dentro del contenedor
curl localhost:80
exit
----

6. Elimina el Pod:
+
[source,bash]
----
kubectl delete pod nginx-pod
----

**Verificación:**
Deberías poder ver el Pod listado con `kubectl get pods`, acceder a su información con `describe`, ver logs y ejecutar comandos dentro del contenedor.

**Preguntas de reflexión:**
1. ¿Qué paso ocurre después de que ejecutas `kubectl apply`?
2. ¿Quién almacena la especificación del Pod que creaste?
3. ¿En qué momento el contenedor nginx comienza a ejecutarse?
4. ¿Qué ocurriría si el Pod muere/falla? ¿Kubernetes lo reinicia automáticamente?

---

=== Ejercicio 1.2: Crear un Deployment con múltiples réplicas

**Objetivo de aprendizaje:**
Entender cómo Kubernetes mantiene el estado deseado usando Deployments y el bucle de reconciliación del Controller Manager.

**Descripción:**
A diferencia de Pods, los Deployments aseguran que siempre haya un número específico de réplicas ejecutándose. Si una réplica falla, el Controller Manager automáticamente crea una nueva. En este ejercicio crearás un Deployment con 3 réplicas de nginx.

**Tareas:**

1. Crea un archivo YAML llamado `nginx-deployment.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
----

2. Despliega el Deployment:
+
[source,bash]
----
kubectl apply -f nginx-deployment.yaml
----

3. Observa cómo se crean los Pods:
+
[source,bash]
----
kubectl get pods
kubectl get pods -w  # Observar cambios en tiempo real
----

4. Visualiza el estado del Deployment:
+
[source,bash]
----
kubectl get deployment
kubectl describe deployment nginx-deployment
----

5. Elimina un Pod manualmente y observa cómo Kubernetes lo recrea:
+
[source,bash]
----
kubectl get pods  # Anota el nombre de un pod
kubectl delete pod <nombre-del-pod>
kubectl get pods -w  # Observa cómo aparece uno nuevo
----

6. Escala el Deployment a 5 réplicas:
+
[source,bash]
----
kubectl scale deployment nginx-deployment --replicas=5
kubectl get pods
----

7. Reduce nuevamente a 3 réplicas:
+
[source,bash]
----
kubectl scale deployment nginx-deployment --replicas=3
----

8. Elimina el Deployment:
+
[source,bash]
----
kubectl delete deployment nginx-deployment
----

**Verificación:**
Deberías ver 3 Pods corriendo. Cuando elimines un Pod, debería aparecer automáticamente uno nuevo. El número de réplicas debería cambiar dinámicamente cuando uses `scale`.

**Preguntas de reflexión:**
1. ¿Cuál es la diferencia entre un Pod y un Deployment?
2. ¿Qué componente se da cuenta de que una réplica está faltando y crea una nueva?
3. ¿Dónde se almacena el número deseado de réplicas (3, 5, etc.)?
4. Si accidentalmente modificas el YAML para que diga 10 réplicas, ¿qué ocurriría?

---

=== Ejercicio 1.3: Crear un Service de tipo ClusterIP para acceder a tus Pods

**Objetivo de aprendizaje:**
Entender cómo los Services actúan como abstracciones estables para acceder a un conjunto de Pods en cambio constante. Comprender cómo kube-proxy implementa las reglas de networking.

**Descripción:**
Los Pods son efímeros; su IP puede cambiar cuando se reinician. Los Services proporcionan una IP estable y un nombre DNS para acceder a un conjunto de Pods. Un Service ClusterIP es accesible solo dentro del cluster.

**Tareas:**

1. Primero, asegúrate de tener un Deployment ejecutándose (reutiliza el de 1.2):
+
[source,bash]
----
kubectl apply -f nginx-deployment.yaml
----

2. Crea un archivo YAML llamado `nginx-service-clusterip.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: ClusterIP
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
----

3. Crea el Service:
+
[source,bash]
----
kubectl apply -f nginx-service-clusterip.yaml
----

4. Inspecciona el Service:
+
[source,bash]
----
kubectl get svc
kubectl describe svc nginx-service
----

5. Obtén la IP del ClusterIP (normalmente en rango 10.x.x.x):
+
[source,bash]
----
kubectl get svc nginx-service -o wide
----

6. Crea un Pod de prueba temporal para acceder al service desde dentro del cluster:
+
[source,bash]
----
kubectl run -it debug-pod --image=busybox --rm -- wget -O- http://nginx-service
----

7. Verifica que DNS resuelve el nombre del service:
+
[source,bash]
----
kubectl run -it debug-pod --image=busybox --rm -- nslookup nginx-service
----

8. Limpia los recursos:
+
[source,bash]
----
kubectl delete -f nginx-service-clusterip.yaml
kubectl delete -f nginx-deployment.yaml
----

**Verificación:**
El Service debería tener una IP estable asignada. Desde un Pod dentro del cluster, deberías poder acceder al service usando tanto la IP como el nombre DNS (nginx-service).

**Preguntas de reflexión:**
1. ¿Cómo sabe el Service a qué Pods enviar tráfico?
2. ¿Qué pasa si eliminas uno de los Pods del Deployment? ¿Sigue funcionando el Service?
3. ¿Qué componente del nodo crea las reglas de iptables que hacen posible que el tráfico llegue a los Pods?
4. ¿Por qué ClusterIP no es accesible desde fuera del cluster?

---

=== Ejercicio 1.4: Crear un Service de tipo NodePort para acceso externo

**Objetivo de aprendizaje:**
Entender cómo exponer un Service fuera del cluster usando NodePort.

**Descripción:**
Mientras que ClusterIP solo es accesible dentro del cluster, NodePort abre un puerto en cada nodo del cluster para que el tráfico externo pueda llegar a tu aplicación.

**Tareas:**

1. Asegúrate de tener un Deployment ejecutándose:
+
[source,bash]
----
kubectl apply -f nginx-deployment.yaml
----

2. Crea un archivo YAML llamado `nginx-service-nodeport.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: nginx-nodeport-service
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30080
----

3. Crea el Service:
+
[source,bash]
----
kubectl apply -f nginx-service-nodeport.yaml
----

4. Verifica el Service:
+
[source,bash]
----
kubectl get svc
kubectl describe svc nginx-nodeport-service
----

5. Obtén la IP de uno de los nodos:
+
[source,bash]
----
kubectl get nodes -o wide
----

6. Accede al servicio desde fuera del cluster:
+
[source,bash]
----
# Si usas Minikube o Docker Desktop
curl http://localhost:30080

# Si usas un cluster en la nube, usa la IP pública del nodo
curl http://<node-ip>:30080
----

7. Limpia:
+
[source,bash]
----
kubectl delete -f nginx-service-nodeport.yaml
kubectl delete -f nginx-deployment.yaml
----

**Verificación:**
Deberías poder acceder a nginx desde tu navegador o con curl en el puerto 30080 desde cualquier máquina que pueda llegar a tu cluster.

**Preguntas de reflexión:**
1. ¿Cuál es la diferencia entre el puerto 80 y el 30080 en la configuración?
2. ¿Qué ocurre si accedes al puerto 30080 de un nodo donde no hay pods de nginx?
3. ¿Por qué Kubernetes necesita abrir un puerto en todos los nodos?
4. ¿Cuál es la ventaja del Service sobre acceder directamente a la IP del Pod?

---

=== Ejercicio 1.5: Entender Resource Requests y Limits (Scheduling)

**Objetivo de aprendizaje:**
Comprender cómo el Scheduler de Kubernetes toma decisiones basándose en los recursos solicitados y disponibles en los nodos.

**Descripción:**
El Scheduler considera los resource requests para decidir en qué nodo colocar un Pod. Sin resource requests adecuados, el Scheduler puede tomar decisiones pobres y sobrecargar nodos. Los limits previenen que un contenedor consuma demasiados recursos.

**Tareas:**

1. Crea un archivo YAML llamado `resource-aware-deployment.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource-aware-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: resource-aware
  template:
    metadata:
      labels:
        app: resource-aware
    spec:
      containers:
      - name: app
        image: nginx:1.21
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        ports:
        - containerPort: 80
----

2. Despliega el Deployment:
+
[source,bash]
----
kubectl apply -f resource-aware-deployment.yaml
----

3. Obtén información sobre los Pods y nodos:
+
[source,bash]
----
kubectl get pods -o wide
kubectl top nodes  # Muestra uso actual de recursos
kubectl top pods   # Muestra uso actual de recursos
----

4. Describe un Pod para ver los recursos asignados:
+
[source,bash]
----
kubectl describe pod <nombre-del-pod>
----

5. Intenta crear un Pod con recursos irrazonablemente altos:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: resource-hungry-pod
spec:
  containers:
  - name: huge
    image: nginx
    resources:
      requests:
        memory: "100Gi"
        cpu: "100"
----

6. Aplica este YAML:
+
[source,bash]
----
kubectl apply -f resource-hungry-pod.yaml
kubectl get pods
kubectl describe pod resource-hungry-pod
----

El Pod debería quedar en estado Pending porque no hay suficientes recursos.

7. Limpia:
+
[source,bash]
----
kubectl delete -f resource-aware-deployment.yaml
kubectl delete -f resource-hungry-pod.yaml
----

**Verificación:**
Los Pods deberían mostrar sus resource requests cuando los describes. El Pod con recursos irrazonables debería quedar Pending.

**Preguntas de reflexión:**
1. ¿Quién toma la decisión de no programar el Pod hambriento en un nodo?
2. ¿Cuál es la diferencia entre requests y limits?
3. ¿Qué ocurre si un contenedor intenta usar más memoria que su límite?
4. ¿Cómo saben los Pods qué recursos están disponibles?

---

=== Ejercicio 1.6: Trabajar con Namespaces y Selectors

**Objetivo de aprendizaje:**
Entender cómo los Namespaces proporcionan aislamiento lógico en un cluster. Comprender cómo los Labels y Selectors agrupan recursos.

**Descripción:**
Los Namespaces son particiones virtuales dentro de un cluster. Los Labels son pares clave-valor que se pueden usar para identificar y seleccionar objetos. Los Selectors usan Labels para filtrar recursos.

**Tareas:**

1. Crea un nuevo namespace:
+
[source,bash]
----
kubectl create namespace my-app
kubectl get namespaces
----

2. Crea un Deployment en el nuevo namespace:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-prod
  namespace: my-app
  labels:
    app: web
    environment: production
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web
      environment: production
  template:
    metadata:
      labels:
        app: web
        environment: production
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
----

3. Aplica el Deployment:
+
[source,bash]
----
kubectl apply -f app-deployment.yaml
----

4. Lista Pods en el namespace específico:
+
[source,bash]
----
kubectl get pods -n my-app
----

5. Crea otro Deployment con labels diferentes en el mismo namespace:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-dev
  namespace: my-app
  labels:
    app: web
    environment: development
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web
      environment: development
  template:
    metadata:
      labels:
        app: web
        environment: development
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
----

6. Aplica el segundo Deployment:
+
[source,bash]
----
kubectl apply -f app-dev-deployment.yaml
----

7. Usa selectors para filtrar Pods:
+
[source,bash]
----
# Todos los pods con label app=web
kubectl get pods -n my-app -l app=web

# Solo pods de producción
kubectl get pods -n my-app -l environment=production

# Pods que NO son development
kubectl get pods -n my-app -l environment!=development
----

8. Describe ambos deployments:
+
[source,bash]
----
kubectl describe deployment app-prod -n my-app
kubectl describe deployment app-dev -n my-app
----

9. Limpia:
+
[source,bash]
----
kubectl delete namespace my-app
----

**Verificación:**
Deberías ver dos Pods en el namespace `my-app`, con selectors que te permitan filtrar por labels.

**Preguntas de reflexión:**
1. ¿Cómo sabe el Deployment qué Pods debe gestionar?
2. ¿Qué ocurriría si tienes dos Deployments con el mismo selector en el mismo namespace?
3. ¿Cuál es la ventaja de usar Labels respecto a los nombres de los Pods?
4. ¿Puedes acceder a Pods en otros namespaces desde el mismo cluster?

---

=== Ejercicio 1.7: Implementar Health Checks (Liveness y Readiness Probes)

**Objetivo de aprendizaje:**
Entender cómo kubelet monitorea la salud de los contenedores y mantiene la aplicación disponible mediante probes.

**Descripción:**
Los Liveness Probes detectan si un contenedor está vivo pero bloqueado (para reiniciarlo). Los Readiness Probes detectan si la aplicación está lista para recibir tráfico. Estos son críticos para mantener la alta disponibilidad.

**Tareas:**

1. Crea un archivo YAML llamado `health-check-deployment.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: health-check-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: health-app
  template:
    metadata:
      labels:
        app: health-app
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
----

2. Despliega el Deployment:
+
[source,bash]
----
kubectl apply -f health-check-deployment.yaml
----

3. Observa los Pods:
+
[source,bash]
----
kubectl get pods
----

4. Describe un Pod para ver información sobre los probes:
+
[source,bash]
----
kubectl describe pod <nombre-del-pod>
----

5. Accede a los logs del contenedor para ver las pruebas de health:
+
[source,bash]
----
kubectl logs <nombre-del-pod>
----

6. Crea un Deployment con un Probe fallido para ver qué ocurre:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: failing-health-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: failing-app
  template:
    metadata:
      labels:
        app: failing-app
    spec:
      containers:
      - name: app
        image: busybox
        command: ["sh", "-c", "echo 'App is running' && sleep 30000"]
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - "exit 1"  # Esto siempre falla
          initialDelaySeconds: 5
          periodSeconds: 5
          failureThreshold: 3
----

7. Aplica este Deployment:
+
[source,bash]
----
kubectl apply -f failing-health-app.yaml
kubectl get pods -w  # Observa cómo se reinicia constantemente
----

8. Limpia:
+
[source,bash]
----
kubectl delete -f health-check-deployment.yaml
kubectl delete -f failing-health-app.yaml
----

**Verificación:**
El primer Deployment debería tener Pods Running y Ready. El segundo debería mostrar múltiples reiniciaciones.

**Preguntas de reflexión:**
1. ¿Cuál es la diferencia entre Liveness y Readiness?
2. ¿Qué componente ejecuta los probes?
3. ¿Qué ocurre si un Pod no pasa el Readiness Probe pero sí el Liveness Probe?
4. ¿Cómo afectan los probes al balanceo de carga del Service?

---

=== Ejercicio 1.8: Usar ConfigMaps para externalizar configuración

**Objetivo de aprendizaje:**
Entender cómo separar la configuración del código usando ConfigMaps. Aprender sobre la gestión de configuración en Kubernetes.

**Descripción:**
ConfigMaps almacenan datos de configuración en pares clave-valor. Esto permite cambiar la configuración sin reconstruir imágenes de contenedor.

**Tareas:**

1. Crea un ConfigMap usando kubectl:
+
[source,bash]
----
kubectl create configmap app-config \
  --from-literal=APP_NAME=MyWebApp \
  --from-literal=LOG_LEVEL=info \
  --from-literal=MAX_CONNECTIONS=100
----

2. Verifica el ConfigMap:
+
[source,bash]
----
kubectl get configmap
kubectl describe configmap app-config
kubectl get configmap app-config -o yaml
----

3. Crea un Deployment que use el ConfigMap:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: configmap-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: configmap-app
  template:
    metadata:
      labels:
        app: configmap-app
    spec:
      containers:
      - name: app
        image: busybox
        command: ["sh", "-c", "env | grep APP && sleep 30000"]
        env:
        - name: APP_NAME
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: APP_NAME
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: LOG_LEVEL
        - name: MAX_CONNECTIONS
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: MAX_CONNECTIONS
----

4. Aplica el Deployment:
+
[source,bash]
----
kubectl apply -f configmap-deployment.yaml
----

5. Verifica las variables de entorno:
+
[source,bash]
----
kubectl logs <nombre-del-pod>
----

6. Crea un ConfigMap a partir de un archivo:
+
[source,bash]
----
# Crea un archivo de configuración
cat > app.properties << EOF
database.host=localhost
database.port=5432
cache.enabled=true
EOF

# Crea el ConfigMap
kubectl create configmap app-properties --from-file=app.properties

# Verifica
kubectl get configmap app-properties -o yaml
----

7. Limpia:
+
[source,bash]
----
kubectl delete deployment configmap-demo
kubectl delete configmap app-config
kubectl delete configmap app-properties
rm app.properties
----

**Verificación:**
Los Pods deberían tener las variables de entorno establecidas desde el ConfigMap.

**Preguntas de reflexión:**
1. ¿Dónde se almacena la información del ConfigMap?
2. ¿Cuándo reciben los Pods la configuración del ConfigMap?
3. ¿Qué ocurre si cambias el ConfigMap después de que el Pod está ejecutándose?
4. ¿Cómo podrías usar ConfigMaps para montar archivos de configuración?

---

=== Ejercicio 1.9: Usar Secrets para almacenar datos sensibles

**Objetivo de aprendizaje:**
Entender cómo Kubernetes almacena datos sensibles como contraseñas y tokens usando Secrets.

**Descripción:**
Secrets almacenan datos sensibles de forma segura (aunque por defecto codificados en base64, no encriptados). Se usan para credenciales de base de datos, tokens API, etc.

**Tareas:**

1. Crea un Secret usando kubectl:
+
[source,bash]
----
kubectl create secret generic db-credentials \
  --from-literal=username=admin \
  --from-literal=password=supersecretpassword
----

2. Verifica el Secret:
+
[source,bash]
----
kubectl get secret
kubectl describe secret db-credentials
kubectl get secret db-credentials -o yaml
----

3. Crea un Deployment que use el Secret:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: secret-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: secret-app
  template:
    metadata:
      labels:
        app: secret-app
    spec:
      containers:
      - name: app
        image: busybox
        command: ["sh", "-c", "echo 'DB User: $DB_USER' && echo 'DB Pass: $DB_PASS' && sleep 30000"]
        env:
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: username
        - name: DB_PASS
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: password
----

4. Aplica el Deployment:
+
[source,bash]
----
kubectl apply -f secret-deployment.yaml
----

5. Verifica que el Pod tenga acceso a las credenciales:
+
[source,bash]
----
kubectl logs <nombre-del-pod>
----

6. Crea un Secret a partir de archivos:
+
[source,bash]
----
# Crea certificados de ejemplo
echo "-----BEGIN CERTIFICATE-----" > cert.pem
echo "MIIDXTCCAkWgAwIBAgIJAJ..." >> cert.pem
echo "-----END CERTIFICATE-----" >> cert.pem

# Crea el Secret
kubectl create secret tls tls-secret --cert=cert.pem --key=key.pem

# Verifica
kubectl get secret tls-secret -o yaml
----

7. Limpia:
+
[source,bash]
----
kubectl delete deployment secret-demo
kubectl delete secret db-credentials
kubectl delete secret tls-secret
rm cert.pem key.pem
----

**Verificación:**
Los Pods deberían tener acceso a los datos del Secret a través de variables de entorno.

**Preguntas de reflexión:**
1. ¿Dónde se almacenan los Secrets en el cluster?
2. ¿Son verdaderamente seguros los Secrets por defecto?
3. ¿Cómo se transmiten los Secrets desde el API Server a los Pods?
4. ¿Qué tipos de Secrets existen en Kubernetes?

---

=== Ejercicio 1.10: Realizar Rolling Updates con Deployment

**Objetivo de aprendizaje:**
Entender cómo Kubernetes actualiza aplicaciones sin downtime usando Rolling Updates. Comprender cómo el Deployment Controller gestiona estas actualizaciones.

**Descripción:**
Los Rolling Updates son el mecanismo que usa Kubernetes para actualizar aplicaciones manteniendo disponibilidad. Los Pods se sustituyen gradualmente con la nueva versión.

**Tareas:**

1. Crea un Deployment inicial con nginx 1.20:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rolling-update-demo
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: rolling-app
  template:
    metadata:
      labels:
        app: rolling-app
      annotations:
        version: "1.20"
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        ports:
        - containerPort: 80
----

2. Despliega el Deployment:
+
[source,bash]
----
kubectl apply -f rolling-update-deployment.yaml
kubectl get pods
----

3. Verifica la versión actual:
+
[source,bash]
----
kubectl describe deployment rolling-update-demo
----

4. Actualiza la imagen a nginx 1.21:
+
[source,bash]
----
kubectl set image deployment/rolling-update-demo nginx=nginx:1.21
----

5. Observa el progreso de la actualización en tiempo real:
+
[source,bash]
----
kubectl get pods -w
----

6. Verifica los ReplicaSets creados:
+
[source,bash]
----
kubectl get rs
kubectl describe rs <nombre-del-rs>
----

7. Revisa el historial de cambios:
+
[source,bash]
----
kubectl rollout history deployment rolling-update-demo
----

8. Si algo sale mal, haz rollback a la versión anterior:
+
[source,bash]
----
kubectl rollout undo deployment rolling-update-demo
kubectl get pods -w  # Observa cómo vuelve a nginx 1.20
----

9. Redeploy a nginx 1.21 nuevamente:
+
[source,bash]
----
kubectl set image deployment/rolling-update-demo nginx=nginx:1.21
----

10. Prueba con una actualización pausada:
+
[source,bash]
----
kubectl rollout pause deployment rolling-update-demo
kubectl get pods  # Algunos estarán en 1.21, otros en 1.20
----

11. Resume la actualización:
+
[source,bash]
----
kubectl rollout resume deployment rolling-update-demo
kubectl get pods -w
----

12. Limpia:
+
[source,bash]
----
kubectl delete deployment rolling-update-demo
----

**Verificación:**
Deberías ver Pods siendo reemplazados gradualmente sin que el servicio se interrumpa. El historial de cambios debería mostrar múltiples revisiones.

**Preguntas de reflexión:**
1. ¿Qué ocurre con los Pods antiguos cuando actualizas la imagen?
2. ¿Quién gestiona el proceso de Rolling Update?
3. ¿Qué significa `maxSurge` y `maxUnavailable`?
4. ¿Cómo sabes si la actualización es segura antes de completarla?
5. ¿Cómo se relaciona el Deployment Controller con los ReplicaSets?
6. ¿Cuántas versiones anteriores se mantienen en el historial?

---

== Resumen de Conceptos

Estos ejercicios cubren los aspectos fundamentales del Módulo 1:

* **Ejercicio 1.1**: Uso básico de kubectl y el flujo Pod → API Server → kubelet → Container Runtime
* **Ejercicio 1.2**: Deployments y el bucle de reconciliación del Controller Manager
* **Ejercicio 1.3**: Services ClusterIP y kube-proxy
* **Ejercicio 1.4**: Services NodePort y acceso externo
* **Ejercicio 1.5**: Scheduler y resource requests/limits
* **Ejercicio 1.6**: Namespaces, Labels y Selectors
* **Ejercicio 1.7**: kubelet health checks (Liveness/Readiness)
* **Ejercicio 1.8**: ConfigMaps para gestión de configuración
* **Ejercicio 1.9**: Secrets para datos sensibles
* **Ejercicio 1.10**: Rolling Updates y Deployment Controller

== Módulo 2: Trabajando con Pods

=== Ejercicio 2.1: Pod multi-contenedor con patrón Sidecar (Logging)

**Objetivo de aprendizaje:**
Entender cómo ejecutar múltiples contenedores en un mismo Pod compartiendo volúmenes. Implementar el patrón Sidecar para logging.

**Descripción:**
Crearás un Pod con dos contenedores: uno principal (nginx) que escribe logs en un volumen compartido, y un sidecar que lee esos logs y los procesa. Esto demuestra cómo los contenedores en un Pod comparten el ciclo de vida y los volúmenes.

**Tareas:**

1. Crea un archivo YAML llamado `sidecar-logging-pod.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: nginx-with-logging-sidecar
  labels:
    app: webserver
spec:
  # Volumen compartido entre contenedores
  volumes:
  - name: shared-logs
    emptyDir: {}

  containers:
  # Contenedor principal - servidor web
  - name: nginx
    image: nginx:1.21
    ports:
    - containerPort: 80
    # Monta el volumen compartido
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
    # Configuración de nginx para escribir en el volumen
    command:
    - sh
    - -c
    - |
      cat > /etc/nginx/nginx.conf <<'EOF'
      user nginx;
      worker_processes auto;
      error_log /var/log/nginx/error.log;
      pid /var/run/nginx.pid;
      events { worker_connections 1024; }
      http {
        access_log /var/log/nginx/access.log;
        server {
          listen 80;
          location / { return 200 "OK\n"; }
        }
      }
      EOF
      nginx -g 'daemon off;'

  # Sidecar - procesador de logs
  - name: log-processor
    image: busybox:1.28
    # Lee logs y los muestra continuamente
    command:
    - sh
    - -c
    - |
      while true; do
        if [ -f /var/log/nginx/access.log ]; then
          echo "=== Access logs at $(date) ==="
          tail -5 /var/log/nginx/access.log
        fi
        sleep 5
      done
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
      readOnly: true
----

2. Despliega el Pod:
+
[source,bash]
----
kubectl apply -f sidecar-logging-pod.yaml
----

3. Verifica que ambos contenedores estén corriendo:
+
[source,bash]
----
kubectl get pod nginx-with-logging-sidecar
kubectl describe pod nginx-with-logging-sidecar
----

4. Accede al contenedor nginx y genera tráfico:
+
[source,bash]
----
kubectl exec -it nginx-with-logging-sidecar -c nginx -- bash
# Dentro del contenedor
curl localhost
exit
----

5. Verifica los logs del sidecar:
+
[source,bash]
----
kubectl logs nginx-with-logging-sidecar -c log-processor
----

6. Ver logs de ambos contenedores por separado:
+
[source,bash]
----
# Logs del contenedor principal
kubectl logs nginx-with-logging-sidecar -c nginx

# Logs del sidecar
kubectl logs nginx-with-logging-sidecar -c log-processor
----

7. Ver todos los logs del Pod:
+
[source,bash]
----
kubectl logs nginx-with-logging-sidecar
----

8. Elimina el Pod:
+
[source,bash]
----
kubectl delete pod nginx-with-logging-sidecar
----

**Verificación:**
El sidecar debería estar leyendo los logs generados por nginx desde el volumen compartido.

**Preguntas de reflexión:**
1. ¿Qué ocurre si uno de los contenedores falla?
2. ¿Cómo se comunican los contenedores en un Pod?
3. ¿Por qué no usar un Pod por contenedor?
4. ¿Qué ventajas tiene compartir un volumen vs usar una API de red?

---

=== Ejercicio 2.2: Pod con Init Containers

**Objetivo de aprendizaje:**
Entender cómo los Init Containers ejecutan antes del contenedor principal y preparan el entorno.

**Descripción:**
Los Init Containers se ejecutan una sola vez al inicio del Pod, antes de que los contenedores principales comiencen. Se usan para preparar datos, esperar a servicios, etc.

**Tareas:**

1. Crea un archivo YAML llamado `init-container-pod.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-init-container
  labels:
    app: app-with-init
spec:
  volumes:
  - name: shared-data
    emptyDir: {}

  # Init Containers - se ejecutan antes que los contenedores principales
  initContainers:
  - name: init-setup
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      echo "Preparando ambiente..."
      echo "App initialized at $(date)" > /shared-data/init.log
      echo "Creando archivo de configuración..."
      cat > /shared-data/config.txt <<'EOF'
      APP_NAME=MyApp
      VERSION=1.0
      ENVIRONMENT=production
      EOF
      echo "Init completado!"
    volumeMounts:
    - name: shared-data
      mountPath: /shared-data

  # Contenedor principal - se ejecuta después de que init containers terminen
  containers:
  - name: main-app
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      echo "=== App principal iniciando ==="
      sleep 2
      echo "=== Contenido del archivo de init ==="
      cat /shared-data/init.log
      echo "=== Contenido de config ==="
      cat /shared-data/config.txt
      echo "=== App ejecutando ==="
      sleep 30000
    volumeMounts:
    - name: shared-data
      mountPath: /shared-data
      readOnly: true

  restartPolicy: Never
----

2. Despliega el Pod:
+
[source,bash]
----
kubectl apply -f init-container-pod.yaml
----

3. Observa el Pod - inicialmente estará en Pending mientras el init container se ejecuta:
+
[source,bash]
----
kubectl get pods
kubectl describe pod pod-with-init-container
----

4. Cuando el init container termine, el contenedor principal iniciará:
+
[source,bash]
----
kubectl get pods -w
----

5. Verifica los logs del init container:
+
[source,bash]
----
# Los logs de init containers no aparecen en "kubectl logs"
# Deben verse en describe
kubectl describe pod pod-with-init-container | grep -A 20 "Init Containers"
----

6. Verifica los logs del contenedor principal:
+
[source,bash]
----
kubectl logs pod-with-init-container
----

7. Accede al contenedor y verifica los archivos:
+
[source,bash]
----
kubectl exec -it pod-with-init-container -- cat /shared-data/config.txt
----

8. Limpia:
+
[source,bash]
----
kubectl delete pod pod-with-init-container
----

**Verificación:**
El contenedor principal debería ejecutarse solo después de que el init container haya completado exitosamente.

**Preguntas de reflexión:**
1. ¿Qué ocurre si el init container falla?
2. ¿Cómo se diferencia un init container de un sidecar?
3. ¿En qué casos usarías init containers?
4. ¿Pueden haber múltiples init containers?

---

=== Ejercicio 2.3: Pod con patrón Ambassador (Proxy)

**Objetivo de aprendizaje:**
Implementar el patrón Ambassador para que el contenedor principal se comunique con servicios externos a través de un proxy.

**Descripción:**
El patrón Ambassador actúa como intermediario. El contenedor principal se conecta a un servicio en localhost, mientras que el ambassador se conecta con el servicio externo real.

**Tareas:**

1. Crea un archivo YAML llamado `ambassador-pod.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-ambassador
  labels:
    app: app-ambassador
spec:
  containers:
  # Contenedor principal - aplicación
  - name: application
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      echo "App iniciada, esperando ambassador..."
      sleep 2
      echo "Conectando a localhost:3000 (ambassador)"
      # Simula un cliente que se conecta al ambassador
      while true; do
        echo "Request desde app" | nc localhost 3000 2>/dev/null || echo "No conectado"
        sleep 10
      done
    ports:
    - containerPort: 8080

  # Ambassador - proxy que expone servicios externos en localhost
  - name: ambassador
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      # El ambassador expone un servicio en localhost:3000
      # Para este ejercicio, usaremos nc para simular un servicio
      echo "Ambassador iniciado, escuchando en puerto 3000"
      while true; do
        echo "Response from ambassador at $(date)" | nc -l -p 3000
      done
    ports:
    - containerPort: 3000
----

2. Despliega el Pod:
+
[source,bash]
----
kubectl apply -f ambassador-pod.yaml
----

3. Verifica el estado del Pod:
+
[source,bash]
----
kubectl get pod app-with-ambassador
kubectl describe pod app-with-ambassador
----

4. Accede al Pod y verifica la comunicación entre contenedores:
+
[source,bash]
----
# Desde el contenedor de aplicación, verifica que pueda alcanzar localhost:3000
kubectl exec -it app-with-ambassador -c application -- \
  sh -c "echo 'Test' | nc localhost 3000"
----

5. Ver logs de ambos contenedores:
+
[source,bash]
----
kubectl logs app-with-ambassador -c application
kubectl logs app-with-ambassador -c ambassador
----

6. Limpia:
+
[source,bash]
----
kubectl delete pod app-with-ambassador
----

**Verificación:**
La aplicación debería poder comunicarse con el ambassador a través de localhost:3000.

**Preguntas de reflexión:**
1. ¿Cuál es la ventaja del ambassador sobre conectar directamente?
2. ¿Por qué ambos contenedores ven localhost de la misma manera?
3. ¿Cómo se diferencia del patrón Sidecar?
4. ¿En qué casos usarías este patrón?

---

=== Ejercicio 2.4: Debugging de Pods - Logs, Exec y Port Forward

**Objetivo de aprendizaje:**
Dominar las técnicas de debugging de Pods usando logs, exec, port-forward y describe.

**Descripción:**
Aprenderás a investigar y solucionar problemas en Pods usando herramientas de debugging incorporadas en kubectl.

**Tareas:**

1. Crea un Pod con algunos problemas para debuggear (`debug-pod.yaml`):
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: debug-demo
  labels:
    app: debug-app
  annotations:
    description: "Pod para practicar debugging"
spec:
  containers:
  - name: web-server
    image: nginx:1.21
    ports:
    - containerPort: 80
      name: http
    - containerPort: 443
      name: https
    resources:
      requests:
        memory: "64Mi"
        cpu: "100m"
      limits:
        memory: "256Mi"
        cpu: "500m"
    livenessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 10
      periodSeconds: 10
    readinessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 5
    env:
    - name: NGINX_HOST
      value: "localhost"
    - name: NGINX_PORT
      value: "80"
----

2. Despliega el Pod:
+
[source,bash]
----
kubectl apply -f debug-pod.yaml
----

3. Obtén información general del Pod:
+
[source,bash]
----
# Lista simple
kubectl get pod debug-demo

# Lista con más detalles
kubectl get pod debug-demo -o wide

# Describe completo (más información)
kubectl describe pod debug-demo
----

4. Obtén el YAML completo del Pod:
+
[source,bash]
----
kubectl get pod debug-demo -o yaml
----

5. Extrae información específica usando JSONPath:
+
[source,bash]
----
# IP del Pod
kubectl get pod debug-demo -o jsonpath='{.status.podIP}'

# Nombre del nodo
kubectl get pod debug-demo -o jsonpath='{.spec.nodeName}'

# Imagen del contenedor
kubectl get pod debug-demo -o jsonpath='{.spec.containers[0].image}'

# Estado del Pod
kubectl get pod debug-demo -o jsonpath='{.status.phase}'
----

6. Accede a los logs del contenedor:
+
[source,bash]
----
# Últimas líneas
kubectl logs debug-demo

# Últimas 20 líneas
kubectl logs debug-demo --tail=20

# Seguir logs en tiempo real
kubectl logs -f debug-demo

# Logs desde hace 1 minuto
kubectl logs debug-demo --since=1m
----

7. Ejecuta comandos dentro del contenedor (exec):
+
[source,bash]
----
# Ver el contenido del directorio raíz
kubectl exec debug-demo -- ls /

# Ver variables de entorno
kubectl exec debug-demo -- env | grep NGINX

# Ver procesos en ejecución
kubectl exec debug-demo -- ps aux

# Shell interactivo
kubectl exec -it debug-demo -- /bin/bash
# Dentro del Pod:
  ps aux
  netstat -tlnp
  curl http://localhost:80
  exit
----

8. Port forward - accede a los puertos del Pod desde tu máquina local:
+
[source,bash]
----
# En una terminal, activa port-forward
kubectl port-forward debug-demo 8080:80 &

# En otra terminal, accede al servicio
curl http://localhost:8080

# Detén el port-forward
pkill -f "port-forward"
----

9. Copia archivos hacia/desde el Pod:
+
[source,bash]
----
# Copia un archivo del Pod a tu máquina local
kubectl cp debug-demo:/etc/nginx/nginx.conf ./nginx.conf

# Copia un archivo desde tu máquina al Pod
echo "Hello from local" > test.txt
kubectl cp test.txt debug-demo:/tmp/test.txt

# Verifica el archivo en el Pod
kubectl exec debug-demo -- cat /tmp/test.txt
----

10. Monitorea el Pod en tiempo real:
+
[source,bash]
----
# Watch mode - actualización continua
kubectl get pod debug-demo -w

# Ver eventos del cluster
kubectl get events --sort-by='.lastTimestamp'
----

11. Limpia:
+
[source,bash]
----
kubectl delete pod debug-demo
rm nginx.conf test.txt
----

**Verificación:**
Deberías poder acceder a toda la información del Pod y ejecutar comandos dentro del contenedor.

**Preguntas de reflexión:**
1. ¿Cuál es la diferencia entre `logs` y `describe`?
2. ¿Cómo accedes a información sensible en JSONPath?
3. ¿Qué limitaciones tiene `exec` para debugging?
4. ¿Por qué port-forward es útil para debugging?

---

=== Ejercicio 2.5: Pod con Restart Policy

**Objetivo de aprendizaje:**
Entender cómo las políticas de reinicio controlan el comportamiento de los contenedores fallidos.

**Descripción:**
Crearás Pods con diferentes políticas de reinicio (Always, OnFailure, Never) y observarás cómo Kubernetes maneja los contenedores que fallan.

**Tareas:**

1. Crea un Pod que falla inmediatamente (`always-restart-pod.yaml`):
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-always-restart
  labels:
    app: always-restart
spec:
  restartPolicy: Always  # Siempre reinicia
  containers:
  - name: failing-app
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      echo "App iniciada"
      sleep 2
      echo "App crasheando..."
      exit 1  # Termina con error
----

2. Despliega y observa los reinicios:
+
[source,bash]
----
kubectl apply -f always-restart-pod.yaml
kubectl get pods
sleep 5
kubectl get pods  # El Pod debería tener más reinicios
sleep 5
kubectl describe pod pod-always-restart
----

3. Ver los reinicios en la columna RESTARTS:
+
[source,bash]
----
kubectl get pods -w
----

4. Crea un Pod con política OnFailure (`onfailure-restart-pod.yaml`):
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-onfailure-restart
  labels:
    app: onfailure-restart
spec:
  restartPolicy: OnFailure  # Solo reinicia si falla
  containers:
  - name: task
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      echo "Ejecutando tarea..."
      sleep 5
      echo "Tarea completada"
      exit 0  # Termina exitosamente
----

5. Despliega y observa:
+
[source,bash]
----
kubectl apply -f onfailure-restart-pod.yaml
kubectl get pods -w
# Debería completar sin reinicios
----

6. Crea un Pod con política Never (`never-restart-pod.yaml`):
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-never-restart
  labels:
    app: never-restart
spec:
  restartPolicy: Never  # Nunca reinicia
  containers:
  - name: one-time-task
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      echo "Ejecutando una sola vez..."
      exit 1  # Falla
----

7. Despliega y observa que NO reinicia:
+
[source,bash]
----
kubectl apply -f never-restart-pod.yaml
kubectl get pods
# Quedará en estado Error, sin reinicios
kubectl describe pod pod-never-restart
----

8. Compara los tres Pods:
+
[source,bash]
----
kubectl get pods
kubectl get pods -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,RESTARTS:.status.containerStatuses[0].restartCount
----

9. Limpia:
+
[source,bash]
----
kubectl delete pod pod-always-restart pod-onfailure-restart pod-never-restart
----

**Verificación:**
Deberías ver comportamientos diferentes en cada Pod según su política de reinicio.

**Preguntas de reflexión:**
1. ¿Cuándo usarías cada política de reinicio?
2. ¿Qué ocurre con los logs después de un reinicio?
3. ¿Cómo afecta restartPolicy a la disponibilidad de la aplicación?
4. ¿Puedes cambiar restartPolicy de un Pod en ejecución?

---

=== Ejercicio 2.6: Pod con ImagePullPolicy

**Objetivo de aprendizaje:**
Entender cómo Kubernetes gestiona la descarga y el almacenamiento en caché de imágenes de contenedor.

**Descripción:**
Aprenderás a controlar cuándo Kubernetes descarga nuevas imágenes usando imagePullPolicy.

**Tareas:**

1. Crea un Pod con imagePullPolicy: Always (`image-always-pull.yaml`):
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-always-pull
  labels:
    app: always-pull
spec:
  containers:
  - name: app
    image: nginx:1.21
    imagePullPolicy: Always  # Descarga siempre la imagen más nueva
    ports:
    - containerPort: 80
----

2. Despliega:
+
[source,bash]
----
kubectl apply -f image-always-pull.yaml
kubectl get pods
kubectl describe pod pod-always-pull | grep -A 5 "Image:"
----

3. Crea un Pod con imagePullPolicy: IfNotPresent (por defecto):
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-ifnotpresent
  labels:
    app: ifnotpresent
spec:
  containers:
  - name: app
    image: nginx:1.21
    imagePullPolicy: IfNotPresent  # Solo descarga si no existe localmente
    ports:
    - containerPort: 80
----

4. Despliega:
+
[source,bash]
----
kubectl apply -f image-ifnotpresent.yaml
kubectl get pods
----

5. Crea un Pod con imagePullPolicy: Never:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-never-pull
  labels:
    app: never-pull
spec:
  containers:
  - name: app
    image: nginx:1.21
    imagePullPolicy: Never  # No intenta descargar, usa caché o falla
    ports:
    - containerPort: 80
----

6. Despliega:
+
[source,bash]
----
kubectl apply -f image-never-pull.yaml
kubectl get pods
----

7. Comprueba el estado de cada Pod:
+
[source,bash]
----
kubectl get pods -o wide
kubectl describe pod pod-always-pull | grep -A 10 "Events:"
kubectl describe pod pod-ifnotpresent | grep -A 10 "Events:"
kubectl describe pod pod-never-pull | grep -A 10 "Events:"
----

8. Limpia:
+
[source,bash]
----
kubectl delete pod pod-always-pull pod-ifnotpresent pod-never-pull
----

**Verificación:**
Los Pods deberían mostrar comportamientos diferentes en cuanto a descarga de imágenes en sus eventos.

**Preguntas de reflexión:**
1. ¿Cuándo deberías usar Always vs IfNotPresent?
2. ¿Qué es el image cache?
3. ¿Cuál es la implicación de seguridad de usar IfNotPresent con tags específicas?
4. ¿Cómo afecta imagePullPolicy al tiempo de inicio?

---

=== Ejercicio 2.7: Pod con SecurityContext

**Objetivo de aprendizaje:**
Implementar controles de seguridad a nivel de Pod usando SecurityContext.

**Descripción:**
Los SecurityContexts permiten definir permisos y capacidades de seguridad para Pods y contenedores.

**Tareas:**

1. Crea un Pod con SecurityContext a nivel de Pod (`security-context-pod.yaml`):
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-security-context
  labels:
    app: secure-pod
spec:
  # SecurityContext a nivel de Pod
  securityContext:
    runAsNonRoot: true  # Obliga a ejecutar como usuario no-root
    runAsUser: 1000     # UID específico
    fsGroup: 2000       # Grupo para volúmenes
    seccompProfile:
      type: RuntimeDefault

  containers:
  - name: secure-app
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      echo "ID del usuario actual:"
      id
      echo "ID de grupos:"
      groups
      sleep 30000
    volumeMounts:
    - name: shared-data
      mountPath: /data

  volumes:
  - name: shared-data
    emptyDir: {}
----

2. Despliega el Pod:
+
[source,bash]
----
kubectl apply -f security-context-pod.yaml
kubectl get pods
----

3. Verifica el usuario dentro del contenedor:
+
[source,bash]
----
kubectl exec pod-with-security-context -- id
kubectl exec pod-with-security-context -- whoami
----

4. Crea un Pod sin SecurityContext (no recomendado):
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-without-security-context
spec:
  containers:
  - name: app
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      echo "Ejecutando como:"
      id
      sleep 30000
----

5. Despliega y compara:
+
[source,bash]
----
kubectl apply -f no-security-pod.yaml
kubectl exec pod-without-security-context -- id
# Este probablemente ejecute como root (UID 0)
----

6. Crea un Pod con SecurityContext en el contenedor:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-container-security-context
spec:
  containers:
  - name: app
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      id
      sleep 30000
    # SecurityContext a nivel de contenedor
    securityContext:
      runAsUser: 2000
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
----

7. Despliega y verifica:
+
[source,bash]
----
kubectl apply -f container-security-pod.yaml
kubectl exec pod-container-security-context -- id
----

8. Intenta escribir en el filesystem (debería fallar):
+
[source,bash]
----
kubectl exec pod-container-security-context -- touch /test.txt
# Error: Read-only file system
----

9. Limpia:
+
[source,bash]
----
kubectl delete pod pod-with-security-context pod-without-security-context pod-container-security-context
----

**Verificación:**
Los Pods con SecurityContext deberían ejecutar con el usuario especificado, y el filesystem de solo lectura debería prevenir escrituras.

**Preguntas de reflexión:**
1. ¿Por qué es importante ejecutar contenedores como non-root?
2. ¿Cuál es la diferencia entre SecurityContext a nivel Pod vs contenedor?
3. ¿Qué es fsGroup?
4. ¿Cómo se relaciona SecurityContext con PodSecurityPolicies?

---

=== Ejercicio 2.8: Pod con Labels y Selectors avanzados

**Objetivo de aprendizaje:**
Practicar el uso avanzado de Labels y Selectors para organizar y filtrar Pods.

**Descripción:**
Los Labels son cruciales para organizar recursos en Kubernetes. Aprenderás a crear y usar Labels complejos.

**Tareas:**

1. Crea múltiples Pods con diferentes labels (`labeled-pods.yaml`):
+
[source,yaml]
----
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-frontend-prod
  labels:
    app: myapp
    component: frontend
    environment: production
    tier: web
    version: "2.0"
spec:
  containers:
  - name: app
    image: nginx:1.21

---
apiVersion: v1
kind: Pod
metadata:
  name: pod-frontend-dev
  labels:
    app: myapp
    component: frontend
    environment: development
    tier: web
    version: "1.9"
spec:
  containers:
  - name: app
    image: nginx:1.20

---
apiVersion: v1
kind: Pod
metadata:
  name: pod-backend-prod
  labels:
    app: myapp
    component: backend
    environment: production
    tier: database
    version: "3.0"
spec:
  containers:
  - name: app
    image: busybox:1.28
    command:
    - sleep
    - "30000"

---
apiVersion: v1
kind: Pod
metadata:
  name: pod-backend-dev
  labels:
    app: myapp
    component: backend
    environment: development
    tier: database
    version: "2.8"
spec:
  containers:
  - name: app
    image: busybox:1.28
    command:
    - sleep
    - "30000"

---
apiVersion: v1
kind: Pod
metadata:
  name: pod-other-app
  labels:
    app: otherapp
    component: frontend
    environment: production
    version: "1.0"
spec:
  containers:
  - name: app
    image: nginx:1.21
----

2. Despliega todos los Pods:
+
[source,bash]
----
kubectl apply -f labeled-pods.yaml
kubectl get pods --show-labels
----

3. Filtra con selectors simples:
+
[source,bash]
----
# Todos los pods con label app=myapp
kubectl get pods -l app=myapp

# Todos los pods con environment=production
kubectl get pods -l environment=production

# Todos los pods con component=backend
kubectl get pods -l component=backend
----

4. Usa operadores de selección:
+
[source,bash]
----
# NOT equal
kubectl get pods -l environment!=development

# IN operator
kubectl get pods -l 'environment in (production,staging)'

# NOT IN operator
kubectl get pods -l 'environment notin (development)'

# Múltiples selectores (AND implícito)
kubectl get pods -l app=myapp,environment=production

# EXISTS operator
kubectl get pods -l tier
----

5. Filtra por ausencia de labels:
+
[source,bash]
----
# Pods sin el label tier
kubectl get pods -l '!tier'
----

6. Ver columnas específicas:
+
[source,bash]
----
kubectl get pods \
  -o custom-columns=NAME:.metadata.name,\
APP:.metadata.labels.app,\
ENV:.metadata.labels.environment,\
COMPONENT:.metadata.labels.component
----

7. Añade un label a un Pod en ejecución:
+
[source,bash]
----
kubectl label pod pod-frontend-prod monitored=true
kubectl get pod pod-frontend-prod --show-labels
----

8. Modifica un label:
+
[source,bash]
----
kubectl label pod pod-frontend-prod environment=staging --overwrite
kubectl get pod pod-frontend-prod --show-labels
----

9. Elimina un label:
+
[source,bash]
----
kubectl label pod pod-frontend-prod monitored-
kubectl get pod pod-frontend-prod --show-labels
----

10. Usa field-selector para filtrar:
+
[source,bash]
----
# Por estatus
kubectl get pods --field-selector=status.phase=Running

# Por namespace
kubectl get pods --field-selector=metadata.namespace=default
----

11. Limpia:
+
[source,bash]
----
kubectl delete -f labeled-pods.yaml
----

**Verificación:**
Deberías poder filtrar los Pods de múltiples formas usando diferentes combinaciones de labels y selectores.

**Preguntas de reflexión:**
1. ¿Cuál es la diferencia entre Labels y Annotations?
2. ¿Cómo usan los Services los Labels?
3. ¿Qué convenciones de naming se recomiendan para Labels?
4. ¿Por qué no usar names en lugar de Labels?

---

=== Ejercicio 2.9: Pod con Liveness, Readiness y Startup Probes

**Objetivo de aprendizaje:**
Implementar health checks avanzados usando los tres tipos de probes.

**Descripción:**
Aunque ya tocamos liveness y readiness en el módulo 1, aquí practicaremos los tres tipos de probes con más detalle.

**Tareas:**

1. Crea un Pod con los tres tipos de probes (`health-checks-complete.yaml`):
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-all-probes
  labels:
    app: health-check-demo
spec:
  containers:
  - name: app-with-slow-startup
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      # Simula una app con startup lento
      echo "App iniciando..."
      sleep 15  # Tarda 15s en estar lista
      echo "App lista para servir requests"

      # Simula servidor HTTP en puerto 8080
      while true; do
        { echo -ne "HTTP/1.1 200 OK\r\nContent-Length: 2\r\n\r\nOK"; } | nc -l -p 8080 -q 1
      done

    ports:
    - containerPort: 8080

    # Startup Probe - espera a que la app esté lista (máximo 60s)
    startupProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 0
      periodSeconds: 5
      failureThreshold: 12  # 12 * 5s = 60s máximo para startup
      successThreshold: 1

    # Readiness Probe - verifica si puede servir requests
    readinessProbe:
      httpGet:
        path: /
        port: 8080
      initialDelaySeconds: 0
      periodSeconds: 5
      timeoutSeconds: 1
      failureThreshold: 2

    # Liveness Probe - verifica si está viva
    livenessProbe:
      httpGet:
        path: /
        port: 8080
      initialDelaySeconds: 20
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 3
----

2. Despliega el Pod:
+
[source,bash]
----
kubectl apply -f health-checks-complete.yaml
----

3. Observa cómo el Pod pasa a través de las fases:
+
[source,bash]
----
# Initially, startupProbe is failing
kubectl get pods -w

# Después de 15 segundos, el Pod debería pasar a Ready
----

4. Describe el Pod para ver detalles de los probes:
+
[source,bash]
----
kubectl describe pod pod-with-all-probes | grep -A 15 "Liveness\|Readiness\|Startup"
----

5. Crea un Pod que falla el readiness pero no el liveness:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-readiness-failing
spec:
  containers:
  - name: app
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      echo "App ejecutando pero no lista..."
      sleep 30000

    # Liveness: siempre pasa
    livenessProbe:
      exec:
        command:
        - sh
        - -c
        - "exit 0"  # Siempre exitoso
      initialDelaySeconds: 5
      periodSeconds: 10

    # Readiness: siempre falla
    readinessProbe:
      exec:
        command:
        - sh
        - -c
        - "exit 1"  # Siempre falla
      initialDelaySeconds: 5
      periodSeconds: 5
----

6. Despliega:
+
[source,bash]
----
kubectl apply -f readiness-failing-pod.yaml
kubectl get pods
# Pod estará Running pero 0/1 Ready
----

7. Describe para entender el estado:
+
[source,bash]
----
kubectl describe pod pod-readiness-failing
# Verás que Liveness es OK pero Readiness está fallando
----

8. Limpia:
+
[source,bash]
----
kubectl delete pod pod-with-all-probes pod-readiness-failing
----

**Verificación:**
El primer Pod debería convertirse en Ready después de su período de startup. El segundo Pod debería estar Running pero no Ready.

**Preguntas de reflexión:**
1. ¿Cuál es la diferencia entre los tres tipos de probes?
2. ¿Qué ocurre si readiness falla pero liveness pasa?
3. ¿Cuándo es importante startup probe?
4. ¿Cómo afecta la readiness probe al Service?

---

=== Ejercicio 2.10: Pod con Volumes emptyDir y resourceQuotas

**Objetivo de aprendizaje:**
Entender cómo los Pods comparten datos usando volúmenes compartidos.

**Descripción:**
Los volúmenes emptyDir proporcionan almacenamiento temporal compartido entre contenedores en un Pod.

**Tareas:**

1. Crea un Pod con volumen emptyDir (`emptydir-volume-pod.yaml`):
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-emptydir-volume
  labels:
    app: volume-demo
spec:
  # Volúmenes disponibles para todos los contenedores
  volumes:
  - name: shared-storage
    emptyDir: {}  # Volumen temporal, se elimina cuando el Pod muere

  - name: cache-storage
    emptyDir:
      medium: Memory  # Almacenado en memoria (tmpfs)
      sizeLimit: 128Mi  # Límite de 128MB

  containers:
  # Contenedor 1 - productor de datos
  - name: producer
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      echo "Productor iniciado"
      for i in $(seq 1 10); do
        echo "Datos generados en segundo $i: $(date)" >> /shared/data.log
        sleep 2
      done
      echo "Productor terminado"
    volumeMounts:
    - name: shared-storage
      mountPath: /shared
    - name: cache-storage
      mountPath: /cache

  # Contenedor 2 - consumidor de datos
  - name: consumer
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      echo "Consumidor iniciado"
      sleep 5  # Espera a que el productor genere datos
      echo "=== Datos del productor ==="
      if [ -f /shared/data.log ]; then
        cat /shared/data.log
      else
        echo "No hay datos aún"
      fi
      echo "Esperando más datos..."
      sleep 10
      echo "=== Datos finales ==="
      cat /shared/data.log

      echo "=== Cache storage ==="
      ls -la /cache
      echo "Cache vacío" > /cache/status.txt
      echo "Consumidor terminado"
    volumeMounts:
    - name: shared-storage
      mountPath: /shared
      readOnly: false
    - name: cache-storage
      mountPath: /cache
----

2. Despliega el Pod:
+
[source,bash]
----
kubectl apply -f emptydir-volume-pod.yaml
kubectl get pods
----

3. Observa los logs de ambos contenedores:
+
[source,bash]
----
# Logs del productor
kubectl logs pod-with-emptydir-volume -c producer

# Logs del consumidor
kubectl logs pod-with-emptydir-volume -c consumer
----

4. Accede al Pod y verifica los archivos:
+
[source,bash]
----
kubectl exec -it pod-with-emptydir-volume -c producer -- cat /shared/data.log
kubectl exec -it pod-with-emptydir-volume -c consumer -- ls -la /shared
----

5. Ver detalles del Pod incluyendo volúmenes:
+
[source,bash]
----
kubectl get pod pod-with-emptydir-volume -o yaml | grep -A 20 "volumes:"
----

6. Crea un Pod con un volumen que tiene límite de tamaño:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-limited-volume
spec:
  volumes:
  - name: limited-storage
    emptyDir:
      sizeLimit: 10Mi  # Máximo 10MB

  containers:
  - name: app
    image: busybox:1.28
    command:
    - sh
    - -c
    - |
      echo "Intentando escribir datos..."
      # Intenta escribir más de 10MB
      dd if=/dev/zero of=/data/bigfile bs=1M count=20
    volumeMounts:
    - name: limited-storage
      mountPath: /data
----

7. Despliega e intenta sobrepasar el límite:
+
[source,bash]
----
kubectl apply -f limited-volume-pod.yaml
kubectl logs pod-with-limited-volume
# Debería fallar al intentar escribir más de 10MB
----

8. Limpia:
+
[source,bash]
----
kubectl delete pod pod-with-emptydir-volume pod-with-limited-volume
----

**Verificación:**
El consumidor debería poder leer los datos escritos por el productor. El Pod con límite de volumen debería fallar al exceder su límite.

**Preguntas de reflexión:**
1. ¿Qué ocurre con los datos de un emptyDir cuando el Pod muere?
2. ¿Cuál es la diferencia entre emptyDir normal y emptyDir con medium: Memory?
3. ¿Cómo se comporta un volumen compartido con múltiples contenedores?
4. ¿Qué alternativas hay a emptyDir para almacenamiento persistente?

---

== Resumen de Conceptos

Estos ejercicios del Módulo 2 cubren:

* **Ejercicio 2.1**: Pods multi-contenedor con patrón Sidecar
* **Ejercicio 2.2**: Init Containers para inicialización
* **Ejercicio 2.3**: Patrón Ambassador para proxying
* **Ejercicio 2.4**: Debugging avanzado de Pods
* **Ejercicio 2.5**: Políticas de reinicio de contenedores
* **Ejercicio 2.6**: Control de descarga de imágenes
* **Ejercicio 2.7**: SecurityContext para controles de seguridad
* **Ejercicio 2.8**: Labels y Selectors avanzados
* **Ejercicio 2.9**: Health checks (Liveness, Readiness, Startup)
* **Ejercicio 2.10**: Volúmenes compartidos (emptyDir)

== Módulo 3: Controllers y Workloads

=== Ejercicio 3.1: Crear y gestionar un ReplicaSet básico

**Objetivo de aprendizaje:**
Entender cómo los ReplicaSets mantienen un número específico de Pods en ejecución y crean/eliminan Pods automáticamente.

**Descripción:**
Crearás un ReplicaSet que gestiona 3 réplicas de nginx. Experimentarás cómo ReplicaSet se auto-recupera cuando eliminas Pods.

**Tareas:**

1. Crea un archivo YAML llamado `replicaset-basic.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-replicaset
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
----

2. Crea el ReplicaSet:
+
[source,bash]
----
kubectl apply -f replicaset-basic.yaml
----

3. Verifica que se crearon 3 Pods:
+
[source,bash]
----
kubectl get replicaset
kubectl get pods -l app=nginx
kubectl describe rs nginx-replicaset
----

4. Observa los Pods en tiempo real:
+
[source,bash]
----
kubectl get pods -l app=nginx -w
----

5. Elimina manualmente un Pod y observa cómo ReplicaSet lo recrea:
+
[source,bash]
----
# En otra terminal, ejecuta:
kubectl delete pod <nombre-del-pod>

# En la primera terminal (con -w), observa cómo se crea uno nuevo automáticamente
----

6. Ver información completa del ReplicaSet:
+
[source,bash]
----
kubectl get rs nginx-replicaset -o yaml
kubectl describe rs nginx-replicaset
----

7. Limpia:
+
[source,bash]
----
kubectl delete rs nginx-replicaset
----

**Verificación:**
Deberías ver 3 Pods en ejecución, y cuando elimines uno, ReplicaSet automáticamente crea uno nuevo.

**Preguntas de reflexión:**
1. ¿Cómo sabe ReplicaSet qué Pods gestiona?
2. ¿Qué ocurre si cambias manualmente el label de un Pod?
3. ¿Por qué deberías usar Deployments en lugar de ReplicaSets directamente?

---

=== Ejercicio 3.2: Escalar un ReplicaSet

**Objetivo de aprendizaje:**
Aprender a cambiar dinámicamente el número de réplicas en un ReplicaSet.

**Descripción:**
Crearás un ReplicaSet y lo escalarás a diferentes números de réplicas observando cómo se crean/eliminan Pods.

**Tareas:**

1. Crea un archivo YAML llamado `replicaset-scalable.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: app-replicaset
  labels:
    app: myapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: busybox:1.28
        command: ["sleep", "3600"]
        resources:
          requests:
            cpu: "50m"
            memory: "64Mi"
----

2. Crea el ReplicaSet:
+
[source,bash]
----
kubectl apply -f replicaset-scalable.yaml
kubectl get pods -l app=myapp
----

3. Escala a 5 réplicas usando kubectl scale:
+
[source,bash]
----
kubectl scale rs app-replicaset --replicas=5
kubectl get pods -l app=myapp -w  # Observa cómo se crean 3 Pods más
----

4. Escala a 10 réplicas:
+
[source,bash]
----
kubectl scale rs app-replicaset --replicas=10
kubectl get pods -l app=myapp | wc -l  # Cuenta los Pods
----

5. Reduce a 3 réplicas:
+
[source,bash]
----
kubectl scale rs app-replicaset --replicas=3
kubectl get pods -l app=myapp -w  # Observa cómo se eliminan Pods
----

6. Escala a 0 para parar todos los Pods:
+
[source,bash]
----
kubectl scale rs app-replicaset --replicas=0
kubectl get pods -l app=myapp  # No debe haber Pods
----

7. Escala nuevamente a 2:
+
[source,bash]
----
kubectl scale rs app-replicaset --replicas=2
kubectl get pods -l app=myapp
----

8. Alterna el número de réplicas varias veces:
+
[source,bash]
----
kubectl scale rs app-replicaset --replicas=7
sleep 3
kubectl get rs app-replicaset
kubectl scale rs app-replicaset --replicas=4
kubectl get rs app-replicaset
----

9. Limpia:
+
[source,bash]
----
kubectl delete rs app-replicaset
----

**Verificación:**
El número de Pods debería cambiar dinámicamente según los valores que especifiques.

**Preguntas de reflexión:**
1. ¿Cuánto tiempo tarda en crear/eliminar Pods?
2. ¿Qué ocurre si intentas escalar a un número muy alto?
3. ¿Cómo podrías hacer que el escalado sea automático basado en métricas?

---

=== Ejercicio 3.3: Crear un Deployment simple

**Objetivo de aprendizaje:**
Entender la diferencia entre ReplicaSets y Deployments. Los Deployments proporcionan actualizaciones declarativas.

**Descripción:**
Crearás un Deployment que gestiona automáticamente un ReplicaSet. Verás que los Deployments son la forma recomendada de trabajar.

**Tareas:**

1. Crea un archivo YAML llamado `deployment-basic.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deployment
  labels:
    app: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "200m"
            memory: "256Mi"
----

2. Crea el Deployment:
+
[source,bash]
----
kubectl apply -f deployment-basic.yaml
----

3. Verifica el Deployment:
+
[source,bash]
----
kubectl get deployment
kubectl get rs -l app=web  # Verifica que Deployment creó un ReplicaSet
kubectl get pods -l app=web
----

4. Describe el Deployment:
+
[source,bash]
----
kubectl describe deployment web-deployment
----

5. Ver el YAML completo:
+
[source,bash]
----
kubectl get deployment web-deployment -o yaml
----

6. Escala el Deployment:
+
[source,bash]
----
kubectl scale deployment web-deployment --replicas=5
kubectl get pods -l app=web
----

7. Ver ReplicaSets asociados:
+
[source,bash]
----
kubectl get rs -l app=web  # Debe haber un ReplicaSet
----

8. Limpia:
+
[source,bash]
----
kubectl delete deployment web-deployment
----

**Verificación:**
El Deployment debería crear un ReplicaSet que a su vez crea los Pods.

**Preguntas de reflexión:**
1. ¿Cuál es la relación entre Deployment y ReplicaSet?
2. ¿Qué ocurre con el ReplicaSet cuando eliminas el Deployment?
3. ¿Por qué los Deployments son preferibles a ReplicaSets?

---

=== Ejercicio 3.4: Rolling Updates - Actualizar imagen de Deployment

**Objetivo de aprendizaje:**
Entender cómo los Deployments actualizan aplicaciones sin downtime usando Rolling Updates.

**Descripción:**
Crearás un Deployment y actualizarás su imagen a una versión más nueva, observando cómo ReplicaSet maneja la transición.

**Tareas:**

1. Crea un archivo YAML llamado `deployment-rolling.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deployment
  labels:
    app: app
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1          # Máximo 1 Pod extra durante actualización
      maxUnavailable: 1    # Máximo 1 Pod puede estar no disponible
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
----

2. Crea el Deployment:
+
[source,bash]
----
kubectl apply -f deployment-rolling.yaml
kubectl get pods -l app=app
----

3. Ver historial de despliegues:
+
[source,bash]
----
kubectl rollout history deployment/app-deployment
----

4. Actualiza la imagen a una versión más nueva:
+
[source,bash]
----
kubectl set image deployment/app-deployment nginx=nginx:1.22 --record
----

5. Monitorea la actualización en tiempo real:
+
[source,bash]
----
# En una terminal:
kubectl get pods -l app=app -w

# En otra terminal:
kubectl rollout status deployment/app-deployment -w
----

6. Ver ReplicaSets durante la actualización:
+
[source,bash]
----
kubectl get rs -l app=app  # Deberías ver 2 ReplicaSets
----

7. Cuando termine la actualización, verifica que todos los Pods están en la nueva versión:
+
[source,bash]
----
kubectl get pods -l app=app -o wide
kubectl rollout status deployment/app-deployment
----

8. Actualiza nuevamente a otra versión:
+
[source,bash]
----
kubectl set image deployment/app-deployment nginx=nginx:1.23 --record
kubectl rollout status deployment/app-deployment -w
----

9. Ver historial actualizado:
+
[source,bash]
----
kubectl rollout history deployment/app-deployment
----

10. Limpia:
+
[source,bash]
----
kubectl delete deployment app-deployment
----

**Verificación:**
Deberías ver Pods siendo reemplazados gradualmente sin que el servicio se interrumpa.

**Preguntas de reflexión:**
1. ¿Cuántos Pods están corriendo durante la actualización?
2. ¿Qué significa maxSurge y maxUnavailable?
3. ¿Cómo se relacionan los ReplicaSets antiguos y nuevos?

---

=== Ejercicio 3.5: Rollback a versión anterior

**Objetivo de aprendizaje:**
Aprender a revertir a una versión anterior de un Deployment rápidamente.

**Descripción:**
Crearás un Deployment, actualizarás varias veces y practicarás rollbacks.

**Tareas:**

1. Crea un archivo YAML llamado `deployment-versions.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: versioned-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
----

2. Crea el Deployment:
+
[source,bash]
----
kubectl apply -f deployment-versions.yaml
kubectl rollout history deployment/versioned-app
# REVISION  CHANGE-CAUSE
# 1         <none>
----

3. Actualiza a versión 1.21:
+
[source,bash]
----
kubectl set image deployment/versioned-app nginx=nginx:1.21 --record
kubectl rollout status deployment/versioned-app
----

4. Verifica el historial:
+
[source,bash]
----
kubectl rollout history deployment/versioned-app
# REVISION  CHANGE-CAUSE
# 1         <none>
# 2         kubectl set image deployment/versioned-app nginx=nginx:1.21 --record
----

5. Actualiza a versión 1.22:
+
[source,bash]
----
kubectl set image deployment/versioned-app nginx=nginx:1.22 --record
kubectl rollout status deployment/versioned-app
----

6. Verifica el historial:
+
[source,bash]
----
kubectl rollout history deployment/versioned-app
----

7. Haz rollback a la revisión anterior (1.21):
+
[source,bash]
----
kubectl rollout undo deployment/versioned-app
kubectl rollout status deployment/versioned-app
----

8. Verifica que volvió a 1.21:
+
[source,bash]
----
kubectl get deployment versioned-app -o jsonpath='{.spec.template.spec.containers[0].image}'
----

9. Haz rollback a una revisión específica (revisión 1):
+
[source,bash]
----
kubectl rollout undo deployment/versioned-app --to-revision=1
kubectl rollout status deployment/versioned-app
----

10. Verifica que volvió a 1.20:
+
[source,bash]
----
kubectl get deployment versioned-app -o jsonpath='{.spec.template.spec.containers[0].image}'
----

11. Ver todos los ReplicaSets creados:
+
[source,bash]
----
kubectl get rs -l app=app
----

12. Limpia:
+
[source,bash]
----
kubectl delete deployment versioned-app
----

**Verificación:**
Deberías poder hacer rollback a cualquier versión anterior y los Pods deberían actualizarse automáticamente.

**Preguntas de reflexión:**
1. ¿Cuántas revisiones se guardan por defecto?
2. ¿Cuántos ReplicaSets se crean?
3. ¿Qué ocurre con los datos si haces rollback?

---

=== Ejercicio 3.6: Pausa y reanuda de Deployments

**Objetivo de aprendizaje:**
Aprender a pausar un despliegue, hacer cambios, y reanudar para aplicarlos todos a la vez.

**Descripción:**
Practicarás pausar un Deployment, realizar múltiples cambios, y luego reanudar para que se apliquen conjuntamente.

**Tareas:**

1. Crea un Deployment:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pause-demo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
----

2. Despliega:
+
[source,bash]
----
kubectl apply -f pause-demo.yaml
kubectl get pods -l app=app
----

3. Pausa el Deployment:
+
[source,bash]
----
kubectl rollout pause deployment/pause-demo
----

4. Realiza varios cambios mientras está pausado:
+
[source,bash]
----
# Cambio 1: Actualizar imagen
kubectl set image deployment/pause-demo nginx=nginx:1.22

# Cambio 2: Escalar a 5 réplicas
kubectl scale deployment/pause-demo --replicas=5

# Cambio 3: Agregar recurso limite
kubectl set resources deployment/pause-demo --limits=cpu=200m,memory=256Mi
----

5. Verifica el estado durante pausa:
+
[source,bash]
----
kubectl get pods -l app=app  # Debería seguir siendo 3 Pods (sin cambios)
kubectl get deployment pause-demo  # Status debe mostrar "PAUSED"
----

6. Reanuda el Deployment:
+
[source,bash]
----
kubectl rollout resume deployment/pause-demo
----

7. Monitorea cómo se aplican todos los cambios:
+
[source,bash]
----
kubectl get pods -l app=app -w  # Verás escalado a 5 y actualización de imagen
kubectl rollout status deployment/pause-demo -w
----

8. Verifica los cambios finales:
+
[source,bash]
----
kubectl get deployment pause-demo
kubectl get pods -l app=app  # Debe haber 5 Pods
kubectl get deployment pause-demo -o jsonpath='{.spec.template.spec.containers[0].image}'
----

9. Limpia:
+
[source,bash]
----
kubectl delete deployment pause-demo
----

**Verificación:**
Todos los cambios deberían aplicarse cuando reanudir, no durante la pausa.

**Preguntas de reflexión:**
1. ¿Por qué es útil pausar despliegues?
2. ¿Qué ocurre si intentas cambios mientras está pausado?
3. ¿Cómo combinarías pausa con el monitoreo de métricas?

---

=== Ejercicio 3.7: StatefulSet básico

**Objetivo de aprendizaje:**
Entender cómo los StatefulSets proporcionan identidades estables a los Pods a diferencia de Deployments.

**Descripción:**
Crearás un StatefulSet y observarás cómo sus Pods tienen nombres ordenados y estables, a diferencia de los Deployments.

**Tareas:**

1. Crea un archivo YAML llamado `statefulset-basic.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web-headless
spec:
  clusterIP: None  # Headless Service
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 80

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web-statefulset
spec:
  serviceName: web-headless  # Requerido para StatefulSet
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
----

2. Crea el StatefulSet:
+
[source,bash]
----
kubectl apply -f statefulset-basic.yaml
----

3. Verifica que se crearon los Pods con nombres ordenados:
+
[source,bash]
----
kubectl get statefulset
kubectl get pods  # Deberías ver: web-statefulset-0, web-statefulset-1, web-statefulset-2
----

4. Compara con un Deployment (los Pods tendrían nombres aleatorios):
+
[source,bash]
----
# Para comparación, crea un Deployment:
kubectl create deployment comparison --image=nginx:1.21 --replicas=3
kubectl get pods  # Verás nombres como comparison-xyz789, comparison-abc123, etc.
----

5. Elimina un Pod del StatefulSet y observa que se recrea con el mismo nombre:
+
[source,bash]
----
kubectl delete pod web-statefulset-1
kubectl get pods  # web-statefulset-1 debería recrearse
----

6. Escala el StatefulSet:
+
[source,bash]
----
kubectl scale statefulset web-statefulset --replicas=5
kubectl get pods  # Deberías ver web-statefulset-3 y web-statefulset-4
----

7. Verifica la identidad ordenada:
+
[source,bash]
----
# Los Pods se crean en orden
# Reducir también es en orden inverso
kubectl scale statefulset web-statefulset --replicas=2
kubectl get pods  # Quedarán web-statefulset-0 y web-statefulset-1
----

8. Limpia:
+
[source,bash]
----
kubectl delete statefulset web-statefulset
kubectl delete service web-headless
kubectl delete deployment comparison
----

**Verificación:**
Los Pods del StatefulSet deben tener nombres como `web-statefulset-0`, `web-statefulset-1`, etc.

**Preguntas de reflexión:**
1. ¿Por qué es importante la identidad estable?
2. ¿Cuál es el rol del Headless Service?
3. ¿Cuándo usarías StatefulSet vs Deployment?

---

=== Ejercicio 3.8: StatefulSet con volumeClaimTemplates

**Objetivo de aprendizaje:**
Entender cómo StatefulSets proporcionan almacenamiento persistente único para cada Pod.

**Descripción:**
Crearás un StatefulSet con volumeClaimTemplates que crea un PVC por cada Pod.

**Tareas:**

1. Crea un archivo YAML llamado `statefulset-storage.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: mysql-headless
spec:
  clusterIP: None
  selector:
    app: mysql
  ports:
  - port: 3306

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql-headless
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "password"
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
----

2. Crea el StatefulSet:
+
[source,bash]
----
kubectl apply -f statefulset-storage.yaml
----

3. Verifica los Pods creados:
+
[source,bash]
----
kubectl get statefulset
kubectl get pods -l app=mysql
----

4. Verifica que se crearon PVCs:
+
[source,bash]
----
kubectl get pvc  # Deberías ver: data-mysql-0, data-mysql-1, data-mysql-2
kubectl describe pvc data-mysql-0
----

5. Verifica que cada Pod tiene su PVC montado:
+
[source,bash]
----
kubectl describe pod mysql-0 | grep -A 5 "Mounts"
----

6. Escala el StatefulSet a 5 réplicas:
+
[source,bash]
----
kubectl scale statefulset mysql --replicas=5
kubectl get pods -l app=mysql
kubectl get pvc  # Deberías ver data-mysql-0 a data-mysql-4
----

7. Elimina un Pod y verifica que el PVC persiste:
+
[source,bash]
----
kubectl delete pod mysql-2
kubectl get pods -l app=mysql  # Se recrea mysql-2
kubectl get pvc  # data-mysql-2 sigue existiendo
----

8. Limpia:
+
[source,bash]
----
kubectl delete statefulset mysql
kubectl delete service mysql-headless
# Nota: Los PVCs no se eliminan automáticamente
kubectl delete pvc -l app=mysql
----

**Verificación:**
Deberías ver PVCs creados con nombres como `data-mysql-0`, `data-mysql-1`, etc.

**Preguntas de reflexión:**
1. ¿Qué ocurre con los datos si eliminas un Pod?
2. ¿Por qué los PVCs no se eliminan automáticamente?
3. ¿Cuándo necesitarías volumeClaimTemplates?

---

=== Ejercicio 3.9: Estrategias de actualización en StatefulSet

**Objetivo de aprendizaje:**
Entender cómo StatefulSets actualiza Pods de forma ordenada (en orden inverso).

**Descripción:**
Crearás un StatefulSet y actualizarás su imagen, observando cómo se actualiza de forma ordenada.

**Tareas:**

1. Crea un archivo YAML llamado `statefulset-update.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: app-headless
spec:
  clusterIP: None
  selector:
    app: app
  ports:
  - port: 8080

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: app
spec:
  serviceName: app-headless
  replicas: 3
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name: app
        image: busybox:1.28
        command: ["sleep", "3600"]
----

2. Crea el StatefulSet:
+
[source,bash]
----
kubectl apply -f statefulset-update.yaml
kubectl get pods -l app=app
----

3. Actualiza la imagen:
+
[source,bash]
----
kubectl set image statefulset/app app=busybox:1.35
----

4. Monitorea la actualización (es lenta, ordenada):
+
[source,bash]
----
kubectl get pods -l app=app -w  # Verás actualizarse: app-2 → app-1 → app-0
----

5. Durante la actualización, verifica qué Pod se está actualizando:
+
[source,bash]
----
kubectl describe statefulset app | grep -A 10 "Update Strategy"
----

6. Una vez terminada, verifica que todos tienen la nueva imagen:
+
[source,bash]
----
kubectl get pods -l app=app -o jsonpath='{range .items[*]}{.metadata.name}: {.spec.containers[0].image}{"\n"}{end}'
----

7. Ahora prueba una actualización Canary con partition:
+
[source,bash]
----
# Cambiar imagen nuevamente
kubectl set image statefulset/app app=busybox:1.32

# Pausar solo actualizando app-2 (partition: 2)
kubectl patch statefulset app -p '{"spec":{"updateStrategy":{"rollingUpdate":{"partition":2}}}}'

# Solo se actualizará app-2
kubectl get pods -l app=app -w
----

8. Cuando esté listo, actualizar el resto (partition: 0):
+
[source,bash]
----
kubectl patch statefulset app -p '{"spec":{"updateStrategy":{"rollingUpdate":{"partition":0}}}}'
kubectl get pods -l app=app -w  # Se actualizan app-1 y app-0
----

9. Limpia:
+
[source,bash]
----
kubectl delete statefulset app
kubectl delete service app-headless
----

**Verificación:**
La actualización debería proceder en orden inverso (del Pod más alto al más bajo).

**Preguntas de reflexión:**
1. ¿Por qué StatefulSet actualiza en orden inverso?
2. ¿Cómo se relaciona partition con canary deployments?
3. ¿Cuál es la ventaja de actualizar ordenadamente?

---

=== Ejercicio 3.10: Headless Service para StatefulSet

**Objetivo de aprendizaje:**
Entender cómo los Headless Services permiten DNS predecible para cada Pod del StatefulSet.

**Descripción:**
Crearás un Headless Service con StatefulSet y probarás la resolución DNS de cada Pod.

**Tareas:**

1. Crea un archivo YAML llamado `statefulset-headless.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: postgres-headless
spec:
  clusterIP: None
  selector:
    app: postgres
  ports:
  - port: 5432
    name: postgres

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres-headless
  replicas: 3
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:13
        env:
        - name: POSTGRES_PASSWORD
          value: "password"
        ports:
        - containerPort: 5432
        command:
        - sleep
        - "3600"
----

2. Crea el StatefulSet y Headless Service:
+
[source,bash]
----
kubectl apply -f statefulset-headless.yaml
kubectl get pods -l app=postgres
kubectl get service postgres-headless
----

3. Crea un Pod de prueba para hacer consultas DNS:
+
[source,bash]
----
kubectl run -it dnstools --image=busybox:1.28 --rm -- sh
----

4. Dentro del Pod de prueba, consulta el DNS:
+
[source,bash]
----
# Resolver el Headless Service
nslookup postgres-headless

# Debería retornar IPs de todos los Pods:
# postgres-0.postgres-headless.default.svc.cluster.local
# postgres-1.postgres-headless.default.svc.cluster.local
# postgres-2.postgres-headless.default.svc.cluster.local

# Resolver un Pod específico
nslookup postgres-0.postgres-headless

# Resolver un Service normal (para comparación)
nslookup postgres-headless

# Salir
exit
----

5. Verifica los registros DNS desde la línea de comandos:
+
[source,bash]
----
# Crear un Pod temporal para resolver DNS
kubectl run -it dns-test --image=busybox:1.28 --rm --restart=Never -- nslookup postgres-headless
----

6. Compara con un Service normal:
+
[source,bash]
----
# Crear Service normal (con ClusterIP)
kubectl expose statefulset postgres --name=postgres-service --port=5432

# Consultar ambos servicios
kubectl run -it dns-comparison --image=busybox:1.28 --rm --restart=Never -- sh -c "nslookup postgres-service && nslookup postgres-headless"
----

7. Verifica qué se obtiene de cada uno:
+
[source,bash]
----
# Service normal: retorna una sola IP (ClusterIP)
# Headless Service: retorna todas las IPs de los Pods
----

8. Limpia:
+
[source,bash]
----
kubectl delete statefulset postgres
kubectl delete service postgres-headless postgres-service
----

**Verificación:**
El Headless Service debería retornar múltiples IPs (una por cada Pod), mientras que un Service normal retorna una sola IP.

**Preguntas de reflexión:**
1. ¿Cuál es la diferencia entre un Service normal y un Headless Service?
2. ¿Por qué un StatefulSet necesita un Headless Service?
3. ¿Cómo usarías los DNS de Pod específicos?

---

== Resumen de Conceptos

Estos ejercicios del Módulo 3 cubren:

* **Ejercicio 3.1**: ReplicaSets y auto-recuperación
* **Ejercicio 3.2**: Escalado manual de ReplicaSets
* **Ejercicio 3.3**: Deployments básicos
* **Ejercicio 3.4**: Rolling Updates en Deployments
* **Ejercicio 3.5**: Rollbacks de versiones
* **Ejercicio 3.6**: Pausa y reanuda de despliegues
* **Ejercicio 3.7**: StatefulSets básicos
* **Ejercicio 3.8**: Almacenamiento persistente con volumeClaimTemplates
* **Ejercicio 3.9**: Estrategias de actualización en StatefulSet
* **Ejercicio 3.10**: Headless Services para StatefulSet

== Módulo 4: Servicios y Redes

=== Ejercicio 4.1: Comunicación Pod-to-Pod directa

**Objetivo de aprendizaje:**
Entender cómo los Pods pueden comunicarse directamente entre sí sin necesidad de un Service, usando IPs de Pods.

**Descripción:**
Crearás dos Pods que se comunicarán directamente entre sí usando sus IPs de Pod, sin la intermediación de un Service.

**Tareas:**

1. Crea un Deployment con un servidor nginx (`pod-server.yaml`):
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: server-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: server
  template:
    metadata:
      labels:
        app: server
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
----

2. Crea el servidor:
+
[source,bash]
----
kubectl apply -f pod-server.yaml
kubectl get pods -l app=server -o wide  # Anota la IP del Pod
----

3. Obtén la IP del servidor:
+
[source,bash]
----
SERVER_IP=$(kubectl get pod -l app=server -o jsonpath='{.items[0].status.podIP}')
echo "IP del servidor: $SERVER_IP"
----

4. Crea un Pod cliente que se conectará al servidor:
+
[source,bash]
----
kubectl run client --image=busybox:1.28 --rm -it --restart=Never -- \
  wget -O- http://$SERVER_IP
----

5. Verifica que el cliente puede acceder al servidor:
+
[source,bash]
----
# La respuesta debería ser la página de nginx
----

6. Crea un Pod cliente persistente para más pruebas:
+
[source,bash]
----
kubectl run test-client --image=nicolaka/netshoot -it --rm --restart=Never -- bash
----

7. Dentro del cliente, prueba comunicación:
+
[source,bash]
----
# Resuelve el nombre del servidor (por IP)
ping $SERVER_IP

# Accede al servidor
curl http://$SERVER_IP

# Salir
exit
----

8. Limpia:
+
[source,bash]
----
kubectl delete deployment server-deployment
----

**Verificación:**
El cliente debería poder alcanzar el servidor usando su IP de Pod directamente.

**Preguntas de reflexión:**
1. ¿Cuál es el problema de usar IPs de Pods directamente?
2. ¿Qué ocurre si el servidor Pod muere y se recrea?
3. ¿Cómo se comunican los Pods entre diferentes nodos?

---

=== Ejercicio 4.2: Crear un Service ClusterIP básico

**Objetivo de aprendizaje:**
Crear un Service ClusterIP que proporciona una dirección IP estable para acceder a Pods.

**Descripción:**
Crearás un Deployment con varios Pods y un Service ClusterIP que actúa como intermediario.

**Tareas:**

1. Crea un Deployment (`web-deployment.yaml`):
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
----

2. Crea un Service ClusterIP (`web-service.yaml`):
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: ClusterIP
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
----

3. Despliega ambos recursos:
+
[source,bash]
----
kubectl apply -f web-deployment.yaml
kubectl apply -f web-service.yaml
----

4. Verifica el Service:
+
[source,bash]
----
kubectl get svc web
kubectl describe svc web
----

5. Obtén la IP del Service:
+
[source,bash]
----
SERVICE_IP=$(kubectl get svc web -o jsonpath='{.status.clusterIP}')
echo "IP del Service: $SERVICE_IP"
----

6. Verifica los Endpoints (Pods detrás del Service):
+
[source,bash]
----
kubectl get endpoints web
# Debería mostrar 3 IPs de Pods
----

7. Crea un Pod cliente y accede al Service:
+
[source,bash]
----
kubectl run client --image=busybox:1.28 --rm -it --restart=Never -- \
  wget -O- http://$SERVICE_IP
----

8. Accede múltiples veces para ver el balanceo de carga:
+
[source,bash]
----
for i in {1..5}; do
  kubectl run client-$i --image=busybox:1.28 --rm --restart=Never -- \
    wget -q -O- http://web 2>/dev/null
done
----

9. Limpia:
+
[source,bash]
----
kubectl delete deployment web
kubectl delete service web
----

**Verificación:**
El Service debería tener una IP estable y los Endpoints deberían listar todos los Pods.

**Preguntas de reflexión:**
1. ¿Por qué es importante la IP estable del Service?
2. ¿Cómo el Service sabe a qué Pods enviar tráfico?
3. ¿Qué ocurre si eliminas un Pod?

---

=== Ejercicio 4.3: Service NodePort para acceso externo

**Objetivo de aprendizaje:**
Exponer un Service externamente usando NodePort.

**Descripción:**
Crearás un Service NodePort que abre un puerto en cada nodo del cluster.

**Tareas:**

1. Crea un Deployment:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name: app
        image: nginx:1.21
        ports:
        - containerPort: 80
----

2. Crea un Service NodePort (`nodeport-service.yaml`):
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: app-nodeport
spec:
  type: NodePort
  selector:
    app: app
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30080  # Puerto fijo en los nodos
----

3. Despliega:
+
[source,bash]
----
kubectl apply -f app-deployment.yaml
kubectl apply -f nodeport-service.yaml
----

4. Verifica el Service:
+
[source,bash]
----
kubectl get svc app-nodeport
# Debería mostrar port 80:30080/TCP
----

5. Obtén la IP de un nodo:
+
[source,bash]
----
# Para Minikube o Docker Desktop:
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[0].address}')
echo "IP del nodo: $NODE_IP"
----

6. Accede desde fuera del cluster:
+
[source,bash]
----
# Si estás en la máquina local:
curl http://localhost:30080

# O usando kubectl port-forward:
kubectl port-forward svc/app-nodeport 8080:80 &
curl http://localhost:8080
pkill -f "port-forward"
----

7. Verifica Endpoints:
+
[source,bash]
----
kubectl get endpoints app-nodeport
----

8. Elimina un Pod y verifica que el Service sigue disponible:
+
[source,bash]
----
kubectl delete pod -l app=app
# El Service debería seguir disponible con los Pods restantes
curl http://localhost:30080
----

9. Limpia:
+
[source,bash]
----
kubectl delete deployment app
kubectl delete service app-nodeport
----

**Verificación:**
Deberías poder acceder al servicio desde fuera del cluster usando `<node-ip>:30080`.

**Preguntas de reflexión:**
1. ¿Cuál es la diferencia entre ClusterIP y NodePort?
2. ¿En qué rango están los puertos NodePort?
3. ¿Por qué NodePort no se usa en producción típicamente?

---

=== Ejercicio 4.4: Load balancing y distribución de tráfico

**Objetivo de aprendizaje:**
Observar cómo el Service distribuye el tráfico entre múltiples Pods.

**Descripción:**
Crearás un Deployment con múltiples Pods que pueden ser identificados, y verás cómo el tráfico se distribuye.

**Tareas:**

1. Crea un Deployment con un Pod que identifica su hostname (`lb-deployment.yaml`):
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-lb
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: app
        image: nginx:1.21
        ports:
        - containerPort: 80
        lifecycle:
          postStart:
            exec:
              command:
              - sh
              - -c
              - echo "Server: $(hostname)" > /usr/share/nginx/html/index.html
----

2. Crea un Service:
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web-lb
spec:
  type: ClusterIP
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
----

3. Despliega:
+
[source,bash]
----
kubectl apply -f lb-deployment.yaml
kubectl apply -f web-lb-service.yaml
----

4. Crea un Pod cliente para enviar múltiples requests:
+
[source,bash]
----
kubectl run client --image=busybox:1.28 --rm -it --restart=Never -- sh
----

5. Dentro del cliente, envía múltiples requests:
+
[source,bash]
----
# Envía 10 requests y observa qué servidor responde
for i in {1..10}; do
  wget -q -O- http://web-lb
  echo ""
done

# Debería mostrar diferentes "Server: web-lb-xxxx" alternando
exit
----

6. Verifica los Pods y sus IPs:
+
[source,bash]
----
kubectl get pods -l app=web -o wide
----

7. Verifica los Endpoints:
+
[source,bash]
----
kubectl get endpoints web-lb
----

8. Limpia:
+
[source,bash]
----
kubectl delete deployment web-lb
kubectl delete service web-lb
----

**Verificación:**
Deberías ver que los requests se distribuyen entre los diferentes Pods (round-robin).

**Preguntas de reflexión:**
1. ¿Cuál es el algoritmo de balanceo que usa el Service?
2. ¿Cómo se distribuye el tráfico?
3. ¿Cómo funciona esto a nivel de red?

---

=== Ejercicio 4.5: Session Affinity (ClientIP)

**Objetivo de aprendizaje:**
Aprender cómo usar Session Affinity para "pegar" un cliente a un Pod específico.

**Descripción:**
Crearás un Service con Session Affinity para que un cliente siempre vaya al mismo Pod.

**Tareas:**

1. Crea un Deployment con Pods identificables (`affinity-deployment.yaml`):
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-affinity
spec:
  replicas: 3
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name: app
        image: nginx:1.21
        ports:
        - containerPort: 80
        lifecycle:
          postStart:
            exec:
              command:
              - sh
              - -c
              - echo "Pod: $(hostname)" > /usr/share/nginx/html/index.html
----

2. Crea un Service con Session Affinity (`affinity-service.yaml`):
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: app-affinity
spec:
  type: ClusterIP
  selector:
    app: app
  sessionAffinity: ClientIP  # Pega cliente a Pod
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 3600   # 1 hora
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
----

3. Despliega:
+
[source,bash]
----
kubectl apply -f affinity-deployment.yaml
kubectl apply -f affinity-service.yaml
----

4. Crea un Pod cliente y envía múltiples requests:
+
[source,bash]
----
kubectl run client --image=busybox:1.28 --rm -it --restart=Never -- sh
----

5. Dentro del cliente, envía requests:
+
[source,bash]
----
# Envía múltiples requests y observa que siempre va al mismo Pod
for i in {1..5}; do
  echo "Request $i:"
  wget -q -O- http://app-affinity
  echo ""
done

# Todos deberían mostrar el mismo "Pod: app-affinity-xxxx"
exit
----

6. Para comparar, crea un segundo cliente:
+
[source,bash]
----
kubectl run client2 --image=busybox:1.28 --rm -it --restart=Never -- sh
----

7. Dentro del segundo cliente:
+
[source,bash]
----
# Este cliente debería estar "pegado" a un Pod diferente
for i in {1..3}; do
  wget -q -O- http://app-affinity
  echo ""
done
exit
----

8. Limpia:
+
[source,bash]
----
kubectl delete deployment app-affinity
kubectl delete service app-affinity
----

**Verificación:**
El mismo cliente debería siempre ir al mismo Pod, pero clientes diferentes pueden ir a Pods diferentes.

**Preguntas de reflexión:**
1. ¿Por qué usar Session Affinity?
2. ¿Cuáles son los problemas potenciales?
3. ¿Cuáles son mejores alternativas?

---

=== Ejercicio 4.6: Multi-port Services

**Objetivo de aprendizaje:**
Entender cómo un Service puede exponer múltiples puertos simultáneamente.

**Descripción:**
Crearás un Service que expone múltiples puertos para diferentes protocolos/servicios.

**Tareas:**

1. Crea un Deployment que expone múltiples puertos (`multiport-deployment.yaml`):
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multiport-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: multiport
  template:
    metadata:
      labels:
        app: multiport
    spec:
      containers:
      - name: app
        image: nginx:1.21
        ports:
        - containerPort: 80
          name: http
        - containerPort: 443
          name: https
        - containerPort: 8080
          name: metrics
----

2. Crea un Service con múltiples puertos (`multiport-service.yaml`):
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: multiport
spec:
  type: ClusterIP
  selector:
    app: multiport
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
  - name: https
    protocol: TCP
    port: 443
    targetPort: 443
  - name: metrics
    protocol: TCP
    port: 9090
    targetPort: 8080
----

3. Despliega:
+
[source,bash]
----
kubectl apply -f multiport-deployment.yaml
kubectl apply -f multiport-service.yaml
----

4. Verifica el Service:
+
[source,bash]
----
kubectl get svc multiport
# Debería mostrar: 80/TCP, 443/TCP, 9090/TCP
----

5. Describe el Service:
+
[source,bash]
----
kubectl describe svc multiport
----

6. Crea un cliente y accede a diferentes puertos:
+
[source,bash]
----
kubectl run client --image=nicolaka/netshoot --rm -it --restart=Never -- bash
----

7. Dentro del cliente, prueba conectar a diferentes puertos:
+
[source,bash]
----
# Acceder al puerto http
curl -v http://multiport:80

# Acceder al puerto https (sin verificación)
curl -v -k https://multiport:443

# Acceder al puerto metrics
curl -v http://multiport:9090

# Resolver el Service
nslookup multiport

# Salir
exit
----

8. Accede por nombre de puerto:
+
[source,bash]
----
# Para Deployments que hacen referencia por nombre de puerto
# (típicamente usado en Istio/service mesh)
----

9. Limpia:
+
[source,bash]
----
kubectl delete deployment multiport-app
kubectl delete service multiport
----

**Verificación:**
El Service debería estar escuchando en múltiples puertos simultáneamente.

**Preguntas de reflexión:**
1. ¿Por qué nombrar los puertos?
2. ¿Cuándo usarías múltiples puertos en un Service?
3. ¿Cómo se relaciona con los nombres de puerto en Deployments?

---

=== Ejercicio 4.7: DNS en Kubernetes

**Objetivo de aprendizaje:**
Entender cómo funciona la resolución DNS de Services y Pods en Kubernetes.

**Descripción:**
Crearás Pods y Services y probarás la resolución DNS usando diferentes FQDNs.

**Tareas:**

1. Crea un Deployment y un Service:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dns-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dns-demo
  template:
    metadata:
      labels:
        app: dns-demo
    spec:
      containers:
      - name: app
        image: nginx:1.21
        ports:
        - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: dns-service
spec:
  type: ClusterIP
  selector:
    app: dns-demo
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
----

2. Despliega:
+
[source,bash]
----
kubectl apply -f dns-demo.yaml
----

3. Crea un Pod cliente con herramientas de DNS:
+
[source,bash]
----
kubectl run dns-client --image=nicolaka/netshoot --rm -it --restart=Never -- bash
----

4. Dentro del cliente, prueba diferentes tipos de resolución:
+
[source,bash]
----
# Ver configuración DNS del Pod
cat /etc/resolv.conf

# Resolver por nombre corto (mismo namespace)
nslookup dns-service

# Resolver por FQDN completo
nslookup dns-service.default.svc.cluster.local

# Resolver un Pod por nombre
POD_NAME=$(kubectl get pods -l app=dns-demo -o jsonpath='{.items[0].metadata.name}')
POD_IP=$(kubectl get pods -l app=dns-demo -o jsonpath='{.items[0].status.podIP}')

# Nota: Los Pods también tienen DNS si el dominio es correcto
nslookup $POD_NAME.default.pod.cluster.local

# Acceder por nombre
wget http://dns-service

# Salir
exit
----

5. Verifica el CoreDNS del cluster:
+
[source,bash]
----
kubectl get pods -n kube-system -l k8s-app=kube-dns
kubectl get svc -n kube-system | grep dns
----

6. Crea un Pod en un namespace diferente:
+
[source,bash]
----
kubectl create namespace test-ns
kubectl run test-app --image=nginx:1.21 --labels=app=test -n test-ns
kubectl expose pod test-app --port=80 -n test-ns
----

7. Desde el cliente original, accede al Service en otro namespace:
+
[source,bash]
----
kubectl run dns-client2 --image=nicolaka/netshoot --rm -it --restart=Never -- bash
----

8. Dentro del cliente2:
+
[source,bash]
----
# Nombre corto no funciona desde otro namespace
wget http://test-app

# FQDN completo funciona
wget http://test-app.test-ns.svc.cluster.local

# O explícitamente:
nslookup test-app.test-ns

exit
----

9. Limpia:
+
[source,bash]
----
kubectl delete deployment dns-demo
kubectl delete service dns-service
kubectl delete namespace test-ns
----

**Verificación:**
Deberías poder resolver Services por diferentes nombres (corto, FQDN).

**Preguntas de reflexión:**
1. ¿Cómo funciona la búsqueda DNS con search domains?
2. ¿Por qué necesitas FQDN para acceder a otro namespace?
3. ¿Cuál es el servicio DNS que proporciona Kubernetes?

---

=== Ejercicio 4.8: Ingress básico

**Objetivo de aprendizaje:**
Crear un Ingress básico para enrutar tráfico HTTP basado en hostname.

**Descripción:**
Crearás un Ingress que enruta tráfico de diferentes hostnames a diferentes Services.

**Tareas:**

1. Crea dos Deployments (`ingress-demo.yaml`):
+
[source,yaml]
----
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: app
        image: nginx:1.21
        ports:
        - containerPort: 80
        lifecycle:
          postStart:
            exec:
              command:
              - sh
              - -c
              - echo "Web Application" > /usr/share/nginx/html/index.html

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      containers:
      - name: app
        image: nginx:1.21
        ports:
        - containerPort: 80
        lifecycle:
          postStart:
            exec:
              command:
              - sh
              - -c
              - echo "API Application" > /usr/share/nginx/html/index.html

---
apiVersion: v1
kind: Service
metadata:
  name: web-service
spec:
  type: ClusterIP
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: api-service
spec:
  type: ClusterIP
  selector:
    app: api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
----

2. Crea un Ingress (`my-ingress.yaml`):
+
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: web.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
  - host: api.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 80
----

3. Despliega todo:
+
[source,bash]
----
kubectl apply -f ingress-demo.yaml
kubectl apply -f my-ingress.yaml
----

4. Verifica el Ingress:
+
[source,bash]
----
kubectl get ingress
kubectl describe ingress my-ingress
----

5. Obtén la IP del Ingress Controller:
+
[source,bash]
----
# Si tienes NGINX Ingress Controller instalado:
kubectl get svc -n ingress-nginx ingress-nginx-controller

# O usa port-forward:
kubectl port-forward svc/my-ingress 8080:80 &
----

6. Prueba acceso con curl:
+
[source,bash]
----
# Opción 1: Añade a /etc/hosts (si tienes IP real)
# 127.0.0.1 web.local
# 127.0.0.1 api.local

# Opción 2: Usa curl con Host header
curl -H "Host: web.local" http://localhost:8080
curl -H "Host: api.local" http://localhost:8080

# Opción 3: Usa port-forward a un Pod del Ingress Controller
----

7. Detén port-forward:
+
[source,bash]
----
pkill -f "port-forward"
----

8. Limpia:
+
[source,bash]
----
kubectl delete ingress my-ingress
kubectl delete -f ingress-demo.yaml
----

**Verificación:**
Diferentes hosts deberían enrutarse a diferentes Services.

**Preguntas de reflexión:**
1. ¿Qué es un Ingress Controller?
2. ¿Cuál es la diferencia entre Service e Ingress?
3. ¿Cómo funciona el enrutamiento basado en hostname?

---

=== Ejercicio 4.9: Ingress con path-based routing

**Objetivo de aprendizaje:**
Crear un Ingress que enruta basado en paths de URLs.

**Descripción:**
Crearás un Ingress que enruta diferentes paths a diferentes Services.

**Tareas:**

1. Crea múltiples Deployments y Services (`path-routing.yaml`):
+
[source,yaml]
----
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: home-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: home
  template:
    metadata:
      labels:
        app: home
    spec:
      containers:
      - name: app
        image: nginx:1.21
        ports:
        - containerPort: 80
        lifecycle:
          postStart:
            exec:
              command:
              - sh
              - -c
              - echo "Home Page" > /usr/share/nginx/html/index.html

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-v1-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: api-v1
  template:
    metadata:
      labels:
        app: api-v1
    spec:
      containers:
      - name: app
        image: nginx:1.21
        ports:
        - containerPort: 80
        lifecycle:
          postStart:
            exec:
              command:
              - sh
              - -c
              - echo "API v1" > /usr/share/nginx/html/index.html

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-v2-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: api-v2
  template:
    metadata:
      labels:
        app: api-v2
    spec:
      containers:
      - name: app
        image: nginx:1.21
        ports:
        - containerPort: 80
        lifecycle:
          postStart:
            exec:
              command:
              - sh
              - -c
              - echo "API v2" > /usr/share/nginx/html/index.html

---
apiVersion: v1
kind: Service
metadata:
  name: home-service
spec:
  selector:
    app: home
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: api-v1-service
spec:
  selector:
    app: api-v1
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: api-v2-service
spec:
  selector:
    app: api-v2
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
----

2. Crea un Ingress con path routing (`path-ingress.yaml`):
+
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: path-ingress
spec:
  rules:
  - host: example.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: home-service
            port:
              number: 80
      - path: /api/v1
        pathType: Prefix
        backend:
          service:
            name: api-v1-service
            port:
              number: 80
      - path: /api/v2
        pathType: Prefix
        backend:
          service:
            name: api-v2-service
            port:
              number: 80
----

3. Despliega:
+
[source,bash]
----
kubectl apply -f path-routing.yaml
kubectl apply -f path-ingress.yaml
----

4. Verifica el Ingress:
+
[source,bash]
----
kubectl get ingress path-ingress
kubectl describe ingress path-ingress
----

5. Prueba path-based routing:
+
[source,bash]
----
# Inicia port-forward
kubectl port-forward svc/home-service 8080:80 &

# Prueba diferentes paths
curl -H "Host: example.local" http://localhost:8080/
curl -H "Host: example.local" http://localhost:8080/api/v1
curl -H "Host: example.local" http://localhost:8080/api/v2

# Detén port-forward
pkill -f "port-forward"
----

6. Observa cómo diferentes paths van a diferentes Services:
+
[source,bash]
----
kubectl get endpoints home-service api-v1-service api-v2-service
----

7. Limpia:
+
[source,bash]
----
kubectl delete ingress path-ingress
kubectl delete -f path-routing.yaml
----

**Verificación:**
Diferentes paths deberían enrutarse a diferentes Services.

**Preguntas de reflexión:**
1. ¿Cómo funciona el path-based routing?
2. ¿Cuál es la diferencia entre `Prefix` y `Exact` pathType?
3. ¿Cuándo usarías path-based vs host-based routing?

---

=== Ejercicio 4.10: ExternalName Service

**Objetivo de aprendizaje:**
Usar ExternalName Service para acceder a servicios externos desde dentro del cluster.

**Descripción:**
Crearás un ExternalName Service que actúa como alias a un servicio externo.

**Tareas:**

1. Crea un ExternalName Service (`external-service.yaml`):
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: external-db
spec:
  type: ExternalName
  externalName: example.com
  ports:
  - protocol: TCP
    port: 443
----

2. Crea otro ExternalName para un DNS externo:
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: google-dns
spec:
  type: ExternalName
  externalName: 8.8.8.8
  ports:
  - protocol: TCP
    port: 53
----

3. Despliega los Services:
+
[source,bash]
----
kubectl apply -f external-service.yaml
----

4. Verifica los Services:
+
[source,bash]
----
kubectl get svc external-db google-dns
kubectl describe svc external-db
----

5. Crea un Pod cliente y prueba la resolución:
+
[source,bash]
----
kubectl run client --image=nicolaka/netshoot --rm -it --restart=Never -- bash
----

6. Dentro del cliente:
+
[source,bash]
----
# Resuelve el ExternalName Service
nslookup external-db

# Debería resolver a example.com

# Intenta acceder
curl -v https://external-db

# Prueba el otro Service
nslookup google-dns

# Salir
exit
----

7. Verifica que no hay endpoints locales:
+
[source,bash]
----
kubectl get endpoints external-db
# Debería estar vacío
----

8. Crea un caso de uso práctico - conectar a una base de datos externa:
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: production-db
spec:
  type: ExternalName
  externalName: db.production.example.com
  ports:
  - name: postgres
    protocol: TCP
    port: 5432
    targetPort: 5432
----

9. Un Deployment podría usar este Service:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name: app
        image: myapp:latest
        env:
        - name: DATABASE_HOST
          value: production-db.default.svc.cluster.local
        - name: DATABASE_PORT
          value: "5432"
----

10. Limpia:
+
[source,bash]
----
kubectl delete service external-db google-dns production-db
----

**Verificación:**
El ExternalName Service debería actuar como CNAME a un servicio externo.

**Preguntas de reflexión:**
1. ¿Cuándo usarías ExternalName?
2. ¿Cómo se diferencia de otros tipos de Services?
3. ¿Qué ventajas proporciona al usar ExternalName?

---

== Resumen de Conceptos

Estos ejercicios del Módulo 4 cubren:

* **Ejercicio 4.1**: Comunicación Pod-to-Pod directa
* **Ejercicio 4.2**: Service ClusterIP básico
* **Ejercicio 4.3**: Service NodePort
* **Ejercicio 4.4**: Load balancing y distribución
* **Ejercicio 4.5**: Session Affinity
* **Ejercicio 4.6**: Multi-port Services
* **Ejercicio 4.7**: DNS en Kubernetes
* **Ejercicio 4.8**: Ingress básico
* **Ejercicio 4.9**: Ingress path-based routing
* **Ejercicio 4.10**: ExternalName Service

== Próximos Pasos

Una vez completes estos ejercicios, estarás listo para:
* Módulo 5: Almacenamiento persistente (PersistentVolumes, PersistentVolumeClaims)
* Módulo 6: RBAC y seguridad
* Módulo 7: Networking avanzado
* Y módulos posteriores...

---

== Módulo 5: Almacenamiento

=== Ejercicio 5.1: Pod Multi-Contenedor con emptyDir

**Objetivo:** Compartir datos entre contenedores usando volúmenes emptyDir.

**Descripción:**

Crea un Pod con dos contenedores que comparten un volumen emptyDir. Un contenedor escribe datos periódicamente, y el otro los lee.

**Tareas:**

1. Crea un archivo `multi-container-pod.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: data-sharer
  namespace: default
spec:
  containers:
  # Contenedor que escribe datos
  - name: writer
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      counter=0
      while true; do
        counter=$((counter+1))
        echo "[$(date '+%H:%M:%S')] Generated data $counter" >> /shared/data.txt
        sleep 3
      done
    volumeMounts:
    - name: shared
      mountPath: /shared
  # Contenedor que lee datos
  - name: reader
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      sleep 2
      while true; do
        echo "=== Data Available ==="
        cat /shared/data.txt
        echo ""
        sleep 5
      done
    volumeMounts:
    - name: shared
      mountPath: /shared
      readOnly: true
  # Definición del volumen compartido
  volumes:
  - name: shared
    emptyDir:
      sizeLimit: 10Mi
----

2. Aplica el Pod:
+
[source,bash]
----
kubectl apply -f multi-container-pod.yaml
----

3. Verifica que el Pod está corriendo:
+
[source,bash]
----
kubectl get pods -w
----

4. Observa los logs del contenedor writer:
+
[source,bash]
----
kubectl logs data-sharer -c writer --tail=10 -f
----

5. Observa los logs del contenedor reader:
+
[source,bash]
----
kubectl logs data-sharer -c reader --tail=10 -f
----

6. Verifica el volumen dentro del Pod:
+
[source,bash]
----
kubectl exec -it data-sharer -c reader -- ls -lh /shared
----

7. Limpia:
+
[source,bash]
----
kubectl delete pod data-sharer
----

**Verificación:**

El contenedor reader debería mostrar datos actualizados del writer mediante el volumen compartido.

**Preguntas de reflexión:**

1. ¿Qué ocurre con los datos del volumen cuando se elimina el Pod?
2. ¿Cuál es el propósito del readOnly: true en el contenedor reader?
3. ¿Cómo cambiaría el ejercicio si necesitaras persistencia después de eliminar el Pod?

---

=== Ejercicio 5.2: Acceso a Sistema Host con hostPath

**Objetivo:** Usar volúmenes hostPath para acceder a datos del nodo.

**Descripción:**

Crea un Pod que acceda a información del sistema del nodo host mediante volúmenes hostPath.

**Tareas:**

1. Crea un archivo `hostpath-pod.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: system-monitor
  namespace: default
spec:
  containers:
  - name: monitor
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Host Information ==="
      echo "Hostname: $(cat /host-etc/hostname)"
      echo ""
      echo "=== CPU Info (first 10 lines) ==="
      head -10 /host-proc/cpuinfo
      echo ""
      echo "=== Memory Info ==="
      grep -E "MemTotal|MemAvailable" /host-proc/meminfo
      echo ""
      echo "Monitoring every 10 seconds..."
      while true; do
        echo "[$(date '+%H:%M:%S')] System uptime:"
        uptime
        sleep 10
      done
    volumeMounts:
    - name: host-proc
      mountPath: /host-proc
      readOnly: true
    - name: host-etc
      mountPath: /host-etc
      readOnly: true
    - name: host-sys
      mountPath: /host-sys
      readOnly: true
  volumes:
  - name: host-proc
    hostPath:
      path: /proc
      type: Directory
  - name: host-etc
    hostPath:
      path: /etc
      type: Directory
  - name: host-sys
    hostPath:
      path: /sys
      type: Directory
----

2. Aplica el Pod:
+
[source,bash]
----
kubectl apply -f hostpath-pod.yaml
----

3. Verifica que el Pod está corriendo:
+
[source,bash]
----
kubectl get pod system-monitor
----

4. Observa los logs del Pod:
+
[source,bash]
----
kubectl logs system-monitor -f
----

5. Accede al contenedor interactivamente:
+
[source,bash]
----
kubectl exec -it system-monitor -- sh
# Dentro del contenedor:
# ls -la /host-proc/
# cat /host-etc/hostname
# exit
----

6. Verifica en qué nodo está ejecutándose el Pod:
+
[source,bash]
----
kubectl get pod system-monitor -o wide
----

7. Limpia:
+
[source,bash]
----
kubectl delete pod system-monitor
----

**Verificación:**

El Pod debe mostrar información real del nodo host (CPU, memoria, hostname).

**Preguntas de reflexión:**

1. ¿Por qué es importante usar readOnly: true con hostPath?
2. ¿Qué riesgos de seguridad presenta hostPath?
3. ¿Cuándo usarías hostPath en lugar de otros tipos de volúmenes?

---

=== Ejercicio 5.3: ConfigMap como Volumen

**Objetivo:** Inyectar archivos de configuración usando ConfigMaps como volúmenes.

**Descripción:**

Crea una aplicación que lee su configuración desde archivos inyectados por un ConfigMap montado como volumen.

**Tareas:**

1. Crea un ConfigMap con archivos de configuración:
+
[source,bash]
----
kubectl create configmap app-config \
  --from-literal=app.properties="APP_NAME=MyApp
APP_VERSION=1.0
LOG_LEVEL=INFO
DATABASE_POOL_SIZE=20" \
  --from-literal=nginx.conf="server {
    listen 8080;
    location / {
      return 200 'App is running';
    }
  }"
----

2. Verifica que el ConfigMap fue creado:
+
[source,bash]
----
kubectl get configmap app-config -o yaml
----

3. Crea un archivo `app-with-config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-config
  namespace: default
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Application Configuration ==="
      echo ""
      echo "--- app.properties ---"
      cat /etc/config/app.properties
      echo ""
      echo "--- nginx.conf ---"
      cat /etc/config/nginx.conf
      echo ""
      echo "Configuration loaded successfully!"
      sleep infinity
    volumeMounts:
    - name: config
      mountPath: /etc/config
      readOnly: true
  volumes:
  - name: config
    configMap:
      name: app-config
      defaultMode: 0444  # Permisos de solo lectura
----

4. Aplica el Pod:
+
[source,bash]
----
kubectl apply -f app-with-config.yaml
----

5. Verifica los logs:
+
[source,bash]
----
kubectl logs app-with-config
----

6. Accede al contenedor y verifica los archivos:
+
[source,bash]
----
kubectl exec -it app-with-config -- ls -la /etc/config/
kubectl exec -it app-with-config -- cat /etc/config/app.properties
----

7. Modifica el ConfigMap:
+
[source,bash]
----
kubectl patch configmap app-config -p '{"data":{"app.properties":"APP_NAME=MyApp\nAPP_VERSION=1.1\nLOG_LEVEL=DEBUG"}}'
----

8. Verifica que el archivo se actualizó en el Pod (puede tomar algunos segundos):
+
[source,bash]
----
kubectl exec -it app-with-config -- cat /etc/config/app.properties
----

9. Limpia:
+
[source,bash]
----
kubectl delete pod app-with-config
kubectl delete configmap app-config
----

**Verificación:**

El Pod debe mostrar la configuración del ConfigMap montada como archivos en `/etc/config/`.

**Preguntas de reflexión:**

1. ¿Cuál es la ventaja de usar ConfigMap como volumen vs. variables de entorno?
2. ¿Cuánto tiempo tarda Kubernetes en actualizar un archivo montado desde ConfigMap?
3. ¿Por qué es importante el parámetro defaultMode?

---

=== Ejercicio 5.4: Secret como Volumen

**Objetivo:** Montar secretos como volúmenes para acceso seguro a credenciales.

**Descripción:**

Crea una aplicación que lee credenciales de base de datos desde un Secret montado como volumen.

**Tareas:**

1. Crea un Secret con credenciales:
+
[source,bash]
----
kubectl create secret generic db-credentials \
  --from-literal=username=admin \
  --from-literal=password=P@ssw0rd123! \
  --from-literal=host=postgres.default.svc.cluster.local \
  --from-literal=port=5432
----

2. Verifica el Secret (sin ver valores):
+
[source,bash]
----
kubectl get secret db-credentials
----

3. Crea un archivo `app-with-secrets.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: db-app
  namespace: default
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Database Credentials ==="
      echo "Username: $(cat /etc/db-secrets/username)"
      echo "Host: $(cat /etc/db-secrets/host)"
      echo "Port: $(cat /etc/db-secrets/port)"
      echo ""
      echo "=== Testing Database Connection ==="
      DB_USER=$(cat /etc/db-secrets/username)
      DB_PASS=$(cat /etc/db-secrets/password)
      DB_HOST=$(cat /etc/db-secrets/host)
      DB_PORT=$(cat /etc/db-secrets/port)
      echo "Would connect to: $DB_HOST:$DB_PORT as $DB_USER"
      echo ""
      echo "File permissions (secure):"
      ls -la /etc/db-secrets/
      echo ""
      sleep infinity
    volumeMounts:
    - name: db-secret
      mountPath: /etc/db-secrets
      readOnly: true
  volumes:
  - name: db-secret
    secret:
      secretName: db-credentials
      defaultMode: 0400  # Solo lectura, solo para owner
----

4. Aplica el Pod:
+
[source,bash]
----
kubectl apply -f app-with-secrets.yaml
----

5. Verifica los logs:
+
[source,bash]
----
kubectl logs db-app
----

6. Accede al contenedor y verifica las credenciales:
+
[source,bash]
----
kubectl exec -it db-app -- cat /etc/db-secrets/username
kubectl exec -it db-app -- cat /etc/db-secrets/password
----

7. Verifica que el archivo tiene permisos restrictivos:
+
[source,bash]
----
kubectl exec -it db-app -- ls -la /etc/db-secrets/
----

8. Intenta sobrescribir el archivo (debe fallar):
+
[source,bash]
----
kubectl exec -it db-app -- sh -c 'echo newpass > /etc/db-secrets/password'
# Debería fallar: Read-only file system
----

9. Limpia:
+
[source,bash]
----
kubectl delete pod db-app
kubectl delete secret db-credentials
----

**Verificación:**

El Pod debe mostrar las credenciales del Secret y los archivos deben ser de solo lectura.

**Preguntas de reflexión:**

1. ¿Por qué se debe usar readOnly: true con Secrets?
2. ¿Cuál es la diferencia de seguridad entre inyectar credenciales via Secret vs. ConfigMap?
3. ¿Cómo se sincroniza el Secret cuando se modifica?

---

=== Ejercicio 5.5: Creando PersistentVolume y PersistentVolumeClaim

**Objetivo:** Crear y usar PersistentVolumes con PersistentVolumeClaims.

**Descripción:**

Crea un PersistentVolume usando almacenamiento local, luego solicita una porción con PersistentVolumeClaim y úsalo en un Pod.

**Tareas:**

1. Crea un directorio en el nodo para simular almacenamiento:
+
[source,bash]
----
# Crear directorio (como si fuera administrador del cluster)
sudo mkdir -p /mnt/data/pv1
sudo chmod 777 /mnt/data/pv1
echo "Kubernetes Storage Data" | sudo tee /mnt/data/pv1/README.txt
----

2. Crea un archivo `pv-pvc.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-1
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /mnt/data/pv1
    type: DirectoryOrCreate
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
  namespace: default
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: ""  # No StorageClass, usar PV manual
  resources:
    requests:
      storage: 2Gi
----

3. Aplica los recursos:
+
[source,bash]
----
kubectl apply -f pv-pvc.yaml
----

4. Verifica el PersistentVolume:
+
[source,bash]
----
kubectl get pv
kubectl describe pv local-pv-1
----

5. Verifica el PersistentVolumeClaim:
+
[source,bash]
----
kubectl get pvc
kubectl describe pvc my-pvc
# Debería estar "Bound" a local-pv-1
----

6. Crea un Pod que use la PVC:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: storage-consumer
  namespace: default
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Persistent Volume Information ==="
      echo "Mounted at: /data"
      ls -la /data
      echo ""
      echo "=== Writing persistent data ==="
      echo "This data persists across Pod restarts!" > /data/persistent-file.txt
      cat /data/persistent-file.txt
      echo ""
      echo "Pod ID: $HOSTNAME" >> /data/pod-ids.log
      echo "Time: $(date)" >> /data/pod-ids.log
      echo ""
      echo "Pod is running..."
      sleep infinity
    volumeMounts:
    - name: storage
      mountPath: /data
  volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: my-pvc
----

7. Aplica el Pod:
+
[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: storage-consumer
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "First write..."
      echo "Data from $(date)" > /data/persistent-file.txt
      cat /data/persistent-file.txt
      sleep infinity
    volumeMounts:
    - name: storage
      mountPath: /data
  volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: my-pvc
EOF
----

8. Verifica los datos:
+
[source,bash]
----
kubectl exec -it storage-consumer -- cat /data/persistent-file.txt
----

9. Elimina el Pod:
+
[source,bash]
----
kubectl delete pod storage-consumer
----

10. Crea un nuevo Pod que use la misma PVC:
+
[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: storage-consumer-2
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Data from previous Pod ==="
      cat /data/persistent-file.txt
      echo ""
      echo "Pod running as: $HOSTNAME"
      sleep infinity
    volumeMounts:
    - name: storage
      mountPath: /data
  volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: my-pvc
EOF
----

11. Verifica que los datos persisten:
+
[source,bash]
----
kubectl exec -it storage-consumer-2 -- cat /data/persistent-file.txt
----

12. Limpia:
+
[source,bash]
----
kubectl delete pod storage-consumer-2
kubectl delete pvc my-pvc
kubectl delete pv local-pv-1
----

**Verificación:**

Los datos en el PersistentVolume deben persister después de eliminar el Pod original.

**Preguntas de reflexión:**

1. ¿Cuál es la diferencia entre PV y volumen normal?
2. ¿Qué ocurre con el PV cuando se elimina la PVC?
3. ¿Por qué se usa persistentVolumeReclaimPolicy: Retain?

---

=== Ejercicio 5.6: StorageClass y Dynamic Provisioning

**Objetivo:** Crear un StorageClass para aprovisionamiento dinámico de almacenamiento.

**Descripción:**

Crea un StorageClass que provisione automáticamente PersistentVolumes cuando se crea una PVC.

**Tareas:**

1. Crea un archivo `storageclass.yaml`:
+
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner  # Para almacenamiento local
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
----

2. Aplica el StorageClass:
+
[source,bash]
----
kubectl apply -f storageclass.yaml
----

3. Verifica el StorageClass:
+
[source,bash]
----
kubectl get storageclass
----

4. Crea una PVC que use el StorageClass:
+
[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dynamic-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 3Gi
EOF
----

5. Verifica el estado de la PVC:
+
[source,bash]
----
kubectl get pvc dynamic-pvc
kubectl describe pvc dynamic-pvc
# Puede estar en Pending si no hay PV disponible
----

6. Crea un PV que matchee con la PVC:
+
[source,bash]
----
sudo mkdir -p /mnt/data/pv2
sudo chmod 777 /mnt/data/pv2

kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-2
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  hostPath:
    path: /mnt/data/pv2
    type: DirectoryOrCreate
EOF
----

7. Verifica que la PVC se vinculó:
+
[source,bash]
----
kubectl get pvc dynamic-pvc
# Debería estar "Bound"
----

8. Crea un Pod que use la PVC:
+
[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: dynamic-storage-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "Storage class: local-storage"
      echo "Mounted PVC: dynamic-pvc"
      echo "Writing dynamic data..."
      echo "Dynamic provisioning works!" > /storage/data.txt
      cat /storage/data.txt
      sleep infinity
    volumeMounts:
    - name: storage
      mountPath: /storage
  volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: dynamic-pvc
EOF
----

9. Verifica el Pod:
+
[source,bash]
----
kubectl get pods
kubectl logs dynamic-storage-pod
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod dynamic-storage-pod
kubectl delete pvc dynamic-pvc
kubectl delete pv local-pv-2
kubectl delete storageclass local-storage
----

**Verificación:**

El Pod debe montarse exitosamente usando la PVC creada con el StorageClass.

**Preguntas de reflexión:**

1. ¿Cuál es la ventaja de usar StorageClass vs. crear PVs manualmente?
2. ¿Qué es volumeBindingMode: WaitForFirstConsumer?
3. ¿Cuándo usarías provisioner: kubernetes.io/no-provisioner vs. un provisioner cloud?

---

=== Ejercicio 5.7: PVC con Múltiples Contenedores

**Objetivo:** Compartir una PVC entre múltiples contenedores en el mismo Pod.

**Descripción:**

Crea un Pod con múltiples contenedores que comparten el mismo PersistentVolumeClaim.

**Tareas:**

1. Crea una PVC:
+
[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-storage
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
EOF
----

2. Crea un PV para la PVC (si es necesario):
+
[source,bash]
----
sudo mkdir -p /mnt/data/pv3
sudo chmod 777 /mnt/data/pv3

kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-3
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  hostPath:
    path: /mnt/data/pv3
    type: DirectoryOrCreate
EOF
----

3. Crea un Pod con múltiples contenedores usando la misma PVC:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-storage
spec:
  containers:
  # Contenedor que escribe en storage
  - name: writer
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      counter=1
      while true; do
        echo "[$(date '+%H:%M:%S')] Event $counter" >> /shared/events.log
        counter=$((counter+1))
        sleep 2
      done
    volumeMounts:
    - name: shared
      mountPath: /shared
  # Contenedor que lee del storage
  - name: reader
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      sleep 1
      while true; do
        echo "=== Events Log ==="
        tail -5 /shared/events.log
        sleep 5
      done
    volumeMounts:
    - name: shared
      mountPath: /shared
      readOnly: true
  # Contenedor que analiza el storage
  - name: analyzer
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      sleep 2
      while true; do
        echo "=== Log Analysis ==="
        wc -l /shared/events.log
        echo "Storage usage:"
        du -sh /shared/
        sleep 10
      done
    volumeMounts:
    - name: shared
      mountPath: /shared
      readOnly: true
  # Volumen compartido
  volumes:
  - name: shared
    persistentVolumeClaim:
      claimName: shared-storage
----

4. Aplica el Pod:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-storage
spec:
  containers:
  - name: writer
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      counter=1
      while true; do
        echo "[$(date '+%H:%M:%S')] Event $counter" >> /shared/events.log
        counter=$((counter+1))
        sleep 2
      done
    volumeMounts:
    - name: shared
      mountPath: /shared
  - name: reader
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      sleep 1
      while true; do
        echo "=== Events Log ==="
        tail -5 /shared/events.log
        sleep 5
      done
    volumeMounts:
    - name: shared
      mountPath: /shared
      readOnly: true
  volumes:
  - name: shared
    persistentVolumeClaim:
      claimName: shared-storage
EOF
----

5. Verifica el Pod:
+
[source,bash]
----
kubectl get pod multi-container-storage
----

6. Observa los logs del writer:
+
[source,bash]
----
kubectl logs multi-container-storage -c writer --tail=5
----

7. Observa los logs del reader:
+
[source,bash]
----
kubectl logs multi-container-storage -c reader --tail=5
----

8. Verifica el contenido del storage:
+
[source,bash]
----
kubectl exec -it multi-container-storage -c reader -- cat /shared/events.log
----

9. Limpia:
+
[source,bash]
----
kubectl delete pod multi-container-storage
kubectl delete pvc shared-storage
kubectl delete pv local-pv-3
----

**Verificación:**

Todos los contenedores deben acceder y compartir datos correctamente a través de la PVC.

**Preguntas de reflexión:**

1. ¿Por qué todos los contenedores pueden usar la misma PVC?
2. ¿Cuál es la diferencia entre emptyDir y PVC para este caso de uso?
3. ¿Qué ocurriría si cambiamos accessModes a ReadOnlyMany?

---

=== Ejercicio 5.8: StatefulSet con volumeClaimTemplates

**Objetivo:** Crear un StatefulSet que aprovisione automáticamente PVCs para cada Pod.

**Descripción:**

Crea un StatefulSet que genere automáticamente PersistentVolumeClaims para cada réplica usando volumeClaimTemplates.

**Tareas:**

1. Crea un StorageClass (si no existe):
+
[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: stateful-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
EOF
----

2. Crea un Headless Service para el StatefulSet:
+
[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: mysql-db
spec:
  clusterIP: None  # Headless Service
  selector:
    app: mysql
  ports:
  - port: 3306
    targetPort: 3306
EOF
----

3. Crea el StatefulSet:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-db
spec:
  serviceName: mysql-db
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: busybox  # Simulamos MySQL con busybox
        command: ["sh", "-c"]
        args:
        - |
          echo "MySQL Replica: $HOSTNAME" > /var/lib/mysql/identity.txt
          echo "Started at: $(date)" >> /var/lib/mysql/identity.txt
          echo ""
          echo "Storage mounted at /var/lib/mysql"
          echo "Displaying identity:"
          cat /var/lib/mysql/identity.txt
          sleep infinity
        volumeMounts:
        - name: mysql-data
          mountPath: /var/lib/mysql
        ports:
        - containerPort: 3306
  # Templates de PVC automáticas
  volumeClaimTemplates:
  - metadata:
      name: mysql-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: stateful-storage
      resources:
        requests:
          storage: 2Gi
EOF
----

4. Verifica el StatefulSet:
+
[source,bash]
----
kubectl get statefulset mysql-db
----

5. Verifica que se crearon los Pods en orden:
+
[source,bash]
----
kubectl get pods -l app=mysql -w
----

6. Verifica las PVCs creadas automáticamente:
+
[source,bash]
----
kubectl get pvc
# Debería mostrar: mysql-data-mysql-db-0, mysql-data-mysql-db-1, mysql-data-mysql-db-2
----

7. Verifica los datos en cada Pod:
+
[source,bash]
----
kubectl logs mysql-db-0
kubectl logs mysql-db-1
----

8. Accede a un Pod y verifica su almacenamiento:
+
[source,bash]
----
kubectl exec -it mysql-db-0 -- cat /var/lib/mysql/identity.txt
----

9. Elimina un Pod y verifica que mantiene su volumen:
+
[source,bash]
----
kubectl delete pod mysql-db-0
# El StatefulSet lo recrea
kubectl get pods -l app=mysql
# Espera a que mysql-db-0 esté Running
kubectl exec -it mysql-db-0 -- cat /var/lib/mysql/identity.txt
# Los datos deberían ser los mismos
----

10. Limpia:
+
[source,bash]
----
kubectl delete statefulset mysql-db
kubectl delete service mysql-db
kubectl delete storageclass stateful-storage
----

**Verificación:**

Cada Pod del StatefulSet debe tener su propio PVC automáticamente creado con datos persistentes.

**Preguntas de reflexión:**

1. ¿Cuándo se crea la PVC en un StatefulSet?
2. ¿Por qué es importante usar Headless Services con StatefulSets?
3. ¿Qué ocurre con los PVCs cuando se elimina el StatefulSet?

---

=== Ejercicio 5.9: Expansión de Volúmenes

**Objetivo:** Expandir dinámicamente el tamaño de un PersistentVolumeClaim.

**Descripción:**

Crea una PVC y luego expande su capacidad dinámicamente sin eliminar el Pod.

**Tareas:**

1. Crea un StorageClass con allowVolumeExpansion:
+
[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: expandable-storage
provisioner: kubernetes.io/no-provisioner
allowVolumeExpansion: true
EOF
----

2. Crea una PVC inicial de 1Gi:
+
[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: expandable-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: expandable-storage
  resources:
    requests:
      storage: 1Gi
EOF
----

3. Crea un PV que soporte esta PVC:
+
[source,bash]
----
sudo mkdir -p /mnt/data/pv4
sudo chmod 777 /mnt/data/pv4

kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-4
spec:
  capacity:
    storage: 10Gi  # Suficientemente grande para expansión
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: expandable-storage
  hostPath:
    path: /mnt/data/pv4
    type: DirectoryOrCreate
EOF
----

4. Verifica la PVC:
+
[source,bash]
----
kubectl get pvc expandable-pvc
kubectl describe pvc expandable-pvc
----

5. Crea un Pod que use la PVC:
+
[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: expansion-test
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "Initial size: 1Gi"
      df -h /data | head -2
      echo ""
      echo "Waiting for expansion..."
      sleep infinity
    volumeMounts:
    - name: storage
      mountPath: /data
  volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: expandable-pvc
EOF
----

6. Verifica el tamaño actual:
+
[source,bash]
----
kubectl exec -it expansion-test -- df -h /data
----

7. Expande la PVC de 1Gi a 2Gi:
+
[source,bash]
----
kubectl patch pvc expandable-pvc -p '{"spec":{"resources":{"requests":{"storage":"2Gi"}}}}'
----

8. Verifica que la expansión comenzó:
+
[source,bash]
----
kubectl get pvc expandable-pvc
kubectl describe pvc expandable-pvc
# Debería mostrar: "Persistently Expanded"
----

9. Espera a que se complete y verifica el tamaño:
+
[source,bash]
----
sleep 3
kubectl exec -it expansion-test -- df -h /data
----

10. Expande nuevamente a 5Gi:
+
[source,bash]
----
kubectl patch pvc expandable-pvc -p '{"spec":{"resources":{"requests":{"storage":"5Gi"}}}}'
kubectl describe pvc expandable-pvc
kubectl exec -it expansion-test -- df -h /data
----

11. Limpia:
+
[source,bash]
----
kubectl delete pod expansion-test
kubectl delete pvc expandable-pvc
kubectl delete pv local-pv-4
kubectl delete storageclass expandable-storage
----

**Verificación:**

El volumen debe expandirse sin interrumpir el Pod.

**Preguntas de reflexión:**

1. ¿Por qué allowVolumeExpansion debe ser true en el StorageClass?
2. ¿Puedes reducir el tamaño de un volumen?
3. ¿Cuál es la ventaja de poder expandir volúmenes sin eliminar Pods?

---

=== Ejercicio 5.10: subPath para Múltiples Montes

**Objetivo:** Montar diferentes subcarpetas del mismo volumen en diferentes rutas del contenedor.

**Descripción:**

Crea un Pod que monta diferentes subcarpetas de un mismo volumen en diferentes rutas usando subPath.

**Tareas:**

1. Crea una PVC:
+
[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: multi-mount-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
EOF
----

2. Crea un PV:
+
[source,bash]
----
sudo mkdir -p /mnt/data/pv5
sudo chmod 777 /mnt/data/pv5

kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-5
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  hostPath:
    path: /mnt/data/pv5
    type: DirectoryOrCreate
EOF
----

3. Crea un Pod con múltiples subPath mounts:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: subpath-pod
spec:
  initContainers:
  # Init container para preparar la estructura
  - name: setup
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      mkdir -p /setup/logs /setup/cache /setup/config
      echo "Logs directory" > /setup/logs/README.txt
      echo "Cache directory" > /setup/cache/README.txt
      echo "Config directory" > /setup/config/README.txt
    volumeMounts:
    - name: storage
      mountPath: /setup
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Application Storage Structure ==="
      echo ""
      echo "--- /app/logs ---"
      ls -la /app/logs
      echo ""
      echo "--- /app/cache ---"
      ls -la /app/cache
      echo ""
      echo "--- /app/config ---"
      ls -la /app/config
      echo ""
      echo "Writing to different paths..."
      echo "Log entry 1" > /app/logs/app.log
      echo "Cache data" > /app/cache/session.cache
      echo "App config" > /app/config/app.ini
      echo ""
      echo "Verifying files:"
      echo "Logs:"
      cat /app/logs/app.log
      echo "Cache:"
      cat /app/cache/session.cache
      echo "Config:"
      cat /app/config/app.ini
      sleep infinity
    volumeMounts:
    # Múltiples subPath mounts del mismo volumen
    - name: storage
      mountPath: /app/logs
      subPath: logs
    - name: storage
      mountPath: /app/cache
      subPath: cache
    - name: storage
      mountPath: /app/config
      subPath: config
  volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: multi-mount-pvc
EOF
----

4. Verifica el Pod:
+
[source,bash]
----
kubectl get pod subpath-pod
kubectl logs subpath-pod
----

5. Verifica que los archivos están en el lugar correcto:
+
[source,bash]
----
kubectl exec -it subpath-pod -- ls -la /app/logs
kubectl exec -it subpath-pod -- cat /app/logs/app.log
----

6. Verifica la estructura en el volumen físico:
+
[source,bash]
----
ls -la /mnt/data/pv5/
ls -la /mnt/data/pv5/logs/
----

7. Crea un segundo Pod con la misma PVC para compartir datos:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: subpath-reader
spec:
  containers:
  - name: reader
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      sleep 2
      echo "=== Reading shared data ==="
      echo "From logs:"
      cat /data/logs/app.log
      echo "From cache:"
      cat /data/cache/session.cache
      echo "From config:"
      cat /data/config/app.ini
      sleep infinity
    volumeMounts:
    - name: shared
      mountPath: /data/logs
      subPath: logs
      readOnly: true
    - name: shared
      mountPath: /data/cache
      subPath: cache
      readOnly: true
    - name: shared
      mountPath: /data/config
      subPath: config
      readOnly: true
  volumes:
  - name: shared
    persistentVolumeClaim:
      claimName: multi-mount-pvc
EOF
----

8. Verifica que el reader puede acceder a los datos:
+
[source,bash]
----
kubectl logs subpath-reader
----

9. Limpia:
+
[source,bash]
----
kubectl delete pod subpath-pod
kubectl delete pod subpath-reader
kubectl delete pvc multi-mount-pvc
kubectl delete pv local-pv-5
----

**Verificación:**

Ambos Pods deben poder acceder a diferentes partes del mismo volumen mediante subPath.

**Preguntas de reflexión:**

1. ¿Cuál es la ventaja de usar subPath vs. crear múltiples PVCs?
2. ¿Por qué es importante el init container en este ejercicio?
3. ¿Cómo podrías usar subPath en una aplicación real?

---

== Resumen de Conceptos

Estos ejercicios del Módulo 5 cubren:

* **Ejercicio 5.1**: emptyDir para compartir datos entre contenedores
* **Ejercicio 5.2**: hostPath para acceso al sistema host
* **Ejercicio 5.3**: ConfigMaps montados como volúmenes
* **Ejercicio 5.4**: Secrets montados como volúmenes
* **Ejercicio 5.5**: PersistentVolumes y PersistentVolumeClaims
* **Ejercicio 5.6**: StorageClass y aprovisionamiento dinámico
* **Ejercicio 5.7**: PVCs compartidas entre contenedores
* **Ejercicio 5.8**: StatefulSets con volumeClaimTemplates
* **Ejercicio 5.9**: Expansión dinámica de volúmenes
* **Ejercicio 5.10**: subPath para múltiples montes

== Próximos Pasos

Una vez completes estos ejercicios, estarás listo para:
* Módulo 6: Configuración y Secrets
* Módulo 7: Seguridad
* Módulo 8: Observabilidad
* Y módulos posteriores...

---

== Módulo 6: Configuración y Secrets

=== Ejercicio 6.1: ConfigMap Básico con Literales

**Objetivo:** Crear un ConfigMap con datos literales e inyectarlos en un Pod.

**Descripción:**

Crea un ConfigMap que almacene parámetros de configuración y úsalos en un Pod mediante variables de entorno.

**Tareas:**

1. Crea un ConfigMap con literales:
+
[source,bash]
----
kubectl create configmap app-config \
  --from-literal=app.name=MyApp \
  --from-literal=app.version=1.0.0 \
  --from-literal=database.host=localhost \
  --from-literal=database.port=5432 \
  --from-literal=log.level=INFO
----

2. Verifica que el ConfigMap fue creado:
+
[source,bash]
----
kubectl get configmap app-config
kubectl get configmap app-config -o yaml
----

3. Crea un Pod que use el ConfigMap:
+
[source,bash]
----
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: app-with-config
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Application Configuration ==="
      echo "App Name: \$APP_NAME"
      echo "App Version: \$APP_VERSION"
      echo "Database Host: \$DB_HOST"
      echo "Database Port: \$DB_PORT"
      echo "Log Level: \$LOG_LEVEL"
      sleep infinity
    env:
    - name: APP_NAME
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: app.name
    - name: APP_VERSION
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: app.version
    - name: DB_HOST
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: database.host
    - name: DB_PORT
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: database.port
    - name: LOG_LEVEL
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: log.level
EOF
----

4. Verifica que el Pod está corriendo:
+
[source,bash]
----
kubectl get pod app-with-config
----

5. Observa los logs del Pod:
+
[source,bash]
----
kubectl logs app-with-config
----

6. Accede al contenedor y verifica las variables:
+
[source,bash]
----
kubectl exec -it app-with-config -- env | grep -E "APP|DB|LOG"
----

7. Modifica el ConfigMap:
+
[source,bash]
----
kubectl patch configmap app-config -p '{"data":{"app.version":"1.1.0"}}'
----

8. Verifica que la variable no se actualizó (sin reiniciar el Pod):
+
[source,bash]
----
kubectl logs app-with-config
# Debería mostrar versión antigua 1.0.0
----

9. Reinicia el Pod para obtener nuevos valores:
+
[source,bash]
----
kubectl delete pod app-with-config
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: app-with-config
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Application Configuration ==="
      echo "App Version: \$APP_VERSION"
      sleep infinity
    env:
    - name: APP_VERSION
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: app.version
EOF
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod app-with-config
kubectl delete configmap app-config
----

**Verificación:**

El Pod debe mostrar los valores del ConfigMap inyectados como variables de entorno.

**Preguntas de reflexión:**

1. ¿Por qué no se actualizaron automáticamente las variables cuando modificaste el ConfigMap?
2. ¿Cuál es la diferencia entre usar literales vs. archivos?
3. ¿Cómo podrías hacer que los cambios se propaguen automáticamente?

---

=== Ejercicio 6.2: ConfigMap desde Archivos

**Objetivo:** Crear un ConfigMap desde archivos de configuración.

**Descripción:**

Crea archivos de configuración y úsalos para crear un ConfigMap que se monte como volumen en un Pod.

**Tareas:**

1. Crea archivos de configuración:
+
[source,bash]
----
mkdir -p /tmp/config
cat > /tmp/config/app.properties <<'EOF'
server.port=8080
server.host=0.0.0.0
app.name=myapp
app.debug=false
EOF

cat > /tmp/config/database.yaml <<'EOF'
host: db-server
port: 5432
username: appuser
database: mydb
pool_size: 20
EOF

cat > /tmp/config/nginx.conf <<'EOF'
server {
  listen 80;
  server_name _;
  location / {
    proxy_pass http://localhost:8080;
  }
}
EOF
----

2. Crea un ConfigMap desde los archivos:
+
[source,bash]
----
kubectl create configmap app-files \
  --from-file=/tmp/config/app.properties \
  --from-file=/tmp/config/database.yaml \
  --from-file=/tmp/config/nginx.conf
----

3. Verifica el ConfigMap:
+
[source,bash]
----
kubectl get configmap app-files -o yaml
----

4. Crea un Pod que monte el ConfigMap como volumen:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: app-with-files
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Configuration Files ==="
      ls -la /etc/config/
      echo ""
      echo "=== app.properties ==="
      cat /etc/config/app.properties
      echo ""
      echo "=== database.yaml ==="
      cat /etc/config/database.yaml
      echo ""
      echo "=== nginx.conf ==="
      cat /etc/config/nginx.conf
      sleep infinity
    volumeMounts:
    - name: config
      mountPath: /etc/config
      readOnly: true
  volumes:
  - name: config
    configMap:
      name: app-files
      defaultMode: 0444
EOF
----

5. Verifica el Pod:
+
[source,bash]
----
kubectl get pod app-with-files
kubectl logs app-with-files
----

6. Accede al contenedor y verifica los archivos:
+
[source,bash]
----
kubectl exec -it app-with-files -- ls -la /etc/config/
kubectl exec -it app-with-files -- cat /etc/config/app.properties
----

7. Intenta modificar un archivo (debe fallar):
+
[source,bash]
----
kubectl exec -it app-with-files -- sh -c 'echo newvalue > /etc/config/app.properties'
# Debería fallar: Read-only file system
----

8. Modifica el ConfigMap (solo un archivo):
+
[source,bash]
----
kubectl patch configmap app-files -p '{"data":{"app.properties":"server.port=9090\napp.name=updated"}}'
----

9. Verifica que el archivo se actualizó en el Pod (espera algunos segundos):
+
[source,bash]
----
sleep 3
kubectl exec -it app-with-files -- cat /etc/config/app.properties
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod app-with-files
kubectl delete configmap app-files
rm -rf /tmp/config
----

**Verificación:**

El Pod debe mostrar los archivos del ConfigMap y reflejar cambios cuando se modifica.

**Preguntas de reflexión:**

1. ¿Cuál es la ventaja de montar ConfigMap como volumen en lugar de variables de entorno?
2. ¿Cuánto tiempo tarda en propagarse el cambio?
3. ¿Qué otros tipos de archivos podrías almacenar en un ConfigMap?

---

=== Ejercicio 6.3: envFrom - Inyección Bulk

**Objetivo:** Usar envFrom para inyectar múltiples variables de entorno desde ConfigMap de una vez.

**Descripción:**

Crea un ConfigMap y úsalo con envFrom para inyectar todas las claves como variables de entorno automáticamente.

**Tareas:**

1. Crea un ConfigMap con múltiples claves:
+
[source,bash]
----
kubectl create configmap bulk-config \
  --from-literal=DATABASE_HOST=db-server \
  --from-literal=DATABASE_PORT=5432 \
  --from-literal=CACHE_HOST=redis-server \
  --from-literal=CACHE_PORT=6379 \
  --from-literal=API_TIMEOUT=30 \
  --from-literal=MAX_CONNECTIONS=100
----

2. Crea un Pod usando envFrom:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: app-bulk-env
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== All Environment Variables ==="
      env | sort
      echo ""
      echo "=== From ConfigMap ==="
      env | grep -E "DATABASE|CACHE|API|MAX"
      sleep infinity
    envFrom:
    - configMapRef:
        name: bulk-config
EOF
----

3. Verifica el Pod:
+
[source,bash]
----
kubectl get pod app-bulk-env
----

4. Observa los logs:
+
[source,bash]
----
kubectl logs app-bulk-env
----

5. Verifica variables específicas:
+
[source,bash]
----
kubectl exec -it app-bulk-env -- sh -c 'echo $DATABASE_HOST'
kubectl exec -it app-bulk-env -- sh -c 'echo $CACHE_PORT'
----

6. Crea otro Pod con prefijo en las variables:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: app-prefixed-env
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Variables with PREFIX ==="
      env | grep MYAPP
      sleep infinity
    envFrom:
    - configMapRef:
        name: bulk-config
        prefix: MYAPP_
EOF
----

7. Verifica el prefijo:
+
[source,bash]
----
kubectl logs app-prefixed-env
----

8. Verifica variables con prefijo:
+
[source,bash]
----
kubectl exec -it app-prefixed-env -- sh -c 'echo $MYAPP_DATABASE_HOST'
----

9. Limpia:
+
[source,bash]
----
kubectl delete pod app-bulk-env
kubectl delete pod app-prefixed-env
kubectl delete configmap bulk-config
----

**Verificación:**

Todas las claves del ConfigMap deben inyectarse como variables de entorno automáticamente.

**Preguntas de reflexión:**

1. ¿Cuál es la ventaja de envFrom vs. inyectar variables individuales?
2. ¿Cómo se convierten los nombres de claves a nombres de variables?
3. ¿Cuándo usarías prefix en envFrom?

---

=== Ejercicio 6.4: Secret Básico

**Objetivo:** Crear un Secret y usarlo en un Pod para almacenar datos sensibles.

**Descripción:**

Crea un Secret con credenciales y úsalas en un Pod para simular una conexión a base de datos.

**Tareas:**

1. Crea un Secret con credenciales:
+
[source,bash]
----
kubectl create secret generic db-credentials \
  --from-literal=username=dbuser \
  --from-literal=password=SuperSecret123! \
  --from-literal=host=postgres.default.svc.cluster.local \
  --from-literal=port=5432 \
  --from-literal=database=myappdb
----

2. Verifica que el Secret existe (sin mostrar valores):
+
[source,bash]
----
kubectl get secret db-credentials
kubectl get secret db-credentials -o yaml
# Los valores están en base64, no en claro
----

3. Decodifica un valor para entender base64:
+
[source,bash]
----
# Ver valor base64
kubectl get secret db-credentials -o yaml | grep password

# Decodificar
echo -n "U3VwZXJTZWNyZXQxMjMh" | base64 -d
# Debería mostrar: SuperSecret123!
----

4. Crea un Pod que use el Secret con variables de entorno:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: db-app
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Database Connection ==="
      echo "Host: $DB_HOST"
      echo "Port: $DB_PORT"
      echo "Database: $DB_NAME"
      echo "User: $DB_USER"
      echo ""
      echo "Full Connection String:"
      echo "postgresql://$DB_USER:$DB_PASSWORD@$DB_HOST:$DB_PORT/$DB_NAME"
      echo ""
      echo "Attempting connection (simulated)..."
      sleep infinity
    env:
    - name: DB_HOST
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: host
    - name: DB_PORT
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: port
    - name: DB_USER
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: username
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: password
    - name: DB_NAME
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: database
EOF
----

5. Verifica el Pod:
+
[source,bash]
----
kubectl get pod db-app
kubectl logs db-app
----

6. Accede al contenedor y verifica las variables:
+
[source,bash]
----
kubectl exec -it db-app -- sh -c 'echo $DB_PASSWORD'
kubectl exec -it db-app -- sh -c 'echo $DB_USER'
----

7. Verifica que las variables están en el ambiente:
+
[source,bash]
----
kubectl exec -it db-app -- env | grep DB_
----

8. Modifica el Secret:
+
[source,bash]
----
kubectl patch secret db-credentials -p '{"data":{"password":"NewPassword456!"}}'
----

9. Reinicia el Pod para obtener el nuevo Secret:
+
[source,bash]
----
kubectl delete pod db-app
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: db-app
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "Updated Password: $DB_PASSWORD"
      sleep infinity
    env:
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: password
EOF
----

10. Verifica que tiene la nueva contraseña:
+
[source,bash]
----
kubectl logs db-app
----

11. Limpia:
+
[source,bash]
----
kubectl delete pod db-app
kubectl delete secret db-credentials
----

**Verificación:**

El Pod debe acceder a las credenciales del Secret sin mostrarlas en claro.

**Preguntas de reflexión:**

1. ¿Por qué está bien usar variables de entorno con Secrets?
2. ¿Cuál es la diferencia entre base64 y encriptación?
3. ¿Cómo se protegen mejor los Secrets en Kubernetes?

---

=== Ejercicio 6.5: Secret como Volumen (Más Seguro)

**Objetivo:** Montar un Secret como volumen para mayor seguridad.

**Descripción:**

Crea un Secret y monta como volumen con permisos restrictivos en lugar de variables de entorno.

**Tareas:**

1. Crea un Secret:
+
[source,bash]
----
kubectl create secret generic api-keys \
  --from-literal=api_key_prod=sk-live-abc123xyz789 \
  --from-literal=api_key_staging=sk-test-def456uvw012 \
  --from-literal=webhook_secret=webhook_secret_xyz123
----

2. Crea un Pod que monte el Secret como volumen:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: secure-app
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== API Keys (mounted as files) ==="
      echo ""
      echo "Files available:"
      ls -la /etc/api-secrets/
      echo ""
      echo "=== Production API Key ==="
      cat /etc/api-secrets/api_key_prod
      echo ""
      echo "=== File Permissions ==="
      stat /etc/api-secrets/api_key_prod
      echo ""
      sleep infinity
    volumeMounts:
    - name: api-keys
      mountPath: /etc/api-secrets
      readOnly: true
  volumes:
  - name: api-keys
    secret:
      secretName: api-keys
      defaultMode: 0400  # Permisos restrictivos
EOF
----

3. Verifica el Pod:
+
[source,bash]
----
kubectl get pod secure-app
kubectl logs secure-app
----

4. Verifica los permisos del archivo:
+
[source,bash]
----
kubectl exec -it secure-app -- ls -la /etc/api-secrets/
----

5. Intenta leer un secreto:
+
[source,bash]
----
kubectl exec -it secure-app -- cat /etc/api-secrets/api_key_prod
----

6. Intenta modificar un secreto (debe fallar):
+
[source,bash]
----
kubectl exec -it secure-app -- sh -c 'echo newkey > /etc/api-secrets/api_key_prod'
# Debería fallar: Read-only file system
----

7. Crea otro Pod que monte el Secret con opción items:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: selective-app
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Selected Secret Keys ==="
      ls -la /etc/secrets/
      echo ""
      echo "Prod Key location:"
      cat /etc/secrets/production-key
      sleep infinity
    volumeMounts:
    - name: selected-keys
      mountPath: /etc/secrets
      readOnly: true
  volumes:
  - name: selected-keys
    secret:
      secretName: api-keys
      defaultMode: 0400
      items:
      - key: api_key_prod
        path: production-key  # Renombrar archivo
      - key: webhook_secret
        path: webhook-config
EOF
----

8. Verifica que solo se montaron los keys seleccionados:
+
[source,bash]
----
kubectl logs selective-app
----

9. Modifica el Secret:
+
[source,bash]
----
kubectl patch secret api-keys -p '{"data":{"api_key_prod":"sk-live-newkey123abc"}}'
----

10. Verifica que el archivo se actualizó en el volumen (automático):
+
[source,bash]
----
sleep 2
kubectl exec -it secure-app -- cat /etc/api-secrets/api_key_prod
----

11. Limpia:
+
[source,bash]
----
kubectl delete pod secure-app
kubectl delete pod selective-app
kubectl delete secret api-keys
----

**Verificación:**

Los Secrets deben estar montados como archivos con permisos restrictivos de solo lectura.

**Preguntas de reflexión:**

1. ¿Por qué es mejor montar Secrets como volumen que como variables de entorno?
2. ¿Cómo se actualizan automáticamente los archivos cuando cambia el Secret?
3. ¿Qué significa defaultMode: 0400?

---

=== Ejercicio 6.6: Secret YAML Declarativo

**Objetivo:** Crear un Secret usando YAML declarativo con base64.

**Descripción:**

Crea un Secret definiendo valores en base64 directamente en el YAML.

**Tareas:**

1. Genera valores base64 para credenciales:
+
[source,bash]
----
echo -n "admin" | base64
# YWRtaW4=

echo -n "P@ssw0rd123!" | base64
# UEBzc3cwcmQxMjMh

echo -n "mongodb://localhost:27017" | base64
# bW9uZ29kYjovL2xvY2FsaG9zdDoyNzAxNw==
----

2. Crea un Secret YAML:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Secret
metadata:
  name: mongo-credentials
  namespace: default
type: Opaque
data:
  username: YWRtaW4=
  password: UEBzc3cwcmQxMjMh
  connection-string: bW9uZ29kYjovL2xvY2FsaG9zdDoyNzAxNw==
EOF
----

3. Verifica el Secret:
+
[source,bash]
----
kubectl get secret mongo-credentials
kubectl get secret mongo-credentials -o yaml
----

4. Decodifica los valores para verificar:
+
[source,bash]
----
kubectl get secret mongo-credentials -o jsonpath='{.data.username}' | base64 -d
# Debería mostrar: admin

kubectl get secret mongo-credentials -o jsonpath='{.data.password}' | base64 -d
# Debería mostrar: P@ssw0rd123!
----

5. Crea un Pod que use el Secret:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: mongo-client
spec:
  containers:
  - name: mongo
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      MONGO_USER=$(cat /etc/mongo/username)
      MONGO_PASS=$(cat /etc/mongo/password)
      MONGO_URL=$(cat /etc/mongo/connection-string)
      echo "=== MongoDB Connection ==="
      echo "User: $MONGO_USER"
      echo "URL: $MONGO_URL"
      echo "Connecting..."
      sleep infinity
    volumeMounts:
    - name: mongo-creds
      mountPath: /etc/mongo
      readOnly: true
  volumes:
  - name: mongo-creds
    secret:
      secretName: mongo-credentials
      defaultMode: 0400
EOF
----

6. Verifica el Pod:
+
[source,bash]
----
kubectl get pod mongo-client
kubectl logs mongo-client
----

7. Crea un Secret alternativo usando stringData (no base64):
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Secret
metadata:
  name: alternate-mongo
type: Opaque
stringData:  # No requiere base64
  username: admin
  password: P@ssw0rd123!
  url: mongodb://localhost:27017
EOF
----

8. Verifica que ambos Secrets contienen lo mismo:
+
[source,bash]
----
kubectl get secret mongo-credentials -o jsonpath='{.data.username}' | base64 -d
kubectl get secret alternate-mongo -o jsonpath='{.data.username}' | base64 -d
# Ambos muestran: admin
----

9. Limpia:
+
[source,bash]
----
kubectl delete pod mongo-client
kubectl delete secret mongo-credentials
kubectl delete secret alternate-mongo
----

**Verificación:**

El Secret YAML debe crear credenciales correctamente con valores base64.

**Preguntas de reflexión:**

1. ¿Cuál es la diferencia entre `data` y `stringData` en un Secret?
2. ¿Por qué se requiere base64 en `data`?
3. ¿Es seguro guardar base64 en control de versiones?

---

=== Ejercicio 6.7: Downward API

**Objetivo:** Usar Downward API para inyectar información del Pod sin ConfigMaps/Secrets.

**Descripción:**

Crea un Pod que inyecte información sobre sí mismo (nombre, namespace, labels) usando Downward API.

**Tareas:**

1. Crea un Pod con Downward API en variables de entorno:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: info-pod
  namespace: default
  labels:
    app: myapp
    version: v1.0
  annotations:
    description: "Test pod for Downward API"
    team: platform
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Pod Information (Downward API) ==="
      echo ""
      echo "=== Metadata ==="
      echo "Pod Name: $POD_NAME"
      echo "Pod Namespace: $POD_NAMESPACE"
      echo "Pod UID: $POD_UID"
      echo ""
      echo "=== Networking ==="
      echo "Pod IP: $POD_IP"
      echo "Node Name: $NODE_NAME"
      echo ""
      echo "=== Labels ==="
      echo "App Label: $POD_APP"
      echo "Version Label: $POD_VERSION"
      echo ""
      echo "=== Resources ==="
      echo "Memory Limit: $MEMORY_LIMIT"
      echo "CPU Request: $CPU_REQUEST"
      echo ""
      sleep infinity
    env:
    # Metadata
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: POD_UID
      valueFrom:
        fieldRef:
          fieldPath: metadata.uid
    # Networking
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    # Labels (si existen)
    - name: POD_APP
      valueFrom:
        fieldRef:
          fieldPath: metadata.labels['app']
    - name: POD_VERSION
      valueFrom:
        fieldRef:
          fieldPath: metadata.labels['version']
    # Resources
    - name: MEMORY_LIMIT
      valueFrom:
        resourceFieldRef:
          containerName: app
          resource: limits.memory
    - name: CPU_REQUEST
      valueFrom:
        resourceFieldRef:
          containerName: app
          resource: requests.cpu
    resources:
      requests:
        memory: "32Mi"
        cpu: "10m"
      limits:
        memory: "128Mi"
        cpu: "100m"
EOF
----

2. Verifica el Pod:
+
[source,bash]
----
kubectl get pod info-pod
----

3. Observa los logs:
+
[source,bash]
----
kubectl logs info-pod
----

4. Verifica variables específicas:
+
[source,bash]
----
kubectl exec -it info-pod -- sh -c 'echo $POD_NAME'
kubectl exec -it info-pod -- sh -c 'echo $POD_NAMESPACE'
kubectl exec -it info-pod -- sh -c 'echo $POD_IP'
----

5. Crea un Pod con Downward API como volumen:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: downward-volume-pod
  labels:
    tier: backend
    version: v2
  annotations:
    owner: platform-team
    environment: test
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Pod Information (Downward API Volume) ==="
      echo ""
      echo "=== Files ==="
      ls -la /etc/podinfo/
      echo ""
      echo "=== Name ==="
      cat /etc/podinfo/name
      echo ""
      echo "=== Namespace ==="
      cat /etc/podinfo/namespace
      echo ""
      echo "=== Labels ==="
      cat /etc/podinfo/labels
      echo ""
      echo "=== Annotations ==="
      cat /etc/podinfo/annotations
      echo ""
      sleep infinity
    volumeMounts:
    - name: podinfo
      mountPath: /etc/podinfo
  volumes:
  - name: podinfo
    downwardAPI:
      items:
      - path: "name"
        fieldRef:
          fieldPath: metadata.name
      - path: "namespace"
        fieldRef:
          fieldPath: metadata.namespace
      - path: "labels"
        fieldRef:
          fieldPath: metadata.labels
      - path: "annotations"
        fieldRef:
          fieldPath: metadata.annotations
EOF
----

6. Verifica el Pod:
+
[source,bash]
----
kubectl logs downward-volume-pod
----

7. Accede al contenedor y verifica los archivos:
+
[source,bash]
----
kubectl exec -it downward-volume-pod -- cat /etc/podinfo/name
kubectl exec -it downward-volume-pod -- cat /etc/podinfo/labels
----

8. Limpia:
+
[source,bash]
----
kubectl delete pod info-pod
kubectl delete pod downward-volume-pod
----

**Verificación:**

El Pod debe acceder a su propia información mediante Downward API sin necesidad de ConfigMaps o Secrets.

**Preguntas de reflexión:**

1. ¿Cuándo usarías Downward API en lugar de hardcodear valores?
2. ¿Qué información está disponible mediante Downward API?
3. ¿Cómo puede una aplicación usar esta información?

---

=== Ejercicio 6.8: ConfigMap multi-archivo desde directorio

**Objetivo:** Crear un ConfigMap desde un directorio completo con múltiples archivos.

**Descripción:**

Crea una estructura de directorios con archivos de configuración y úsalos en un ConfigMap.

**Tareas:**

1. Crea una estructura de directorios de configuración:
+
[source,bash]
----
mkdir -p /tmp/app-config/{services,db,logging}

cat > /tmp/app-config/services/api.conf <<'EOF'
listen_port 8080
max_connections 1000
timeout 30
enable_compression true
EOF

cat > /tmp/app-config/services/queue.conf <<'EOF'
broker rabbitmq://localhost
workers 10
prefetch 2
EOF

cat > /tmp/app-config/db/postgres.conf <<'EOF'
host db-server
port 5432
pool_size 20
ssl true
EOF

cat > /tmp/app-config/logging/main.conf <<'EOF'
level INFO
format json
output stdout
EOF
----

2. Crea un ConfigMap desde el directorio:
+
[source,bash]
----
kubectl create configmap app-config-dir --from-file=/tmp/app-config/
----

3. Verifica la estructura del ConfigMap:
+
[source,bash]
----
kubectl get configmap app-config-dir -o yaml
----

4. Crea un Pod que monte el ConfigMap preservando la estructura:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: app-with-config-dir
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Config Directory Structure ==="
      find /etc/app-config -type f | sort
      echo ""
      echo "=== services/api.conf ==="
      cat /etc/app-config/services/api.conf
      echo ""
      echo "=== db/postgres.conf ==="
      cat /etc/app-config/db/postgres.conf
      echo ""
      echo "=== logging/main.conf ==="
      cat /etc/app-config/logging/main.conf
      sleep infinity
    volumeMounts:
    - name: config
      mountPath: /etc/app-config
      readOnly: true
  volumes:
  - name: config
    configMap:
      name: app-config-dir
      defaultMode: 0444
EOF
----

5. Verifica el Pod:
+
[source,bash]
----
kubectl logs app-with-config-dir
----

6. Verifica la estructura dentro del Pod:
+
[source,bash]
----
kubectl exec -it app-with-config-dir -- find /etc/app-config -type f
----

7. Accede a archivos específicos:
+
[source,bash]
----
kubectl exec -it app-with-config-dir -- cat /etc/app-config/db/postgres.conf
----

8. Modifica un archivo específico en el ConfigMap:
+
[source,bash]
----
kubectl patch configmap app-config-dir -p '{"data":{"db/postgres.conf":"host db-server\nport 5432\npool_size 50\nssl true"}}'
----

9. Verifica que el cambio se propagó:
+
[source,bash]
----
sleep 2
kubectl exec -it app-with-config-dir -- cat /etc/app-config/db/postgres.conf
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod app-with-config-dir
kubectl delete configmap app-config-dir
rm -rf /tmp/app-config
----

**Verificación:**

El ConfigMap debe preservar la estructura de directorios del archivo original.

**Preguntas de reflexión:**

1. ¿Cómo mantiene Kubernetes la estructura de directorios?
2. ¿Qué ventajas tiene crear ConfigMap desde directorio?
3. ¿Hay límite de tamaño para un ConfigMap?

---

=== Ejercicio 6.9: Combinando ConfigMap y Secret en un Pod

**Objetivo:** Usar múltiples ConfigMaps y Secrets en el mismo Pod.

**Descripción:**

Crea un Pod que combine tanto datos de configuración (ConfigMap) como datos sensibles (Secret).

**Tareas:**

1. Crea un ConfigMap con configuración no sensible:
+
[source,bash]
----
kubectl create configmap app-config \
  --from-literal=app.name=ProductionApp \
  --from-literal=app.version=2.1.0 \
  --from-literal=log.level=INFO \
  --from-literal=server.port=8080
----

2. Crea un Secret con datos sensibles:
+
[source,bash]
----
kubectl create secret generic app-secrets \
  --from-literal=api_key=sk-live-prod-12345 \
  --from-literal=jwt_secret=jwt_very_secret_key_xyz \
  --from-literal=db_password=DbP@ssw0rd123!
----

3. Crea un Pod que use ambos:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: combined-config-app
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Configuration (Non-sensitive) ==="
      echo "App: $APP_NAME v$APP_VERSION"
      echo "Log Level: $LOG_LEVEL"
      echo "Server Port: $SERVER_PORT"
      echo ""
      echo "=== Secrets (Sensitive) ==="
      echo "API Key: $API_KEY"
      echo "JWT Secret: $JWT_SECRET"
      echo ""
      echo "=== Combined Setup ==="
      echo "Application '$APP_NAME' with:"
      echo "- API Key: $API_KEY"
      echo "- DB Password: $DB_PASSWORD"
      echo "- Server running on port $SERVER_PORT"
      sleep infinity
    env:
    # ConfigMap variables
    - name: APP_NAME
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: app.name
    - name: APP_VERSION
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: app.version
    - name: LOG_LEVEL
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: log.level
    - name: SERVER_PORT
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: server.port
    # Secret variables
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          name: app-secrets
          key: api_key
    - name: JWT_SECRET
      valueFrom:
        secretKeyRef:
          name: app-secrets
          key: jwt_secret
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: app-secrets
          key: db_password
    # ConfigMap volume
    volumeMounts:
    - name: config-files
      mountPath: /etc/config
      readOnly: true
    - name: secret-files
      mountPath: /etc/secrets
      readOnly: true
  volumes:
  - name: config-files
    configMap:
      name: app-config
  - name: secret-files
    secret:
      secretName: app-secrets
      defaultMode: 0400
EOF
----

4. Verifica el Pod:
+
[source,bash]
----
kubectl get pod combined-config-app
kubectl logs combined-config-app
----

5. Verifica variables de entorno:
+
[source,bash]
----
kubectl exec -it combined-config-app -- env | grep -E "APP|LOG|SERVER|API|JWT|DB"
----

6. Verifica archivos montados:
+
[source,bash]
----
kubectl exec -it combined-config-app -- ls -la /etc/config/
kubectl exec -it combined-config-app -- ls -la /etc/secrets/
----

7. Modifica ambos y reinicia:
+
[source,bash]
----
kubectl patch configmap app-config -p '{"data":{"app.version":"2.2.0"}}'
kubectl patch secret app-secrets -p '{"data":{"api_key":"sk-live-prod-updated"}}'
----

8. Elimina y recrea el Pod:
+
[source,bash]
----
kubectl delete pod combined-config-app
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: combined-config-app
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "Updated Version: $APP_VERSION"
      echo "Updated API Key: $API_KEY"
      sleep infinity
    env:
    - name: APP_VERSION
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: app.version
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          name: app-secrets
          key: api_key
EOF
----

9. Verifica los cambios:
+
[source,bash]
----
kubectl logs combined-config-app
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod combined-config-app
kubectl delete configmap app-config
kubectl delete secret app-secrets
----

**Verificación:**

El Pod debe tener acceso tanto a ConfigMaps como a Secrets en variables y volúmenes.

**Preguntas de reflexión:**

1. ¿Por qué separar ConfigMap y Secret en lugar de meter todo en un Secret?
2. ¿Cómo manejarías la rotación de Secrets?
3. ¿Qué estrategia usarías para diferentes ambientes (dev, staging, prod)?

---

=== Ejercicio 6.10: Docker Registry Secret para ImagePullSecrets

**Objetivo:** Crear un Secret para autenticarse con un registro Docker privado.

**Descripción:**

Crea un Secret Docker Registry y úsalo en un Pod para descargar imágenes de un registro privado.

**Tareas:**

1. Crea un Secret para Docker Registry (simulado):
+
[source,bash]
----
kubectl create secret docker-registry regcred \
  --docker-server=docker.example.com \
  --docker-username=myuser \
  --docker-password=mypassword123 \
  --docker-email=myuser@example.com
----

2. Verifica el Secret:
+
[source,bash]
----
kubectl get secret regcred
kubectl get secret regcred -o yaml | head -20
----

3. Verifica el contenido del Secret:
+
[source,bash]
----
# El Secret contiene .dockerconfigjson en base64
kubectl get secret regcred -o jsonpath='{.data.\.dockerconfigjson}' | base64 -d | head -20
----

4. Crea un Pod que usa ImagePullSecrets:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: private-image-pod
spec:
  imagePullSecrets:
  - name: regcred  # Referencia al Secret Docker
  containers:
  - name: app
    image: docker.example.com/myapp:latest
    command: ["sh", "-c"]
    args:
    - |
      echo "Pod running with private registry image"
      echo "Image: $IMAGE"
      sleep infinity
    env:
    - name: IMAGE
      value: docker.example.com/myapp:latest
EOF
----

5. Observa el Pod (probablemente esté en ImagePullBackOff):
+
[source,bash]
----
kubectl get pod private-image-pod
kubectl describe pod private-image-pod
# Debería mostrar error de imagen no encontrada (expected)
----

6. Crea un Pod alternativo que sí use una imagen válida pero muestra el concepto:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: config-with-auth
spec:
  imagePullSecrets:
  - name: regcred
  containers:
  - name: app
    image: busybox:latest  # Imagen pública para demo
    command: ["sh", "-c"]
    args:
    - |
      echo "=== ImagePullSecret Demo ==="
      echo ""
      echo "Este Pod está configurado para:"
      echo "- Usar Secret 'regcred' para autenticación"
      echo "- Descargar desde: docker.example.com"
      echo ""
      echo "En producción:"
      echo "1. Secret contiene credenciales de registro privado"
      echo "2. Kubelet usa Secret para autenticarse"
      echo "3. Solo así puede descargar imágenes privadas"
      sleep infinity
EOF
----

7. Verifica el Pod:
+
[source,bash]
----
kubectl logs config-with-auth
----

8. Verifica que el Secret se usó correctamente:
+
[source,bash]
----
kubectl describe pod config-with-auth | grep -i secret
----

9. Crea un Secret alternativo usando kubectl:
+
[source,bash]
----
# Crear manualmente
mkdir -p ~/.docker
cat > ~/.docker/config.json <<'EOF'
{
  "auths": {
    "docker.example.com": {
      "username": "myuser",
      "password": "mypassword123",
      "email": "myuser@example.com",
      "auth": "bXl1c2VyOm15cGFzc3dvcmQxMjM="
    }
  }
}
EOF

# Crear Secret desde el archivo
kubectl create secret generic dockerconfig-secret \
  --from-file=.dockerconfigjson=~/.docker/config.json \
  --type=kubernetes.io/dockerconfigjson

# Verifica
kubectl get secret dockerconfig-secret -o yaml
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod private-image-pod
kubectl delete pod config-with-auth
kubectl delete secret regcred
kubectl delete secret dockerconfig-secret
rm -rf ~/.docker/config.json
----

**Verificación:**

El Secret debe contener credenciales para autenticar con registro Docker privado.

**Preguntas de reflexión:**

1. ¿Por qué necesitas ImagePullSecrets para registros privados?
2. ¿Cómo Kubernetes utiliza las credenciales?
3. ¿Cuál es la diferencia entre .dockerconfig y .dockerconfigjson?

---

== Resumen de Conceptos

Estos ejercicios del Módulo 6 cubren:

* **Ejercicio 6.1**: ConfigMap básico con literales
* **Ejercicio 6.2**: ConfigMap desde archivos
* **Ejercicio 6.3**: envFrom para inyección bulk
* **Ejercicio 6.4**: Secret básico
* **Ejercicio 6.5**: Secret como volumen (más seguro)
* **Ejercicio 6.6**: Secret YAML declarativo
* **Ejercicio 6.7**: Downward API
* **Ejercicio 6.8**: ConfigMap multi-archivo desde directorio
* **Ejercicio 6.9**: Combinando ConfigMap y Secret
* **Ejercicio 6.10**: Docker Registry Secret (ImagePullSecrets)

== Próximos Pasos

Una vez completes estos ejercicios, estarás listo para:
* Módulo 7: Seguridad (RBAC, NetworkPolicies, Pod Security)
* Módulo 8: Observabilidad (Logging, Monitoring)
* Módulo 9: Escalado y Performance
* Y módulos posteriores...

---

== Módulo 7: Seguridad

=== Ejercicio 7.1: Creación de Service Account

**Objetivo:** Crear y configurar un Service Account en Kubernetes.

**Descripción:**

Crea un Service Account y verifica que se genera automáticamente un token JWT para autenticación.

**Tareas:**

1. Crea un Service Account declarativamente:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: ServiceAccount
metadata:
  name: myapp-sa
  namespace: default
  annotations:
    description: "Service account for myapp"
automountServiceAccountToken: true
EOF
----

2. Verifica que el Service Account fue creado:
+
[source,bash]
----
kubectl get serviceaccount
kubectl get sa myapp-sa -o yaml
----

3. Verifica que se creó automáticamente un Secret con el token:
+
[source,bash]
----
kubectl get secrets | grep myapp-sa
kubectl get secret <myapp-sa-token-xxx> -o yaml
----

4. Extrae y decodifica el token:
+
[source,bash]
----
# Obtener nombre del secret
TOKEN_SECRET=$(kubectl get sa myapp-sa -o jsonpath='{.secrets[0].name}')
echo "Token Secret: $TOKEN_SECRET"

# Obtener token en base64
TOKEN_B64=$(kubectl get secret $TOKEN_SECRET -o jsonpath='{.data.token}')

# Decodificar
TOKEN=$(echo $TOKEN_B64 | base64 -d)
echo "Token: $TOKEN"

# Decodificar payload del JWT (segunda parte)
echo $TOKEN | cut -d. -f2 | base64 -d | head -c 100
echo ""
----

5. Crea un Pod que use este Service Account:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  serviceAccountName: myapp-sa
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Service Account Information ==="
      echo "Service Account: $(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)"
      echo ""
      echo "=== Token Available ==="
      TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
      echo "Token (first 50 chars): ${TOKEN:0:50}..."
      echo ""
      echo "=== CA Certificate Available ==="
      ls -la /var/run/secrets/kubernetes.io/serviceaccount/
      sleep infinity
EOF
----

6. Verifica que el token está disponible en el Pod:
+
[source,bash]
----
kubectl logs myapp-pod
kubectl exec -it myapp-pod -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace
----

7. Crea un Service Account sin automountaje:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: ServiceAccount
metadata:
  name: notoken-sa
automountServiceAccountToken: false
EOF
----

8. Crea un Pod con automountaje deshabilitado:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: notoken-pod
spec:
  serviceAccountName: notoken-sa
  automountServiceAccountToken: false
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "Token directory:"
      ls -la /var/run/secrets/kubernetes.io/serviceaccount/ 2>&1
      echo "Result: Token NO está montado"
      sleep infinity
EOF
----

9. Verifica que no hay token:
+
[source,bash]
----
kubectl logs notoken-pod
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod myapp-pod
kubectl delete pod notoken-pod
kubectl delete sa myapp-sa
kubectl delete sa notoken-sa
----

**Verificación:**

El Service Account debe tener un token JWT automáticamente creado y disponible en los Pods.

**Preguntas de reflexión:**

1. ¿Por qué cada Service Account necesita un token?
2. ¿Cuándo deshabilitarías automountServiceAccountToken?
3. ¿Cómo se autentica un Pod con la API de Kubernetes?

---

=== Ejercicio 7.2: Role y RoleBinding básico

**Objetivo:** Crear un Role con permisos limitados y vincularlo a un Service Account.

**Descripción:**

Crea un Role que permite solo leer Pods y un RoleBinding que vincula ese Role a un Service Account.

**Tareas:**

1. Crea un Role con permisos de solo lectura:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: default
rules:
# Regla 1: Permite leer pods
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
# Regla 2: Permite leer logs
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get"]
EOF
----

2. Verifica que el Role fue creado:
+
[source,bash]
----
kubectl get role
kubectl get role pod-reader -o yaml
----

3. Crea un Service Account:
+
[source,bash]
----
kubectl create serviceaccount pod-reader-sa
----

4. Crea un RoleBinding que vincula Role a Service Account:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pod-reader
subjects:
- kind: ServiceAccount
  name: pod-reader-sa
  namespace: default
EOF
----

5. Verifica que el RoleBinding fue creado:
+
[source,bash]
----
kubectl get rolebinding
kubectl describe rolebinding read-pods
----

6. Crea un Pod que use el Service Account y acceda a la API:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: api-client
spec:
  serviceAccountName: pod-reader-sa
  containers:
  - name: client
    image: curlimages/curl:latest
    command: ["sh", "-c"]
    args:
    - |
      # Leer credenciales
      TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
      CA=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)

      echo "=== Service Account Permissions ==="
      echo "Namespace: $NS"
      echo "Token: ${TOKEN:0:50}..."
      echo ""
      echo "=== Testing API Access ==="
      echo "Listing Pods (permitted):"
      curl -s -H "Authorization: Bearer $TOKEN" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/namespaces/$NS/pods | grep '"name"' | head -3

      sleep infinity
EOF
----

7. Verifica que el Pod puede acceder a Pods:
+
[source,bash]
----
kubectl logs -f api-client
----

8. Intenta crear un Pod que trate de crear recursos (debe fallar):
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: api-create-attempt
spec:
  serviceAccountName: pod-reader-sa
  containers:
  - name: client
    image: curlimages/curl:latest
    command: ["sh", "-c"]
    args:
    - |
      TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
      CA=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)

      echo "=== Attempting to CREATE Pod (should fail) ==="
      curl -s -X POST \
        -H "Authorization: Bearer $TOKEN" \
        -H "Content-Type: application/json" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/namespaces/$NS/pods \
        -d '{"apiVersion":"v1","kind":"Pod","metadata":{"name":"test"}}' | jq .

      sleep infinity
EOF
----

9. Verifica el error de permiso:
+
[source,bash]
----
kubectl logs api-create-attempt | grep -i "forbidden\|error"
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod api-client
kubectl delete pod api-create-attempt
kubectl delete rolebinding read-pods
kubectl delete role pod-reader
kubectl delete sa pod-reader-sa
----

**Verificación:**

El Pod debe poder leer Pods pero no crearlos.

**Preguntas de reflexión:**

1. ¿Cuál es la diferencia entre Role y ClusterRole?
2. ¿Por qué necesitas vincular un Role a través de RoleBinding?
3. ¿Qué pasaría si no especificaras namespace en el RoleBinding?

---

=== Ejercicio 7.3: ClusterRole y ClusterRoleBinding

**Objetivo:** Crear un ClusterRole que se aplique a todo el cluster.

**Descripción:**

Crea un ClusterRole que permite leer Pods en todos los namespaces y vincularlo a un Service Account.

**Tareas:**

1. Crea un ClusterRole:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-viewer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get"]
EOF
----

2. Verifica que el ClusterRole fue creado:
+
[source,bash]
----
kubectl get clusterrole pod-viewer -o yaml
----

3. Crea un Service Account:
+
[source,bash]
----
kubectl create serviceaccount cluster-viewer-sa
----

4. Crea un ClusterRoleBinding:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: view-all-pods
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: pod-viewer
subjects:
- kind: ServiceAccount
  name: cluster-viewer-sa
  namespace: default
EOF
----

5. Crea otro namespace para probar:
+
[source,bash]
----
kubectl create namespace test-ns
----

6. Crea Pods en ambos namespaces:
+
[source,bash]
----
kubectl run pod-default --image=busybox --command -- sleep 3600
kubectl run pod-test --image=busybox -n test-ns --command -- sleep 3600
----

7. Crea un Pod en el namespace default que liste Pods en TODOS los namespaces:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: cluster-reader
spec:
  serviceAccountName: cluster-viewer-sa
  containers:
  - name: reader
    image: curlimages/curl:latest
    command: ["sh", "-c"]
    args:
    - |
      TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
      CA=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt

      echo "=== ClusterRole Access ==="
      echo "Listing Pods in default namespace:"
      curl -s -H "Authorization: Bearer $TOKEN" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/pods | jq '.items[] | .metadata | {name, namespace}' | head -10

      echo ""
      echo "Listing Pods in test-ns namespace:"
      curl -s -H "Authorization: Bearer $TOKEN" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/namespaces/test-ns/pods | jq '.items[] | .metadata | {name, namespace}' | head -5

      sleep infinity
EOF
----

8. Verifica acceso a múltiples namespaces:
+
[source,bash]
----
kubectl logs cluster-reader
----

9. Compara con un Role normal (solo un namespace):
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: namespace-viewer
  namespace: default
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: namespace-viewer-sa
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: view-ns-pods
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: namespace-viewer
subjects:
- kind: ServiceAccount
  name: namespace-viewer-sa
  namespace: default
EOF
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod cluster-reader
kubectl delete pod pod-default
kubectl delete pod pod-test -n test-ns
kubectl delete clusterrolebinding view-all-pods
kubectl delete clusterrole pod-viewer
kubectl delete sa cluster-viewer-sa
kubectl delete rolebinding view-ns-pods -n default
kubectl delete role namespace-viewer -n default
kubectl delete sa namespace-viewer-sa -n default
kubectl delete namespace test-ns
----

**Verificación:**

El ClusterRole debe permitir acceso a Pods en todos los namespaces.

**Preguntas de reflexión:**

1. ¿Cuándo usarías ClusterRole en lugar de Role?
2. ¿Qué riesgos de seguridad tiene un ClusterRole muy permisivo?
3. ¿Cómo limitarías un ClusterRole para solo leer?

---

=== Ejercicio 7.4: RBAC con Usuarios y Grupos

**Objetivo:** Usar RBAC no solo para Service Accounts sino también para usuarios humanos.

**Descripción:**

Crea un RoleBinding que otorgue permisos a un usuario específico y a un grupo.

**Tareas:**

1. Crea un Role:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer-role
  namespace: default
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "list", "create", "update", "patch"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get"]
EOF
----

2. Crea un RoleBinding para un usuario específico:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: developer-access
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: developer-role
subjects:
# Usuario específico
- kind: User
  name: alice@example.com
  apiGroup: rbac.authorization.k8s.io
# Grupo de usuarios
- kind: Group
  name: developers
  apiGroup: rbac.authorization.k8s.io
# También un Service Account
- kind: ServiceAccount
  name: dev-sa
  namespace: default
EOF
----

3. Verifica el RoleBinding:
+
[source,bash]
----
kubectl get rolebinding developer-access -o yaml
----

4. Crea un Service Account para la prueba:
+
[source,bash]
----
kubectl create serviceaccount dev-sa
----

5. Crea un Pod con el Service Account dev-sa:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: dev-pod
spec:
  serviceAccountName: dev-sa
  containers:
  - name: dev
    image: curlimages/curl:latest
    command: ["sh", "-c"]
    args:
    - |
      TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
      CA=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)

      echo "=== Developer Permissions ==="
      echo "Listing Pods (allowed):"
      curl -s -H "Authorization: Bearer $TOKEN" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/namespaces/$NS/pods | jq '.items | length'

      echo "Pods listed successfully"
      sleep infinity
EOF
----

6. Verifica los permisos del Service Account:
+
[source,bash]
----
kubectl logs dev-pod
----

7. Crea un segundo Role más restrictivo:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: viewer-role
  namespace: default
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
EOF
----

8. Crea un RoleBinding que solo otorgue permisos de lectura:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: viewer-access
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: viewer-role
subjects:
- kind: Group
  name: viewers
  apiGroup: rbac.authorization.k8s.io
EOF
----

9. Verifica ambos bindings:
+
[source,bash]
----
kubectl get rolebinding
kubectl describe rolebinding developer-access
kubectl describe rolebinding viewer-access
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod dev-pod
kubectl delete rolebinding developer-access viewer-access
kubectl delete role developer-role viewer-role
kubectl delete sa dev-sa
----

**Verificación:**

El RoleBinding debe permitir acceso a usuarios, grupos y Service Accounts.

**Preguntas de reflexión:**

1. ¿Cómo se mapean los usuarios en Kubernetes con identidades reales?
2. ¿Cuál es la diferencia entre User y Group en RBAC?
3. ¿Cómo implementarías la rotación de usuarios?

---

=== Ejercicio 7.5: Security Context - Ejecutar como usuario no-root

**Objetivo:** Usar Security Context para ejecutar contenedores como usuario no-root.

**Descripción:**

Crea un Pod con Security Context que ejecuta el contenedor con UID específico, no como root.

**Tareas:**

1. Crea un Pod con Security Context a nivel de Pod:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: non-root-pod
spec:
  securityContext:
    runAsUser: 1000      # UID
    runAsGroup: 3000     # GID
    fsGroup: 2000        # Grupo para volúmenes

  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Security Context Information ==="
      echo "Current UID: $(id -u)"
      echo "Current GID: $(id -g)"
      echo "Current User: $(id)"
      echo ""
      echo "=== File System Group ==="
      echo "fsGroup setting: Afecta permisos de volúmenes"
      sleep infinity
EOF
----

2. Verifica el usuario ejecutando:
+
[source,bash]
----
kubectl logs non-root-pod
----

3. Accede al Pod y verifica identidad:
+
[source,bash]
----
kubectl exec -it non-root-pod -- id
kubectl exec -it non-root-pod -- whoami
----

4. Crea un Pod con Security Context a nivel de contenedor:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: container-security-pod
spec:
  containers:
  - name: app
    image: busybox
    securityContext:
      runAsUser: 2000
      readOnlyRootFilesystem: true
    volumeMounts:
    - name: writable
      mountPath: /tmp
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Container Security Context ==="
      echo "UID: $(id -u)"
      echo "Root filesystem is read-only"
      echo "Only /tmp is writable"
      echo ""
      echo "Test write to /tmp:"
      echo "test data" > /tmp/test.txt && cat /tmp/test.txt
      sleep infinity

  volumes:
  - name: writable
    emptyDir: {}
EOF
----

5. Verifica el Pod:
+
[source,bash]
----
kubectl logs container-security-pod
----

6. Intenta escribir en raíz (debe fallar):
+
[source,bash]
----
kubectl exec -it container-security-pod -- sh -c 'echo test > /test.txt'
# Debería fallar: Read-only file system
----

7. Escribe en /tmp (debe funcionar):
+
[source,bash]
----
kubectl exec -it container-security-pod -- sh -c 'echo test > /tmp/test.txt && cat /tmp/test.txt'
----

8. Crea un Pod que intenta ejecutarse como root (inseguro):
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: root-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Running as root (INSECURE) ==="
      echo "UID: $(id -u)"
      echo "GID: $(id -g)"
      sleep infinity
EOF
----

9. Verifica que corre como root:
+
[source,bash]
----
kubectl logs root-pod
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod non-root-pod
kubectl delete pod container-security-pod
kubectl delete pod root-pod
----

**Verificación:**

El Pod debe ejecutarse con el UID especificado, no como root.

**Preguntas de reflexión:**

1. ¿Por qué es importante ejecutar como no-root?
2. ¿Cuál es la diferencia entre runAsUser y fsGroup?
3. ¿Qué es readOnlyRootFilesystem?

---

=== Ejercicio 7.6: Capabilities de Linux en Security Context

**Objetivo:** Usar Security Context para limitar capabilities de Linux.

**Descripción:**

Crea un Pod con capabilities limitadas para mejorar la seguridad.

**Tareas:**

1. Crea un Pod con capabilities limitadas:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: limited-caps-pod
spec:
  containers:
  - name: app
    image: busybox
    securityContext:
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Capabilities ==="
      echo "Dropped: ALL (excepto las agregadas)"
      echo "Added: NET_BIND_SERVICE"
      echo ""
      echo "Pod puede bindear puertos < 1024"
      echo "Pero no puede hacer otras operaciones privilegiadas"
      sleep infinity
EOF
----

2. Verifica el Pod:
+
[source,bash]
----
kubectl logs limited-caps-pod
----

3. Crea un Pod con privilegios completos (inseguro):
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: privileged-pod
spec:
  containers:
  - name: app
    image: busybox
    securityContext:
      privileged: true
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Privileged Container (INSECURE) ==="
      echo "Privileged: true"
      echo "Acceso a todos los dispositivos"
      echo "Acceso sin restricciones al host"
      sleep infinity
EOF
----

4. Verifica el Pod privilegiado:
+
[source,bash]
----
kubectl logs privileged-pod
----

5. Crea un Pod con allowPrivilegeEscalation = false:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: no-escalation-pod
spec:
  containers:
  - name: app
    image: busybox
    securityContext:
      allowPrivilegeEscalation: false
      runAsUser: 1000
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Privilege Escalation Prevention ==="
      echo "allowPrivilegeEscalation: false"
      echo "No puede usar setuid binarios para escalar"
      echo "Running as UID: $(id -u)"
      sleep infinity
EOF
----

6. Verifica:
+
[source,bash]
----
kubectl logs no-escalation-pod
----

7. Crea un Pod con capabilities específicas:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: custom-caps-pod
spec:
  containers:
  - name: app
    image: busybox
    securityContext:
      capabilities:
        add:
        - NET_BIND_SERVICE
        - NET_RAW
        - CHOWN
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Custom Capabilities ==="
      echo "Added capabilities:"
      echo "- NET_BIND_SERVICE: Bindear puertos < 1024"
      echo "- NET_RAW: Usar raw sockets"
      echo "- CHOWN: Cambiar propietario de archivos"
      sleep infinity
EOF
----

8. Verifica:
+
[source,bash]
----
kubectl logs custom-caps-pod
----

9. Describe el Pod para ver capabilities:
+
[source,bash]
----
kubectl describe pod limited-caps-pod | grep -A 5 Securitycontext
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod limited-caps-pod
kubectl delete pod privileged-pod
kubectl delete pod no-escalation-pod
kubectl delete pod custom-caps-pod
----

**Verificación:**

Los Pods deben ejecutarse con capabilities limitadas.

**Preguntas de reflexión:**

1. ¿Cuáles son las capabilities más peligrosas?
2. ¿Por qué es importante limitar capabilities?
3. ¿Qué capabilities son necesarias para la mayoría de aplicaciones?

---

=== Ejercicio 7.7: Pod Security Policy simulada (Pod Security Standards)

**Objetivo:** Entender cómo restringir la ejecución de Pods inseguros.

**Descripción:**

Aunque PSP está deprecated, crear Pods que cumplan con Pod Security Standards (restricted).

**Tareas:**

1. Crea un Pod que cumple con el estándar "restricted":
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 2000

  containers:
  - name: app
    image: busybox
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities:
        drop:
        - ALL
    volumeMounts:
    - name: tmp
      mountPath: /tmp
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Secure Pod (PSS Restricted) ==="
      echo "✓ Running as non-root (UID: $(id -u))"
      echo "✓ allowPrivilegeEscalation: false"
      echo "✓ readOnlyRootFilesystem: true"
      echo "✓ Drop all capabilities"
      sleep infinity

  volumes:
  - name: tmp
    emptyDir: {}
EOF
----

2. Verifica el Pod seguro:
+
[source,bash]
----
kubectl logs secure-pod
kubectl describe pod secure-pod
----

3. Intenta crear un Pod que no cumple (debería funcionar pero es inseguro):
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: insecure-pod
spec:
  containers:
  - name: app
    image: busybox
    securityContext:
      privileged: true
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Insecure Pod ==="
      echo "✗ Running as root"
      echo "✗ Privileged: true"
      echo "✗ No capability restrictions"
      sleep infinity
EOF
----

4. Verifica el Pod inseguro (será creado):
+
[source,bash]
----
kubectl get pod insecure-pod
----

5. Crea un Pod que cumple estándar "baseline":
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: baseline-pod
spec:
  securityContext:
    runAsNonRoot: true
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "=== Baseline Pod ==="
      echo "Running as non-root but with some flexibility"
      sleep infinity
EOF
----

6. Verifica:
+
[source,bash]
----
kubectl logs baseline-pod
----

7. Compara configuraciones:
+
[source,bash]
----
echo "=== Secure Pod ==="
kubectl get pod secure-pod -o yaml | grep -A 20 securityContext

echo ""
echo "=== Insecure Pod ==="
kubectl get pod insecure-pod -o yaml | grep -A 20 securityContext

echo ""
echo "=== Baseline Pod ==="
kubectl get pod baseline-pod -o yaml | grep -A 20 securityContext
----

8. Limpia:
+
[source,bash]
----
kubectl delete pod secure-pod
kubectl delete pod insecure-pod
kubectl delete pod baseline-pod
----

**Verificación:**

Los Pods seguros deben cumplir con Pod Security Standards restricted.

**Preguntas de reflexión:**

1. ¿Cuáles son los tres niveles de Pod Security Standards?
2. ¿Por qué fue deprecated Pod Security Policy?
3. ¿Cómo implementarías seguridad similar en un cluster?

---

=== Ejercicio 7.8: RBAC con Resources específicas

**Objetivo:** Limitar permisos a recursos específicos usando resourceNames.

**Descripción:**

Crea un Role que solo permite acceso a Pods/Secrets específicas por nombre.

**Tareas:**

1. Crea un Service Account:
+
[source,bash]
----
kubectl create serviceaccount limited-sa
----

2. Crea un Role que solo accede a una Secret específica:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: app-secrets-reader
  namespace: default
rules:
# Solo puede leer secrets específicas
- apiGroups: [""]
  resources: ["secrets"]
  resourceNames: ["app-secret", "db-secret"]
  verbs: ["get"]
# Puede listar todos los pods
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list"]
EOF
----

3. Vincula el Role:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: app-access
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: app-secrets-reader
subjects:
- kind: ServiceAccount
  name: limited-sa
  namespace: default
EOF
----

4. Crea algunos Secrets:
+
[source,bash]
----
kubectl create secret generic app-secret --from-literal=key=value1
kubectl create secret generic db-secret --from-literal=key=value2
kubectl create secret generic other-secret --from-literal=key=value3
----

5. Crea un Pod que intenta acceder a todos los Secrets:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: limited-access-pod
spec:
  serviceAccountName: limited-sa
  containers:
  - name: client
    image: curlimages/curl:latest
    command: ["sh", "-c"]
    args:
    - |
      TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
      CA=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)

      echo "=== Resource-Specific RBAC ==="
      echo ""
      echo "Accessing allowed secret (app-secret):"
      curl -s -H "Authorization: Bearer $TOKEN" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/namespaces/$NS/secrets/app-secret | jq '.metadata.name'

      echo ""
      echo "Accessing allowed secret (db-secret):"
      curl -s -H "Authorization: Bearer $TOKEN" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/namespaces/$NS/secrets/db-secret | jq '.metadata.name'

      echo ""
      echo "Attempting to access forbidden secret (other-secret):"
      curl -s -H "Authorization: Bearer $TOKEN" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/namespaces/$NS/secrets/other-secret | jq '.message'

      sleep infinity
EOF
----

6. Verifica acceso:
+
[source,bash]
----
kubectl logs -f limited-access-pod | head -20
----

7. Crea un Role que solo puede deletear Pods específicas:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-deleter
  namespace: default
rules:
- apiGroups: [""]
  resources: ["pods"]
  resourceNames: ["temporary-pod"]
  verbs: ["delete"]
EOF
----

8. Verifica que el Role es restrictivo:
+
[source,bash]
----
kubectl get role pod-deleter -o yaml
----

9. Limpia:
+
[source,bash]
----
kubectl delete pod limited-access-pod
kubectl delete rolebinding app-access
kubectl delete role app-secrets-reader pod-deleter
kubectl delete sa limited-sa
kubectl delete secret app-secret db-secret other-secret
----

**Verificación:**

El Service Account debe acceder solo a recursos específicos.

**Preguntas de reflexión:**

1. ¿Cómo se especifican recursos específicos en RBAC?
2. ¿Cuándo usarías resourceNames?
3. ¿Qué riesgos evita la limitación por resourceNames?

---

=== Ejercicio 7.9: Auditoría de RBAC

**Objetivo:** Verificar y auditar permisos RBAC en el cluster.

**Descripción:**

Usa herramientas para verificar qué permisos tiene un Service Account.

**Tareas:**

1. Crea varios Service Accounts con diferentes permisos:
+
[source,bash]
----
kubectl create serviceaccount viewer-sa
kubectl create serviceaccount editor-sa
kubectl create serviceaccount admin-sa
----

2. Crea Roles y RoleBindings:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: viewer
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: editor
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "list", "create", "update", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: viewer-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: viewer
subjects:
- kind: ServiceAccount
  name: viewer-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: editor-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: editor
subjects:
- kind: ServiceAccount
  name: editor-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-sa
  namespace: default
EOF
----

3. Lista todos los Roles:
+
[source,bash]
----
echo "=== Roles in default namespace ==="
kubectl get roles
kubectl get rolebindings

echo ""
echo "=== ClusterRoles ==="
kubectl get clusterroles | grep -E "(viewer|editor|admin)"
----

4. Verifica qué puede hacer cada Service Account:
+
[source,bash]
----
echo "=== viewer-sa permissions ==="
kubectl describe rolebinding viewer-binding

echo ""
echo "=== editor-sa permissions ==="
kubectl describe rolebinding editor-binding

echo ""
echo "=== admin-sa permissions ==="
kubectl get clusterrolebinding admin-binding -o yaml
----

5. Usa `kubectl auth` para verificar permisos:
+
[source,bash]
----
echo "=== Check if viewer-sa can get pods ==="
kubectl auth can-i get pods --as=system:serviceaccount:default:viewer-sa

echo "=== Check if viewer-sa can create pods ==="
kubectl auth can-i create pods --as=system:serviceaccount:default:viewer-sa

echo "=== Check if editor-sa can create pods ==="
kubectl auth can-i create pods --as=system:serviceaccount:default:editor-sa

echo "=== Check if admin-sa can delete clusterroles ==="
kubectl auth can-i delete clusterroles --as=system:serviceaccount:default:admin-sa
----

6. Crea un Pod para auditar desde dentro:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: audit-pod
spec:
  serviceAccountName: viewer-sa
  containers:
  - name: auditor
    image: curlimages/curl:latest
    command: ["sh", "-c"]
    args:
    - |
      TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
      CA=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)

      echo "=== Auditing viewer-sa ==="
      echo "Attempting allowed action (list pods):"
      curl -s -H "Authorization: Bearer $TOKEN" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/namespaces/$NS/pods | jq '.items | length'

      echo "Attempting forbidden action (create pod):"
      curl -s -X POST \
        -H "Authorization: Bearer $TOKEN" \
        -H "Content-Type: application/json" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/namespaces/$NS/pods \
        -d '{"apiVersion":"v1","kind":"Pod","metadata":{"name":"test"}}' | jq -r '.message // .reason'

      sleep infinity
EOF
----

7. Verifica la salida:
+
[source,bash]
----
kubectl logs audit-pod
----

8. Limpia:
+
[source,bash]
----
kubectl delete pod audit-pod
kubectl delete serviceaccount viewer-sa editor-sa admin-sa
kubectl delete roles viewer editor
kubectl delete rolebindings viewer-binding editor-binding
kubectl delete clusterrolebinding admin-binding
----

**Verificación:**

Debe ser posible auditar y verificar permisos RBAC en el cluster.

**Preguntas de reflexión:**

1. ¿Cómo auditas regularmente los permisos RBAC?
2. ¿Cuáles son los riesgos de over-privileging?
3. ¿Cómo implementarías una política de menor privilegio?

---

=== Ejercicio 7.10: Namespace Isolation con RBAC

**Objetivo:** Aislar aplicaciones por namespace usando RBAC.

**Descripción:**

Crea dos namespaces con diferentes aplicaciones y limita acceso usando RBAC.

**Tareas:**

1. Crea dos namespaces:
+
[source,bash]
----
kubectl create namespace app-a
kubectl create namespace app-b
----

2. Crea Service Accounts para cada aplicación:
+
[source,bash]
----
kubectl create serviceaccount app-a-sa -n app-a
kubectl create serviceaccount app-b-sa -n app-b
----

3. Crea Roles específicos para cada namespace:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
# Role en app-a namespace
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: app-a-role
  namespace: app-a
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "list", "create", "update"]
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get", "list"]
---
# Role en app-b namespace
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: app-b-role
  namespace: app-b
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "list", "create", "update"]
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list"]
---
# RoleBinding en app-a
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: app-a-binding
  namespace: app-a
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: app-a-role
subjects:
- kind: ServiceAccount
  name: app-a-sa
  namespace: app-a
---
# RoleBinding en app-b
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: app-b-binding
  namespace: app-b
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: app-b-role
subjects:
- kind: ServiceAccount
  name: app-b-sa
  namespace: app-b
EOF
----

4. Crea recursos en cada namespace:
+
[source,bash]
----
kubectl create configmap app-a-config -n app-a --from-literal=key=value
kubectl create secret generic app-b-secret -n app-b --from-literal=key=value
----

5. Crea un Pod en app-a que intenta leer configmaps y secrets:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: app-a-pod
  namespace: app-a
spec:
  serviceAccountName: app-a-sa
  containers:
  - name: app
    image: curlimages/curl:latest
    command: ["sh", "-c"]
    args:
    - |
      TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
      CA=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)

      echo "=== App-A (namespace: $NS) ==="
      echo "Can read configmaps:"
      curl -s -H "Authorization: Bearer $TOKEN" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/namespaces/$NS/configmaps | jq '.items | length'

      echo "Cannot read secrets (forbidden):"
      curl -s -H "Authorization: Bearer $TOKEN" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/namespaces/$NS/secrets | jq -r '.reason // "Success"'

      sleep infinity
EOF
----

6. Crea un Pod en app-b que intenta leer secrets y configmaps:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: app-b-pod
  namespace: app-b
spec:
  serviceAccountName: app-b-sa
  containers:
  - name: app
    image: curlimages/curl:latest
    command: ["sh", "-c"]
    args:
    - |
      TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
      CA=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)

      echo "=== App-B (namespace: $NS) ==="
      echo "Can read secrets:"
      curl -s -H "Authorization: Bearer $TOKEN" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/namespaces/$NS/secrets | jq '.items | length'

      echo "Cannot read configmaps (forbidden):"
      curl -s -H "Authorization: Bearer $TOKEN" \
        --cacert $CA \
        https://kubernetes.default.svc.cluster.local/api/v1/namespaces/$NS/configmaps | jq -r '.reason // "Success"'

      sleep infinity
EOF
----

7. Verifica aislamiento:
+
[source,bash]
----
echo "App-A logs:"
kubectl logs app-a-pod -n app-a

echo ""
echo "App-B logs:"
kubectl logs app-b-pod -n app-b
----

8. Intenta que app-a acceda a app-b (debe fallar):
+
[source,bash]
----
kubectl auth can-i get secrets --as=system:serviceaccount:app-a:app-a-sa -n app-b
----

9. Limpia:
+
[source,bash]
----
kubectl delete pod app-a-pod -n app-a
kubectl delete pod app-b-pod -n app-b
kubectl delete namespace app-a app-b
----

**Verificación:**

Cada aplicación debe tener acceso solo a sus propios recursos de namespace.

**Preguntas de reflexión:**

1. ¿Cómo aislas aplicaciones en un cluster compartido?
2. ¿Qué riesgos evita RBAC por namespace?
3. ¿Cómo implementarías una política multi-tenant?

---

== Resumen de Conceptos

Estos ejercicios del Módulo 7 cubren:

* **Ejercicio 7.1**: Creación de Service Account
* **Ejercicio 7.2**: Role y RoleBinding básico
* **Ejercicio 7.3**: ClusterRole y ClusterRoleBinding
* **Ejercicio 7.4**: RBAC con Usuarios y Grupos
* **Ejercicio 7.5**: Security Context - Usuario no-root
* **Ejercicio 7.6**: Capabilities de Linux
* **Ejercicio 7.7**: Pod Security Standards
* **Ejercicio 7.8**: RBAC con recursos específicas
* **Ejercicio 7.9**: Auditoría de RBAC
* **Ejercicio 7.10**: Namespace Isolation

== Próximos Pasos

Una vez completes estos ejercicios, estarás listo para:
* Módulo 8: Observabilidad (Logging, Monitoring)
* Módulo 9: Escalado y Performance
* Módulo 10: Helm y Package Management
* Y módulos posteriores...

---

== Módulo 8: Observabilidad

=== Ejercicio 8.1: Obtener Logs de Contenedores

**Objetivo:** Acceder y visualizar logs de Pods usando kubectl logs.

**Descripción:**

Crea un Pod que genera logs y aprende a consultarlos usando diferentes opciones de kubectl.

**Tareas:**

1. Crea un Pod que genera logs continuamente:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: logger-pod
spec:
  containers:
  - name: logger
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      counter=1
      while true; do
        echo "[INFO] $(date '+%H:%M:%S') - Log message #$counter"
        counter=$((counter+1))
        sleep 2
      done
EOF
----

2. Ver logs del Pod:
+
[source,bash]
----
kubectl logs logger-pod
----

3. Ver logs en tiempo real (follow):
+
[source,bash]
----
kubectl logs logger-pod -f
# Presiona Ctrl+C para detener
----

4. Ver últimas 10 líneas:
+
[source,bash]
----
kubectl logs logger-pod --tail=10
----

5. Ver logs desde hace 1 minuto:
+
[source,bash]
----
kubectl logs logger-pod --since=1m
----

6. Ver logs con timestamps:
+
[source,bash]
----
kubectl logs logger-pod -f
----

7. Crea un Pod que genera logs con errores:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: error-logger-pod
spec:
  containers:
  - name: logger
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "Starting application..."
      sleep 2
      echo "[ERROR] Database connection failed"
      sleep 1
      echo "[WARN] Retrying connection..."
      sleep 1
      echo "[INFO] Connection established"
      sleep infinity
EOF
----

8. Filtra líneas con "ERROR":
+
[source,bash]
----
kubectl logs error-logger-pod | grep ERROR
----

9. Obtén logs con formato específico:
+
[source,bash]
----
kubectl logs error-logger-pod -f --timestamps=true
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod logger-pod error-logger-pod
----

**Verificación:**

Debes poder ver logs del Pod con diferentes opciones (tail, since, follow).

**Preguntas de reflexión:**

1. ¿Dónde se almacenan los logs en el nodo?
2. ¿Qué sucede con los logs cuando se elimina un Pod?
3. ¿Cómo captura Kubernetes logs de stdout/stderr?

---

=== Ejercicio 8.2: Logs en Pods Multi-Contenedor

**Objetivo:** Acceder a logs de contenedores específicos en Pods con múltiples contenedores.

**Descripción:**

Crea un Pod con múltiples contenedores y aprende a acceder a logs de cada uno.

**Tareas:**

1. Crea un Pod con múltiples contenedores:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-logger
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      counter=1
      while true; do
        echo "APP: Message #$counter"
        counter=$((counter+1))
        sleep 3
      done

  - name: sidecar
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      counter=1
      while true; do
        echo "SIDECAR: Processing message #$counter"
        counter=$((counter+1))
        sleep 3
      done

  - name: monitor
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      while true; do
        echo "MONITOR: Health check OK at $(date '+%H:%M:%S')"
        sleep 5
      done
EOF
----

2. Ver logs de todos los contenedores:
+
[source,bash]
----
kubectl logs multi-container-logger --all-containers=true
----

3. Ver logs de un contenedor específico:
+
[source,bash]
----
kubectl logs multi-container-logger -c app
kubectl logs multi-container-logger -c sidecar
kubectl logs multi-container-logger -c monitor
----

4. Ver logs del último contenedor:
+
[source,bash]
----
kubectl logs multi-container-logger
----

5. Ver logs en tiempo real de un contenedor:
+
[source,bash]
----
kubectl logs multi-container-logger -c app -f
----

6. Ver logs ordenados por timestamp:
+
[source,bash]
----
kubectl logs multi-container-logger -c sidecar --timestamps=true
----

7. Crea un Pod con Init container:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: init-container-logger
spec:
  initContainers:
  - name: init
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "INIT: Initializing..."
      sleep 2
      echo "INIT: Ready!"

  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "APP: Starting after init"
      sleep infinity
EOF
----

8. Ver logs del init container:
+
[source,bash]
----
# Los init containers solo aparecen en el estado "Init"
kubectl describe pod init-container-logger
kubectl logs init-container-logger -c init
----

9. Ver logs de un Pod con un contenedor que ha fallado:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: failing-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "Starting..."
      sleep 1
      echo "ERROR: Fatal error"
      exit 1
EOF
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod multi-container-logger init-container-logger failing-pod
----

**Verificación:**

Debes poder acceder a logs de contenedores específicos usando la opción -c.

**Preguntas de reflexión:**

1. ¿Cómo ves logs de un init container después de que el Pod está corriendo?
2. ¿Qué muestra kubectl logs si el Pod tiene múltiples contenedores?
3. ¿Cómo accedes a logs de un contenedor que falló?

---

=== Ejercicio 8.3: Logs de Deployments

**Objetivo:** Ver logs de Deployments y Pods gestionados por controladores.

**Descripción:**

Crea un Deployment y aprende a acceder a logs de sus Pods.

**Tareas:**

1. Crea un Deployment:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: busybox
        command: ["sh", "-c"]
        args:
        - |
          POD_NAME=$(hostname)
          counter=1
          while true; do
            echo "[$POD_NAME] Processing request #$counter"
            counter=$((counter+1))
            sleep 2
          done
EOF
----

2. Verifica los Pods creados:
+
[source,bash]
----
kubectl get pods -l app=myapp
----

3. Ver logs de todos los Pods del Deployment:
+
[source,bash]
----
kubectl logs deployment/app-deployment
----

4. Ver logs de un Pod específico:
+
[source,bash]
----
POD_NAME=$(kubectl get pods -l app=myapp -o jsonpath='{.items[0].metadata.name}')
kubectl logs $POD_NAME
----

5. Ver logs de todos los Pods con label selector:
+
[source,bash]
----
kubectl logs -l app=myapp -f
----

6. Ver logs ordenados:
+
[source,bash]
----
kubectl logs -l app=myapp --all-containers=true | head -20
----

7. Crea un Deployment que genera errores:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: error-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: error-app
  template:
    metadata:
      labels:
        app: error-app
    spec:
      containers:
      - name: app
        image: busybox
        command: ["sh", "-c"]
        args:
        - |
          echo "Starting..."
          sleep 2
          echo "ERROR: Connection failed"
          sleep 2
          echo "Retrying..."
          sleep 1
          echo "SUCCESS: Connected"
          sleep infinity
EOF
----

8. Filtra logs por línea que contenga ERROR:
+
[source,bash]
----
kubectl logs -l app=error-app | grep ERROR
----

9. Cuenta cuántas veces aparece ERROR:
+
[source,bash]
----
kubectl logs -l app=error-app | grep -c ERROR
----

10. Limpia:
+
[source,bash]
----
kubectl delete deployment app-deployment error-deployment
----

**Verificación:**

Debes poder ver logs de Deployments y filtrar por contenido.

**Preguntas de reflexión:**

1. ¿Cómo accedes a logs de todos los Pods de un Deployment?
2. ¿Qué sucede si especificas múltiples labels?
3. ¿Cómo filtras logs de todos los Pods juntos?

---

=== Ejercicio 8.4: Eventos del Cluster

**Objetivo:** Monitorear eventos de Kubernetes para debugging.

**Descripción:**

Aprende a usar eventos de Kubernetes para entender qué sucede en el cluster.

**Tareas:**

1. Ver todos los eventos del namespace:
+
[source,bash]
----
kubectl get events
----

2. Ver eventos en tiempo real:
+
[source,bash]
----
kubectl get events -w
# Presiona Ctrl+C para detener
----

3. Ver eventos detallados de un Pod específico:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: event-demo-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "Running..."
      sleep 10
EOF
----

4. Describe el Pod para ver eventos:
+
[source,bash]
----
kubectl describe pod event-demo-pod
# Los eventos aparecen al final
----

5. Crea un Pod que falla para ver eventos:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: failing-pod
spec:
  containers:
  - name: app
    image: nonexistent-image:latest
    imagePullPolicy: Always
EOF
----

6. Observa los eventos del Pod fallido:
+
[source,bash]
----
kubectl describe pod failing-pod
# Verás eventos como ImagePullBackOff
----

7. Ver eventos de un Deployment:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: events-test-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      containers:
      - name: app
        image: busybox
        command: ["sh", "-c"]
        args:
        - |
          sleep infinity
EOF
----

8. Ver eventos del Deployment:
+
[source,bash]
----
kubectl describe deployment events-test-deployment
----

9. Ordena eventos por timestamp:
+
[source,bash]
----
kubectl get events --sort-by='.lastTimestamp'
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod event-demo-pod failing-pod
kubectl delete deployment events-test-deployment
----

**Verificación:**

Debes poder ver eventos del cluster y entender qué sucede en los Pods.

**Preguntas de reflexión:**

1. ¿Cuál es la diferencia entre logs y eventos?
2. ¿Cuánto tiempo se retienen los eventos?
3. ¿Cómo usas eventos para debugging?

---

=== Ejercicio 8.5: Métricas con kubectl top

**Objetivo:** Monitorear uso de recursos (CPU y memoria) usando kubectl top.

**Descripción:**

Usa kubectl top para visualizar métricas de Pods y Nodos.

**Tareas:**

1. Verifica que Metrics Server está instalado:
+
[source,bash]
----
kubectl get deployment metrics-server -n kube-system
----

2. Si no está instalado, instálalo:
+
[source,bash]
----
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
----

3. Ver uso de nodos:
+
[source,bash]
----
kubectl top nodes
----

4. Ver uso detallado de un nodo:
+
[source,bash]
----
FIRST_NODE=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')
kubectl top node $FIRST_NODE
----

5. Crea Pods que consumen recursos:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource-hog
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hog
  template:
    metadata:
      labels:
        app: hog
    spec:
      containers:
      - name: cpu-user
        image: busybox
        resources:
          requests:
            cpu: 100m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 128Mi
        command: ["sh", "-c"]
        args:
        - |
          while true; do
            dd if=/dev/zero bs=1M count=50 2>/dev/null | md5sum > /dev/null
          done
EOF
----

6. Espera un momento para que se generen métricas:
+
[source,bash]
----
sleep 30
----

7. Ver uso de Pods:
+
[source,bash]
----
kubectl top pods
----

8. Ver uso de Pods en todos los namespaces:
+
[source,bash]
----
kubectl top pods -A
----

9. Ordena Pods por CPU:
+
[source,bash]
----
kubectl top pods --sort-by=cpu
----

10. Limpia:
+
[source,bash]
----
kubectl delete deployment resource-hog
----

**Verificación:**

Debes poder ver métricas de CPU y memoria de Pods y Nodos.

**Preguntas de reflexión:**

1. ¿De dónde obtiene Metrics Server las métricas?
2. ¿Con qué frecuencia se actualizan las métricas?
3. ¿Cuánto tiempo tarda en aparecer un Pod en kubectl top?

---

=== Ejercicio 8.6: Monitoreo de Recursos con Resource Limits

**Objetivo:** Establecer limites de recursos y observar el comportamiento.

**Descripción:**

Crea Pods con resource requests y limits, y observa qué sucede cuando los exceden.

**Tareas:**

1. Crea un Pod con requests apropiados:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: well-configured-pod
spec:
  containers:
  - name: app
    image: busybox
    resources:
      requests:
        cpu: 100m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 128Mi
    command: ["sh", "-c"]
    args:
    - |
      echo "Running with requested resources"
      sleep infinity
EOF
----

2. Ver métricas después de 30 segundos:
+
[source,bash]
----
sleep 30
kubectl top pod well-configured-pod
----

3. Crea un Pod que intenta usar más memoria que el límite:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: memory-limit-pod
spec:
  containers:
  - name: app
    image: busybox
    resources:
      requests:
        memory: 64Mi
      limits:
        memory: 128Mi
    command: ["sh", "-c"]
    args:
    - |
      echo "Allocating memory..."
      # Intenta asignar más memoria que el límite
      dd if=/dev/zero bs=1M count=200 of=/dev/null 2>&1 &
      sleep infinity
EOF
----

4. Observa los eventos del Pod:
+
[source,bash]
----
kubectl describe pod memory-limit-pod
# Debería mostrar OOMKilled si excede memoria
----

5. Crea un Pod sin límites (para comparar):
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: unlimited-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      echo "No resource limits"
      sleep infinity
EOF
----

6. Compara métricas después de 30 segundos:
+
[source,bash]
----
sleep 30
kubectl top pods
----

7. Ver descripción de Pods para ver limits:
+
[source,bash]
----
kubectl describe pod well-configured-pod | grep -A 5 "Limits\|Requests"
----

8. Crea un Deployment con resource requests:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource-aware-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: resource-aware
  template:
    metadata:
      labels:
        app: resource-aware
    spec:
      containers:
      - name: app
        image: busybox
        resources:
          requests:
            cpu: 50m
            memory: 32Mi
          limits:
            cpu: 100m
            memory: 64Mi
        command: ["sh", "-c"]
        args:
        - |
          sleep infinity
EOF
----

9. Ver métricas del Deployment:
+
[source,bash]
----
sleep 30
kubectl top pods -l app=resource-aware
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod well-configured-pod memory-limit-pod unlimited-pod
kubectl delete deployment resource-aware-deployment
----

**Verificación:**

Los Pods deben mostrar métricas de uso real vs. recursos solicitados/limitados.

**Preguntas de reflexión:**

1. ¿Qué sucede cuando un Pod excede su memory limit?
2. ¿Por qué es importante establecer requests?
3. ¿Cómo afecta esto al scheduler?

---

=== Ejercicio 8.7: Logging con Sidecar Pattern

**Objetivo:** Implementar el patrón sidecar para logging.

**Descripción:**

Crea un Pod con un contenedor que genera logs en archivo y un sidecar que lo procesa.

**Tareas:**

1. Crea un Pod con patrón sidecar de logging:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: sidecar-logger-pod
spec:
  containers:
  # Contenedor principal
  - name: app
    image: busybox
    volumeMounts:
    - name: log-volume
      mountPath: /var/log/app
    command: ["sh", "-c"]
    args:
    - |
      counter=1
      while true; do
        echo "[INFO] $(date '+%H:%M:%S') - Application event #$counter" >> /var/log/app/app.log
        counter=$((counter+1))
        sleep 2
      done

  # Sidecar que procesa los logs
  - name: log-processor
    image: busybox
    volumeMounts:
    - name: log-volume
      mountPath: /var/log/app
    command: ["sh", "-c"]
    args:
    - |
      echo "Log processor sidecar started"
      while true; do
        if [ -f /var/log/app/app.log ]; then
          echo "=== Latest logs at $(date '+%H:%M:%S') ==="
          tail -3 /var/log/app/app.log
          echo ""
        fi
        sleep 5
      done

  volumes:
  - name: log-volume
    emptyDir: {}
EOF
----

2. Ver logs del contenedor app:
+
[source,bash]
----
kubectl logs sidecar-logger-pod -c app
----

3. Ver logs del contenedor sidecar:
+
[source,bash]
----
kubectl logs sidecar-logger-pod -c log-processor -f
----

4. Accede directamente al contenedor app:
+
[source,bash]
----
kubectl exec -it sidecar-logger-pod -c app -- cat /var/log/app/app.log
----

5. Crea un Pod con sidecar que envía logs a stdout:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: sidecar-forwarder-pod
spec:
  containers:
  - name: app
    image: busybox
    volumeMounts:
    - name: log-volume
      mountPath: /var/log/app
    command: ["sh", "-c"]
    args:
    - |
      counter=1
      while true; do
        echo "[APP] Event #$counter at $(date '+%H:%M:%S')" >> /var/log/app/app.log
        counter=$((counter+1))
        sleep 1
      done

  - name: log-forwarder
    image: busybox
    volumeMounts:
    - name: log-volume
      mountPath: /var/log/app
    command: ["sh", "-c"]
    args:
    - |
      while true; do
        if [ -f /var/log/app/app.log ]; then
          tail -n +1 /var/log/app/app.log
          > /var/log/app/app.log  # Limpia el archivo
        fi
        sleep 3
      done

  volumes:
  - name: log-volume
    emptyDir: {}
EOF
----

6. Ver logs del forwarder:
+
[source,bash]
----
kubectl logs sidecar-forwarder-pod -c log-forwarder -f
----

7. Crea un Pod con múltiples sidecars:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: multi-sidecar-pod
spec:
  containers:
  - name: app
    image: busybox
    volumeMounts:
    - name: app-logs
      mountPath: /var/log/app
    - name: access-logs
      mountPath: /var/log/access
    command: ["sh", "-c"]
    args:
    - |
      counter=1
      while true; do
        echo "[APP] Event #$counter" >> /var/log/app/app.log
        echo "GET /api/users HTTP/1.1 200" >> /var/log/access/access.log
        counter=$((counter+1))
        sleep 1
      done

  - name: app-log-processor
    image: busybox
    volumeMounts:
    - name: app-logs
      mountPath: /var/log/app
    command: ["sh", "-c"]
    args:
    - |
      while true; do
        echo "[APP-PROCESSOR] $(date '+%H:%M:%S'): Processing app logs"
        sleep 5
      done

  - name: access-log-processor
    image: busybox
    volumeMounts:
    - name: access-logs
      mountPath: /var/log/access
    command: ["sh", "-c"]
    args:
    - |
      while true; do
        echo "[ACCESS-PROCESSOR] $(date '+%H:%M:%S'): Processing access logs"
        sleep 5
      done

  volumes:
  - name: app-logs
    emptyDir: {}
  - name: access-logs
    emptyDir: {}
EOF
----

8. Ver logs de cada sidecar:
+
[source,bash]
----
kubectl logs multi-sidecar-pod -c app-log-processor -f
----

9. Describe el Pod para ver estructura:
+
[source,bash]
----
kubectl describe pod multi-sidecar-pod
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod sidecar-logger-pod sidecar-forwarder-pod multi-sidecar-pod
----

**Verificación:**

Los sidecars deben procesar logs del contenedor principal.

**Preguntas de reflexión:**

1. ¿Cuál es la ventaja del patrón sidecar?
2. ¿Cómo se comunican contenedores en el mismo Pod?
3. ¿Cuándo usarías un sidecar de logging?

---

=== Ejercicio 8.8: Exposición de Métricas de Aplicación

**Objetivo:** Crear una aplicación que expone métricas para Prometheus.

**Descripción:**

Crea un Pod con una aplicación que expone métricas en el formato de Prometheus.

**Tareas:**

1. Crea un Pod que simula exponer métricas:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: metrics-app
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
    prometheus.io/path: "/metrics"
spec:
  containers:
  - name: app
    image: busybox
    ports:
    - name: metrics
      containerPort: 8000
    command: ["sh", "-c"]
    args:
    - |
      # Simula un servidor que expone métricas
      while true; do
        {
          echo "HTTP/1.1 200 OK"
          echo "Content-Type: text/plain"
          echo ""
          echo "# HELP requests_total Total number of requests"
          echo "# TYPE requests_total counter"
          echo "requests_total{method=\"GET\"} 100"
          echo "requests_total{method=\"POST\"} 50"
          echo ""
          echo "# HELP request_duration_seconds Request latency"
          echo "# TYPE request_duration_seconds histogram"
          echo "request_duration_seconds_bucket{le=\"0.1\"} 50"
          echo "request_duration_seconds_bucket{le=\"0.5\"} 90"
          echo "request_duration_seconds_bucket{le=\"1\"} 100"
          echo ""
          echo "# HELP memory_bytes Memory usage in bytes"
          echo "# TYPE memory_bytes gauge"
          echo "memory_bytes 1024000"
        } | nc -l -p 8000 -q 1
      done
EOF
----

2. Verifica que el Pod está corriendo:
+
[source,bash]
----
kubectl get pod metrics-app
----

3. Accede al endpoint de métricas desde otro Pod:
+
[source,bash]
----
kubectl run -it curl-pod --image=curlimages/curl --rm --restart=Never -- \
  curl http://metrics-app:8000/metrics
----

4. Crea un Pod con Deployment y métricas:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: metrics
  template:
    metadata:
      labels:
        app: metrics
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: busybox
        ports:
        - name: metrics
          containerPort: 8000
        command: ["sh", "-c"]
        args:
        - |
          counter=0
          while true; do
            counter=$((counter+1))
            {
              echo "HTTP/1.1 200 OK"
              echo "Content-Type: text/plain"
              echo ""
              echo "# Pod metrics"
              echo "pod_requests{pod=\"$(hostname)\"} $counter"
              echo "pod_uptime_seconds $((RANDOM % 3600))"
            } | nc -l -p 8000 -q 1
            sleep 1
          done
EOF
----

5. Verifica los Pods del Deployment:
+
[source,bash]
----
kubectl get pods -l app=metrics
----

6. Accede a métricas de un Pod específico:
+
[source,bash]
----
POD_NAME=$(kubectl get pods -l app=metrics -o jsonpath='{.items[0].metadata.name}')
kubectl port-forward $POD_NAME 8000:8000 &
----

7. Consulta desde otro terminal:
+
[source,bash]
----
sleep 2
curl localhost:8000/metrics
----

8. Crea un Service para acceder a las métricas:
+
[source,bash]
----
kubectl expose deployment metrics-deployment --port=8000 --target-port=8000
----

9. Consulta las métricas a través del Service:
+
[source,bash]
----
kubectl run -it curl-pod --image=curlimages/curl --rm --restart=Never -- \
  curl http://metrics-deployment:8000/metrics
----

10. Limpia:
+
[source,bash]
----
pkill -f "port-forward"
kubectl delete pod metrics-app
kubectl delete deployment metrics-deployment
kubectl delete service metrics-deployment
----

**Verificación:**

La aplicación debe exponer métricas en formato Prometheus.

**Preguntas de reflexión:**

1. ¿Cuál es el formato de métricas de Prometheus?
2. ¿Cómo sabe Prometheus dónde encontrar las métricas?
3. ¿Qué tipos de métricas existen (counter, gauge, histogram)?

---

=== Ejercicio 8.9: Análisis de Logs con Herramientas

**Objetivo:** Usar herramientas para analizar y filtrar logs.

**Descripción:**

Aprende técnicas avanzadas de búsqueda y análisis de logs.

**Tareas:**

1. Crea un Pod que genera logs variados:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: analysis-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c"]
    args:
    - |
      counter=1
      while true; do
        case $((counter % 5)) in
          1) echo "[INFO] User login from 192.168.1.100" ;;
          2) echo "[ERROR] Database connection timeout" ;;
          3) echo "[WARN] Memory usage at 85%" ;;
          4) echo "[INFO] API request completed in 150ms" ;;
          0) echo "[DEBUG] Cache hit for key user:123" ;;
        esac
        counter=$((counter+1))
        sleep 1
      done
EOF
----

2. Captura 100 líneas de logs:
+
[source,bash]
----
kubectl logs analysis-pod --tail=100 > /tmp/logs.txt
----

3. Cuenta mensajes por nivel:
+
[source,bash]
----
kubectl logs analysis-pod | grep -o "\[.*\]" | sort | uniq -c
----

4. Filtra solo errores:
+
[source,bash]
----
kubectl logs analysis-pod | grep ERROR
----

5. Busca por IP:
+
[source,bash]
----
kubectl logs analysis-pod | grep "192.168"
----

6. Estadísticas de logs:
+
[source,bash]
----
kubectl logs analysis-pod | wc -l
----

7. Busca logs entre timestamps (aproximadamente):
+
[source,bash]
----
kubectl logs analysis-pod --since=2m --tail=50
----

8. Crea un Deployment y analiza logs de todos los Pods:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: analysis-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: analyzer
  template:
    metadata:
      labels:
        app: analyzer
    spec:
      containers:
      - name: app
        image: busybox
        command: ["sh", "-c"]
        args:
        - |
          POD_ID=$(echo $HOSTNAME | tail -c 5)
          counter=1
          while true; do
            echo "[$POD_ID] $(date '+%H:%M:%S') - Process event #$counter"
            counter=$((counter+1))
            sleep 1
          done
EOF
----

9. Espera a que los Pods estén corriendo:
+
[source,bash]
----
sleep 10
----

10. Análisis de logs de todo el Deployment:
+
[source,bash]
----
# Todos los logs
kubectl logs -l app=analyzer | head -30

# Contar eventos por Pod
kubectl logs -l app=analyzer | awk '{print $1}' | sort | uniq -c
----

**Verificación:**

Debes poder filtrar y analizar logs usando comandos de línea.

**Preguntas de reflexión:**

1. ¿Cómo buscas logs específicos en múltiples Pods?
2. ¿Cómo analizas patrones en logs?
3. ¿Cuáles son las limitaciones de analizar logs así?

---

=== Ejercicio 8.10: Health Checks y Probes

**Objetivo:** Implementar liveness, readiness y startup probes para monitoreo.

**Descripción:**

Crea Pods con diferentes tipos de probes para que Kubernetes monitoree su salud.

**Tareas:**

1. Crea un Pod con liveness probe:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: liveness-probe-pod
spec:
  containers:
  - name: app
    image: busybox
    ports:
    - containerPort: 8080
    command: ["sh", "-c"]
    args:
    - |
      counter=0
      while true; do
        # Simula un servidor en el puerto 8080
        {
          if [ $((counter % 20)) -lt 15 ]; then
            echo "HTTP/1.1 200 OK"
            echo ""
            echo "OK"
          else
            echo "HTTP/1.1 500 Internal Server Error"
            echo ""
            echo "ERROR"
          fi
        } | nc -l -p 8080 -q 1
        counter=$((counter+1))
        sleep 1
      done

    livenessProbe:
      httpGet:
        path: /
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 3
EOF
----

2. Observa el Pod:
+
[source,bash]
----
kubectl get pod liveness-probe-pod -w
----

3. Describe el Pod para ver resultados del probe:
+
[source,bash]
----
kubectl describe pod liveness-probe-pod
----

4. Crea un Pod con readiness probe:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: readiness-probe-pod
spec:
  containers:
  - name: app
    image: busybox
    ports:
    - containerPort: 8080
    command: ["sh", "-c"]
    args:
    - |
      echo "Initializing..."
      sleep 5
      echo "Ready!"
      while true; do
        {
          echo "HTTP/1.1 200 OK"
          echo ""
          echo "READY"
        } | nc -l -p 8080 -q 1
      done

    readinessProbe:
      httpGet:
        path: /
        port: 8080
      initialDelaySeconds: 2
      periodSeconds: 3
EOF
----

5. Observa cómo tarda en estar ready:
+
[source,bash]
----
kubectl get pod readiness-probe-pod -w
----

6. Crea un Pod con startup probe:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: startup-probe-pod
spec:
  containers:
  - name: app
    image: busybox
    ports:
    - containerPort: 8080
    command: ["sh", "-c"]
    args:
    - |
      echo "Starting application... (slow startup)"
      for i in $(seq 1 15); do
        echo "Initializing step $i..."
        sleep 1
      done
      echo "Application ready!"
      while true; do
        {
          echo "HTTP/1.1 200 OK"
          echo ""
          echo "RUNNING"
        } | nc -l -p 8080 -q 1
      done

    startupProbe:
      httpGet:
        path: /
        port: 8080
      failureThreshold: 30
      periodSeconds: 1
EOF
----

7. Observa el pod en arranque:
+
[source,bash]
----
kubectl logs startup-probe-pod -f
----

8. Crea un Pod con todos los probes:
+
[source,bash]
----
kubectl apply -f - <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: all-probes-pod
spec:
  containers:
  - name: app
    image: busybox
    ports:
    - containerPort: 8080
    command: ["sh", "-c"]
    args:
    - |
      sleep 3
      while true; do
        {
          echo "HTTP/1.1 200 OK"
          echo ""
          echo "HEALTHY"
        } | nc -l -p 8080 -q 1
      done

    startupProbe:
      httpGet:
        path: /
        port: 8080
      failureThreshold: 10
      periodSeconds: 1

    readinessProbe:
      httpGet:
        path: /
        port: 8080
      periodSeconds: 5

    livenessProbe:
      httpGet:
        path: /
        port: 8080
      periodSeconds: 10
      failureThreshold: 2
EOF
----

9. Describe el Pod para ver todos los probes:
+
[source,bash]
----
kubectl describe pod all-probes-pod
----

10. Limpia:
+
[source,bash]
----
kubectl delete pod liveness-probe-pod readiness-probe-pod startup-probe-pod all-probes-pod
----

**Verificación:**

Los Pods deben mostrar resultados de probes en su estado.

**Preguntas de reflexión:**

1. ¿Cuál es la diferencia entre liveness, readiness y startup probes?
2. ¿Cómo usa Kubernetes los resultados de los probes?
3. ¿Cuándo debería usar cada tipo de probe?

---

== Resumen de Conceptos

Estos ejercicios del Módulo 8 cubren:

* **Ejercicio 8.1**: Obtener logs de contenedores
* **Ejercicio 8.2**: Logs en Pods multi-contenedor
* **Ejercicio 8.3**: Logs de Deployments
* **Ejercicio 8.4**: Eventos del cluster
* **Ejercicio 8.5**: Métricas con kubectl top
* **Ejercicio 8.6**: Resource limits y monitoreo
* **Ejercicio 8.7**: Logging con sidecar pattern
* **Ejercicio 8.8**: Exposición de métricas
* **Ejercicio 8.9**: Análisis de logs
* **Ejercicio 8.10**: Health checks y probes

== Módulo 9: Escalado y Performance

=== Ejercicio 9.1: Horizontal Pod Autoscaler con métrica CPU

**Objetivo de aprendizaje:**
Crear un HPA que automáticamente escale un Deployment basado en utilización de CPU. Comprender cómo el HPA monitorea métricas y toma decisiones de escalado.

**Descripción:**
Crearás un Deployment con requests de CPU, configurarás un HPA para escalar basado en CPU, y luego generarás carga para observar el escalado automático.

**Tareas:**

1. Crea un archivo YAML llamado `hpa-cpu-deployment.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-intensive-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cpu-app
  template:
    metadata:
      labels:
        app: cpu-app
    spec:
      containers:
      - name: app
        image: polinux/stress
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi
        args: ["stress", "--cpu", "1", "--timeout", "600s"]
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: cpu-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: cpu-intensive-app
  minReplicas: 2
  maxReplicas: 8
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
    scaleDown:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
----

2. Despliega el Deployment y el HPA:
+
[source,bash]
----
kubectl apply -f hpa-cpu-deployment.yaml
----

3. Verifica que el HPA está activo:
+
[source,bash]
----
kubectl get hpa
kubectl describe hpa cpu-hpa
----

4. Monitorea el escalado en tiempo real:
+
[source,bash]
----
# En una ventana de terminal
kubectl get hpa -w

# En otra ventana
kubectl get pods -l app=cpu-app -w
----

5. Limpia los recursos cuando termines:
+
[source,bash]
----
kubectl delete -f hpa-cpu-deployment.yaml
----

**Preguntas de reflexión:**

1. ¿Cuándo comenzó el HPA a escalar hacia arriba? ¿Cuál fue el trigger?
2. ¿Cuántas replicas máximas se alcanzaron?
3. ¿Cuál es la diferencia entre stabilizationWindowSeconds en scaleUp vs scaleDown?

---

=== Ejercicio 9.2: HPA con múltiples métricas (CPU y Memoria)

**Objetivo de aprendizaje:**
Configurar un HPA que evalúe múltiples métricas simultáneamente. Comprender cómo el HPA elige el escalado cuando hay conflicto entre métricas.

**Descripción:**
Crearás un HPA que escale basado en CPU O memoria, lo que significa que cualquiera de las dos métricas que alcance su umbral provocará escalado.

**Tareas:**

1. Crea un archivo YAML llamado `hpa-multi-metrics.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: memory-intensive-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mem-app
  template:
    metadata:
      labels:
        app: mem-app
    spec:
      containers:
      - name: app
        image: polinux/stress
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 512Mi
        args: ["stress", "--vm", "1", "--vm-bytes", "100M", "--timeout", "600s"]
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: multi-metric-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: memory-intensive-app
  minReplicas: 2
  maxReplicas: 6
  metrics:
  # Métrica 1: CPU
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
  # Métrica 2: Memoria
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 2
        periodSeconds: 15
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      selectPolicy: Min
----

2. Despliega el HPA multi-métrica:
+
[source,bash]
----
kubectl apply -f hpa-multi-metrics.yaml
----

3. Monitorea las métricas del HPA:
+
[source,bash]
----
# Ver estado actual
kubectl get hpa multi-metric-hpa

# Ver detalles de ambas métricas
kubectl describe hpa multi-metric-hpa

# Salida esperada:
# Metrics:
#   resource cpu on pods (as a percentage of request):
#     Current / Target: ...
#   resource memory on pods (as a percentage of request):
#     Current / Target: ...
----

4. Observa el comportamiento de escalado:
+
[source,bash]
----
# Monitorear cambios
kubectl get hpa multi-metric-hpa -w
----

5. Limpia los recursos:
+
[source,bash]
----
kubectl delete -f hpa-multi-metrics.yaml
----

**Preguntas de reflexión:**

1. ¿Cuál métrica alcanzó su umbral primero?
2. ¿Cómo selecciona el HPA qué métrica usar para escalar?
3. ¿Cuál es la diferencia entre `selectPolicy: Max` y `selectPolicy: Min`?

---

=== Ejercicio 9.3: Vertical Pod Autoscaler en modo "off" (Recomendaciones)

**Objetivo de aprendizaje:**
Usar VPA para obtener recomendaciones de recursos sin aplicarlas automáticamente. Comprender cómo VPA calcula recomendaciones basadas en uso histórico.

**Descripción:**
Crearás un Deployment con resources mal configurados y usarás VPA en modo "off" para ver qué cambios serían recomendados sin ejecutarlos.

**Tareas:**

1. Crea un archivo YAML llamado `vpa-recommendations.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: poorly-configured-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: vpa-test-app
  template:
    metadata:
      labels:
        app: vpa-test-app
    spec:
      containers:
      - name: app
        image: nginx:1.21
        resources:
          requests:
            cpu: 1000m        # Sobrer-estimado
            memory: 512Mi     # Sobrer-estimado
          limits:
            cpu: 2000m
            memory: 1Gi
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: app-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: poorly-configured-app
  updatePolicy:
    updateMode: "off"       # Solo recomendaciones, sin aplicar
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 1000m
        memory: 1Gi
----

2. Verifica que VPA esté instalado (si no, asume que está disponible):
+
[source,bash]
----
kubectl api-resources | grep VerticalPodAutoscaler
----

3. Despliega el Deployment y VPA:
+
[source,bash]
----
kubectl apply -f vpa-recommendations.yaml
----

4. Espera a que VPA recolecte datos (2-3 minutos):
+
[source,bash]
----
kubectl get vpa
----

5. Ver las recomendaciones de VPA:
+
[source,bash]
----
kubectl describe vpa app-vpa

# Salida esperada:
# Recommendation:
#   Container Recommendations:
#   - Container Name: app
#     Lower Bound:
#       cpu: 10m
#       memory: 20Mi
#     Target:
#       cpu: 50m
#       memory: 50Mi
#     Upper Bound:
#       cpu: 100m
#       memory: 100Mi
----

6. Comparar requests actuales vs recomendados:
+
[source,bash]
----
kubectl get deployment poorly-configured-app -o yaml | grep -A 4 "requests:"
----

7. Limpia los recursos:
+
[source,bash]
----
kubectl delete -f vpa-recommendations.yaml
----

**Preguntas de reflexión:**

1. ¿Cuál es el `Target` que VPA recomendó para CPU y memoria?
2. ¿Cuánta CPU y memoria podría ahorrar aplicando estas recomendaciones?
3. ¿Por qué es útil ver recomendaciones antes de aplicarlas automáticamente?

---

=== Ejercicio 9.4: Vertical Pod Autoscaler en modo "initial"

**Objetivo de aprendizaje:**
Usar VPA para ajustar automáticamente los recursos de nuevos Pods, dejando los existentes sin cambios. Comprender cómo VPA puede migrar gradualmente hacia mejor configuración.

**Descripción:**
Configurarás VPA en modo "initial" y luego actualizarás el Deployment para ver cómo VPA ajusta los nuevos Pods.

**Tareas:**

1. Crea un archivo YAML llamado `vpa-initial-mode.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vpa-initial-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: initial-vpa-app
  template:
    metadata:
      labels:
        app: initial-vpa-app
    spec:
      containers:
      - name: app
        image: nginx:1.21
        resources:
          requests:
            cpu: 500m        # Estimado inicial
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 512Mi
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: initial-mode-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vpa-initial-app
  updatePolicy:
    updateMode: "initial"   # Ajusta solo Pods nuevos
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 50m
        memory: 64Mi
      maxAllowed:
        cpu: 500m
        memory: 512Mi
----

2. Despliega el Deployment con VPA:
+
[source,bash]
----
kubectl apply -f vpa-initial-mode.yaml
----

3. Registra los requests actuales de los Pods existentes:
+
[source,bash]
----
kubectl get pods -l app=initial-vpa-app -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[0].resources.requests}{"\n"}{end}'
----

4. Espera a que VPA genere recomendaciones (2-3 minutos):
+
[source,bash]
----
kubectl describe vpa initial-mode-vpa
----

5. Fuerza un rollout para crear nuevos Pods:
+
[source,bash]
----
kubectl rollout restart deployment/vpa-initial-app
----

6. Verifica que los nuevos Pods tienen diferentes requests:
+
[source,bash]
----
kubectl get pods -l app=initial-vpa-app -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[0].resources.requests}{"\n"}{end}'
----

7. Limpia los recursos:
+
[source,bash]
----
kubectl delete -f vpa-initial-mode.yaml
----

**Preguntas de reflexión:**

1. ¿Cuáles fueron los requests del primer set de Pods?
2. ¿Cuáles fueron los requests del nuevo set después del rollout?
3. ¿Por qué es útil el modo "initial" en lugar de "off" o "recreate"?

---

=== Ejercicio 9.5: Resource Quotas para limitar uso de namespace

**Objetivo de aprendizaje:**
Crear un Resource Quota para limitar el consumo total de CPU y memoria en un namespace. Comprender cómo Kubernetes puede prevenir que un namespace monopolice recursos.

**Descripción:**
Crearás un namespace con un Resource Quota restrictivo y luego intentarás desplegar Pods que exceden el quota.

**Tareas:**

1. Crea un archivo YAML llamado `resource-quota.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: quota-demo
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: quota-demo
spec:
  hard:
    requests.cpu: "2"          # 2 CPUs totales
    requests.memory: "2Gi"     # 2 GB memoria
    limits.cpu: "4"            # 4 CPUs en limits
    limits.memory: "4Gi"       # 4 GB en limits
    pods: "10"                 # Máximo 10 Pods
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values: ["default"]
---
apiVersion: v1
kind: LimitRange
metadata:
  name: container-limits
  namespace: quota-demo
spec:
  limits:
  - max:
      cpu: "1"
      memory: "1Gi"
    min:
      cpu: "50m"
      memory: "64Mi"
    default:
      cpu: "100m"
      memory: "128Mi"
    defaultRequest:
      cpu: "50m"
      memory: "64Mi"
    type: Container
----

2. Crea el namespace y quota:
+
[source,bash]
----
kubectl apply -f resource-quota.yaml
----

3. Verifica el quota:
+
[source,bash]
----
kubectl describe resourcequota compute-quota -n quota-demo
----

4. Despliega un Deployment que usa 1 CPU y 512 Mi de memoria (dentro del quota):
+
[source,bash]
----
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: within-quota-app
  namespace: quota-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: test-app
  template:
    metadata:
      labels:
        app: test-app
    spec:
      containers:
      - name: app
        image: nginx:1.21
        resources:
          requests:
            cpu: 500m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 512Mi
EOF
----

5. Verifica el uso actual del quota:
+
[source,bash]
----
kubectl describe resourcequota compute-quota -n quota-demo
----

6. Intenta desplegar otro Deployment que excedería el quota:
+
[source,bash]
----
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: exceeds-quota-app
  namespace: quota-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: excess-app
  template:
    metadata:
      labels:
        app: excess-app
    spec:
      containers:
      - name: app
        image: nginx:1.21
        resources:
          requests:
            cpu: 1500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 2Gi
EOF

# Salida esperada: Error - exceeded quota
----

7. Limpia los recursos:
+
[source,bash]
----
kubectl delete namespace quota-demo
----

**Preguntas de reflexión:**

1. ¿Qué sucedió cuando intentaste crear un Deployment que excedía el quota?
2. ¿Cómo se calcula el uso en el quota (suma de requests o suma de limits)?
3. ¿Cuáles son otros recursos que se pueden limitar con Resource Quota?

---

=== Ejercicio 9.6: Pod Anti-Affinity para alta disponibilidad

**Objetivo de aprendizaje:**
Configurar Pod Anti-Affinity para asegurar que replicas de una aplicación se distribuyan en diferentes nodos. Comprender cómo mejorar la tolerancia a fallos.

**Descripción:**
Crearás un Deployment con Pod Anti-Affinity que requiere que cada Pod esté en un nodo diferente.

**Tareas:**

1. Crea un archivo YAML llamado `pod-anti-affinity.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: anti-affinity-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: distributed-app
  template:
    metadata:
      labels:
        app: distributed-app
    spec:
      affinity:
        podAntiAffinity:
          # Requerido: cada Pod en nodo diferente
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - distributed-app
            topologyKey: kubernetes.io/hostname

      containers:
      - name: app
        image: nginx:1.21
        resources:
          requests:
            cpu: 100m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 128Mi
        ports:
        - containerPort: 80
----

2. Despliega el Deployment:
+
[source,bash]
----
kubectl apply -f pod-anti-affinity.yaml
----

3. Verifica la distribución de Pods en nodos:
+
[source,bash]
----
kubectl get pods -o wide -l app=distributed-app

# Salida esperada: cada Pod en diferentes nodos
----

4. Ver detalles de scheduling:
+
[source,bash]
----
kubectl describe deployment anti-affinity-app

# Busca información sobre affinity
----

5. Observa el comportamiento cuando intentas escalar:
+
[source,bash]
----
# Escala a 5 replicas (puede fallar si no hay suficientes nodos)
kubectl scale deployment anti-affinity-app --replicas=5

# Ver estado
kubectl get pods -o wide -l app=distributed-app

# Algunos pueden quedar Pending si no hay suficientes nodos
----

6. Limpia los recursos:
+
[source,bash]
----
kubectl delete -f pod-anti-affinity.yaml
----

**Preguntas de reflexión:**

1. ¿Cuál fue la distribución de Pods en los nodos?
2. ¿Qué pasó cuando intentaste escalar a 5 replicas?
3. ¿Cuál es la diferencia entre `required` y `preferred` en Pod Anti-Affinity?

---

=== Ejercicio 9.7: Taints y Tolerations para nodos especializados

**Objetivo de aprendizaje:**
Usar Taints para marcar nodos especializados y Tolerations para permitir que solo ciertos Pods se ejecuten en ellos. Comprender cómo aislar workloads.

**Descripción:**
Simulará un escenario donde algunos nodos están reservados para aplicaciones específicas, como GPU o alta memoria.

**Tareas:**

1. Obtén el nombre de un nodo:
+
[source,bash]
----
kubectl get nodes

# Elige un nodo para experimentar (no debe ser crítico)
# Asumamos que el nodo se llama "worker-node-1"
----

2. Agrega un taint al nodo (lo que impide que Pods normales se ejecuten):
+
[source,bash]
----
# Este es un comando informativo, requiere node existente
# kubectl taint nodes worker-node-1 gpu=required:NoSchedule

# Para este ejercicio, crea un Pod con toleration
----

3. Crea un archivo YAML llamado `taints-tolerations.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: gpu-application
spec:
  containers:
  - name: app
    image: nginx:1.21

  # Pod normal SIN toleration
  tolerations: []

---
apiVersion: v1
kind: Pod
metadata:
  name: gpu-tolerating-app
spec:
  containers:
  - name: app
    image: nginx:1.21

  # Pod CON toleration para el taint
  tolerations:
  - key: gpu
    operator: Equal
    value: required
    effect: NoSchedule
----

4. Ver información de taints en nodos (informativo):
+
[source,bash]
----
# Ver taints del cluster
kubectl describe nodes | grep Taints

# Ver master node taints (por defecto tienen NoSchedule)
----

5. Crear un Pod con toleration de demostración:
+
[source,bash]
----
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: master-tolerating-pod
spec:
  tolerations:
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule

  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: 10m
        memory: 32Mi
EOF
----

6. Verifica que el Pod está corriendo en un nodo master/control-plane:
+
[source,bash]
----
kubectl get pods master-tolerating-pod -o wide
----

7. Limpia los recursos:
+
[source,bash]
----
kubectl delete pod master-tolerating-pod
----

**Preguntas de reflexión:**

1. ¿Cuáles son los taints por defecto en nodos master?
2. ¿Cuál es la diferencia entre `NoSchedule`, `PreferNoSchedule` y `NoExecute`?
3. ¿Cuándo usarías Taints en lugar de Node Affinity?

---

=== Ejercicio 9.8: Priority Classes para preemption

**Objetivo de aprendizaje:**
Crear Priority Classes para definir la importancia de Pods y comprender cómo el scheduler puede preemptar (desalojar) Pods de baja prioridad cuando hay demanda de Pods críticos.

**Descripción:**
Crearás dos Priority Classes (alta y baja) y observarás cómo Kubernetes prioriza Pods críticos.

**Tareas:**

1. Crea un archivo YAML llamado `priority-classes.yaml`:
+
[source,yaml]
----
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: critical-priority
value: 1000
globalDefault: false
description: "Aplicaciones críticas de producción"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 100
globalDefault: false
description: "Batch jobs y operaciones no críticas"
---
apiVersion: v1
kind: Pod
metadata:
  name: critical-pod
spec:
  priorityClassName: critical-priority
  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: 100m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 128Mi
---
apiVersion: v1
kind: Pod
metadata:
  name: batch-pod
spec:
  priorityClassName: low-priority
  containers:
  - name: app
    image: busybox
    resources:
      requests:
        cpu: 100m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 128Mi
    command: ["sleep", "3600"]
----

2. Crea las Priority Classes:
+
[source,bash]
----
kubectl apply -f priority-classes.yaml
----

3. Verifica que las Priority Classes fueron creadas:
+
[source,bash]
----
kubectl get priorityclass

# Salida:
# NAME                       VALUE   GLOBAL-DEFAULT   AGE
# critical-priority          1000    false            5m
# low-priority               100     false            5m
----

4. Ver prioridades de los Pods:
+
[source,bash]
----
kubectl get pods -o custom-columns=NAME:.metadata.name,PRIORITY:.spec.priorityClassName
----

5. Describe los Pods para ver más detalles:
+
[source,bash]
----
kubectl describe pod critical-pod
kubectl describe pod batch-pod

# Busca "Priority" en la salida
----

6. Limpia los recursos:
+
[source,bash]
----
kubectl delete pod critical-pod batch-pod
kubectl delete priorityclass critical-priority low-priority
----

**Preguntas de reflexión:**

1. ¿Cuál es la diferencia entre `value` en Priority Class?
2. ¿Qué significa `globalDefault: true` en una Priority Class?
3. ¿Cuándo Kubernetes preempta (desaloja) un Pod de baja prioridad?

---

=== Ejercicio 9.9: Resource Limits y monitoreo con kubectl top

**Objetivo de aprendizaje:**
Establecer resource limits para Pods y usar kubectl top para monitorear su consumo real versus los límites definidos.

**Descripción:**
Crearás Pods con diferentes límites de recursos y observarás cómo el kernel los enforza.

**Tareas:**

1. Crea un archivo YAML llamado `resource-limits-demo.yaml`:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: limited-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: limited-app
  template:
    metadata:
      labels:
        app: limited-app
    spec:
      containers:
      - name: app
        image: nginx:1.21
        resources:
          requests:
            cpu: 100m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 128Mi
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: memory-leak-simulator
spec:
  containers:
  - name: app
    image: polinux/stress
    args: ["stress", "--vm", "1", "--vm-bytes", "200M", "--timeout", "300s"]
    resources:
      requests:
        cpu: 100m
        memory: 64Mi
      limits:
        cpu: 500m
        memory: 256Mi        # Será excedido, causa OOM kill
----

2. Despliega los recursos:
+
[source,bash]
----
kubectl apply -f resource-limits-demo.yaml
----

3. Espera unos segundos y monitorea los Pods:
+
[source,bash]
----
kubectl get pods -o wide
----

4. Ver consumo de recursos con kubectl top (si Metrics Server está instalado):
+
[source,bash]
----
# Ver consumo de nodos
kubectl top nodes

# Ver consumo de Pods
kubectl top pods

# Ver consumo de pods específicos
kubectl top pod limited-app-xxx limited-app-yyy memory-leak-simulator

# Ordena por CPU
kubectl top pods --sort-by=cpu
----

5. Describe el Pod que simula memory leak:
+
[source,bash]
----
kubectl describe pod memory-leak-simulator

# Busca "OOMKilled" o estado de contenedor
----

6. Ver los logs si está disponible:
+
[source,bash]
----
kubectl get events | grep memory-leak-simulator
----

7. Limpia los recursos:
+
[source,bash]
----
kubectl delete -f resource-limits-demo.yaml
----

**Preguntas de reflexión:**

1. ¿Qué pasó cuando el Pod intentó usar más memoria que su límite?
2. ¿Cuál es la diferencia entre OOMKilled y Evicted?
3. ¿Cómo puedes saber si un Pod excedió sus límites de CPU?

---

=== Ejercicio 9.10: Cluster Autoscaler y Node Affinity

**Objetivo de aprendizaje:**
Entender cómo Cluster Autoscaler interactúa con Node Affinity para escalar nodos cuando Pods no pueden ser scheduled.

**Descripción:**
Crearás Deployments con Node Affinity y observarás cómo Cluster Autoscaler crearía nuevos nodos si fuera necesario.

**Tareas:**

1. Ver información de nodos del cluster:
+
[source,bash]
----
kubectl get nodes
kubectl describe nodes | head -50
----

2. Crea un archivo YAML llamado `cluster-autoscaler-demo.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: autoscaler-demo
---
apiVersion: v1
kind: Pod
metadata:
  name: affinity-demo-pod
  namespace: autoscaler-demo
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-role.kubernetes.io/control-plane
            operator: NotIn
            values:
            - "true"
          - key: disktype
            operator: In
            values:
            - ssd

  containers:
  - name: app
    image: nginx:1.21
    resources:
      requests:
        cpu: 500m
        memory: 256Mi
      limits:
        cpu: 1000m
        memory: 512Mi

  # Si el nodo especificado no existe, el Pod quedará Pending
  # Si Cluster Autoscaler estuviera activo, crearía un nuevo nodo
----

3. Despliega el demo:
+
[source,bash]
----
kubectl apply -f cluster-autoscaler-demo.yaml
----

4. Verifica el estado del Pod:
+
[source,bash]
----
kubectl get pods -n autoscaler-demo

# Probablemente estará en estado Pending
kubectl describe pod affinity-demo-pod -n autoscaler-demo

# Busca "Unable to schedule" en los eventos
----

5. Ver Pending Pods (esto es lo que triggerea Cluster Autoscaler):
+
[source,bash]
----
kubectl get pods --field-selector=status.phase=Pending -A
----

6. Monitorear Cluster Autoscaler (si existe):
+
[source,bash]
----
# Ver logs de Cluster Autoscaler (si está instalado)
kubectl logs -n kube-system -l app=cluster-autoscaler -f --tail=20

# En un cluster real con autoscaling:
# Verías logs como: "Scale-up is enabled. Will try to scale nodes..."
----

7. Limpia los recursos:
+
[source,bash]
----
kubectl delete namespace autoscaler-demo
----

**Preguntas de reflexión:**

1. ¿Por qué el Pod quedó en estado Pending?
2. ¿Qué sucedería en un cluster con Cluster Autoscaler activo?
3. ¿Cuál es la diferencia entre HPA (escala Pods) y Cluster Autoscaler (escala nodos)?

---

== Resumen de Conceptos

Estos ejercicios del Módulo 9 cubren:

* **Ejercicio 9.1**: HPA con métrica CPU
* **Ejercicio 9.2**: HPA con múltiples métricas
* **Ejercicio 9.3**: VPA recomendaciones (modo off)
* **Ejercicio 9.4**: VPA auto-actualización (modo initial)
* **Ejercicio 9.5**: Resource Quotas por namespace
* **Ejercicio 9.6**: Pod Anti-Affinity para distribución
* **Ejercicio 9.7**: Taints y Tolerations
* **Ejercicio 9.8**: Priority Classes
* **Ejercicio 9.9**: Resource Limits y monitoreo
* **Ejercicio 9.10**: Cluster Autoscaler y scheduling

== Módulo 10: Helm y Package Management

=== Ejercicio 10.1: Gestión de Repositorios de Helm

**Objetivo de aprendizaje:**
Aprender a agregar, actualizar y gestionar repositorios de Helm. Entender cómo Helm accede a Charts públicos desde repositorios remotos.

**Descripción:**
Trabajarás con repositorios de Helm, agregando fuentes oficiales y buscando Charts disponibles.

**Tareas:**

1. Verifica que Helm está instalado:
+
[source,bash]
----
helm version

# Salida esperada:
# version.BuildInfo{Version:"v3.x.x", ...}
----

2. Agregar el repositorio oficial de Helm:
+
[source,bash]
----
# Agregar repositorio stable
helm repo add stable https://charts.helm.sh/stable

# Agregar repositorio Bitnami (Charts de alta calidad)
helm repo add bitnami https://charts.bitnami.com/bitnami

# Ver repositorios agregados
helm repo list

# Salida esperada:
# NAME      URL
# stable    https://charts.helm.sh/stable
# bitnami   https://charts.bitnami.com/bitnami
----

3. Actualizar índices de repositorios:
+
[source,bash]
----
# Actualizar todos los repositorios
helm repo update

# Salida esperada:
# Hang tight while we grab the latest from your chart repositories...
# ...Successfully got an update from the "stable" chart repository
# ...Successfully got an update from the "bitnami" chart repository
# Update Complete. ⎈ Happy Helming!
----

4. Buscar Charts en repositorios:
+
[source,bash]
----
# Buscar nginx
helm search repo nginx

# Buscar postgresql
helm search repo postgresql

# Buscar todos los Charts en bitnami
helm search repo bitnami | head -20
----

5. Ver detalles de un Chart específico:
+
[source,bash]
----
# Ver metadatos del Chart
helm show chart bitnami/nginx

# Ver valores por defecto
helm show values bitnami/nginx | head -30

# Ver todo (Chart + README + valores)
helm show all bitnami/nginx | head -50
----

6. Listar versiones disponibles de un Chart:
+
[source,bash]
----
# Ver todas las versiones
helm search repo bitnami/nginx --versions | head -10
----

7. Remover un repositorio:
+
[source,bash]
----
# Remover repositorio stable (opcional)
# helm repo remove stable

# Ver repositorios actuales
helm repo list
----

**Preguntas de reflexión:**

1. ¿Cuál es la diferencia entre el repositorio "stable" y "bitnami"?
2. ¿Por qué es importante actualizar los índices de repositorios con `helm repo update`?
3. ¿Cómo podrías usar un repositorio privado o corporativo?

---

=== Ejercicio 10.2: Instalación de un Chart desde Repositorio

**Objetivo de aprendizaje:**
Instalar un Chart desde un repositorio remoto con valores por defecto. Comprender el ciclo completo de instalación de Helm.

**Descripción:**
Instalarás un Chart nginx desde Bitnami y verificarás la release creada.

**Tareas:**

1. Instalar nginx desde Bitnami con valores por defecto:
+
[source,bash]
----
# Instalación simple
helm install my-nginx bitnami/nginx

# Salida esperada:
# NAME: my-nginx
# LAST DEPLOYED: ...
# NAMESPACE: default
# STATUS: deployed
# ...
----

2. Listar releases instaladas:
+
[source,bash]
----
# Ver todas las releases
helm list

# Salida esperada:
# NAME      NAMESPACE STATUS    CHART        APP VERSION
# my-nginx  default   deployed  nginx-15...  1.25.x
----

3. Ver status de la release:
+
[source,bash]
----
# Status detallado
helm status my-nginx

# Ver recursos creados en Kubernetes
kubectl get all -l app.kubernetes.io/instance=my-nginx

# Ver Pods
kubectl get pods -l app.kubernetes.io/instance=my-nginx
----

4. Ver valores de la release:
+
[source,bash]
----
# Ver solo valores personalizados (sobrescritos)
helm get values my-nginx

# Ver todos los valores (incluyendo defaults)
helm get values my-nginx --all | head -50
----

5. Ver manifiestos YAML desplegados:
+
[source,bash]
----
# Ver todos los manifiestos
helm get manifest my-nginx | head -50
----

6. Ver Chart.yaml de la release:
+
[source,bash]
----
# Ver metadatos del Chart
helm get chart my-nginx
----

7. Ver notas post-instalación:
+
[source,bash]
----
# Ver instrucciones
helm get notes my-nginx
----

8. Desinstalar la release:
+
[source,bash]
----
# Desinstalar
helm uninstall my-nginx

# Verificar que se removió
helm list
----

**Preguntas de reflexión:**

1. ¿Cuál fue el status de la release después de instalar?
2. ¿Cuáles fueron los recursos de Kubernetes creados por Helm?
3. ¿Cómo se diferencian los valores por defecto de los personalizados?

---

=== Ejercicio 10.3: Instalación con Valores Personalizados

**Objetivo de aprendizaje:**
Instalar un Chart personalizando valores con --set y archivos values. Comprender cómo parametrizar Deployments.

**Descripción:**
Instalarás nginx con réplicas, tipo de servicio y otras configuraciones personalizadas.

**Tareas:**

1. Crear un archivo values personalizado:
+
[source,bash]
----
cat > custom-values.yaml << 'EOF'
# Configuración personalizada para nginx
replicaCount: 3

image:
  repository: bitnami/nginx
  tag: "1.25.0"

service:
  type: LoadBalancer
  port: 80

resources:
  limits:
    cpu: 200m
    memory: 256Mi
  requests:
    cpu: 100m
    memory: 128Mi

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 5
  targetCPUUtilizationPercentage: 70
EOF
----

2. Instalar con archivo values:
+
[source,bash]
----
# Instalar usando archivo values
helm install my-nginx-custom bitnami/nginx -f custom-values.yaml

# Ver detalles
helm status my-nginx-custom
----

3. Verificar configuración:
+
[source,bash]
----
# Ver valores personalizados
helm get values my-nginx-custom

# Ver Pods creados
kubectl get pods -l app.kubernetes.io/instance=my-nginx-custom

# Ver Service
kubectl get svc -l app.kubernetes.io/instance=my-nginx-custom
----

4. Instalar con --set desde línea de comandos:
+
[source,bash]
----
# Instalar con múltiples --set
helm install my-nginx-cli bitnami/nginx \
  --set replicaCount=2 \
  --set service.type=NodePort \
  --set resources.requests.cpu=50m

# Ver valores
helm get values my-nginx-cli
----

5. Combinar archivo values y --set:
+
[source,bash]
----
# El --set sobreescribe el archivo values
helm install my-nginx-combined bitnami/nginx \
  -f custom-values.yaml \
  --set replicaCount=4

# Ver valores (replicaCount debe ser 4)
helm get values my-nginx-combined | grep replicaCount
----

6. Usar --set-string para valores complejos:
+
[source,bash]
----
# --set-string evita conversiones automáticas
helm install my-nginx-string bitnami/nginx \
  --set-string image.tag="1.25.0"

# Ver el tag (debe ser string, no convertido a número)
helm get values my-nginx-string | grep tag
----

7. Ver manifiestos antes de instalar (--dry-run):
+
[source,bash]
----
# Simulación: ver qué se instalaría sin hacerlo realmente
helm install my-nginx-test bitnami/nginx \
  --dry-run \
  --debug | head -50

# No crea recursos reales, solo muestra salida
----

8. Limpiar todas las releases:
+
[source,bash]
----
helm uninstall my-nginx-custom my-nginx-cli my-nginx-combined my-nginx-string
----

**Preguntas de reflexión:**

1. ¿Cuál es la diferencia entre usar --set y un archivo values?
2. ¿Qué pasa cuando especificas el mismo valor en archivo values y --set?
3. ¿Por qué --dry-run es útil antes de instalar en producción?

---

=== Ejercicio 10.4: Actualización de Releases (helm upgrade)

**Objetivo de aprendizaje:**
Actualizar una release existente con nuevas versiones del Chart o cambios de configuración. Entender cómo Helm maneja upgrades.

**Descripción:**
Instalarás un Chart, luego lo actualizarás cambiando valores y versión.

**Tareas:**

1. Instalar una versión inicial:
+
[source,bash]
----
# Instalar con valores iniciales
helm install my-app bitnami/nginx \
  --set replicaCount=2 \
  --set image.tag="1.24.0"

# Ver status
helm status my-app
----

2. Ver historial de la release (solo 1 entry):
+
[source,bash]
----
helm history my-app

# Salida esperada:
# REVISION STATUS      CHART         APP VERSION DESCRIPTION
# 1        deployed    nginx-15.0.0  1.24.0      Install complete
----

3. Realizar un upgrade cambiando valores:
+
[source,bash]
----
# Upgrade: aumentar replicas
helm upgrade my-app bitnami/nginx \
  --set replicaCount=4 \
  --set image.tag="1.25.0"

# Ver status
helm status my-app
----

4. Ver historial después del upgrade:
+
[source,bash]
----
helm history my-app

# Salida esperada (now 2 revisions):
# REVISION STATUS      CHART         APP VERSION DESCRIPTION
# 1        superseded  nginx-15.0.0  1.24.0      Install complete
# 2        deployed    nginx-15.0.0  1.25.0      Upgrade complete
----

5. Verificar cambios en los Pods:
+
[source,bash]
----
# Pods nuevos con nueva imagen
kubectl get pods -l app.kubernetes.io/instance=my-app

# Ver imagen en uso
kubectl get pod -l app.kubernetes.io/instance=my-app -o jsonpath='{range .items[*]}{.spec.containers[0].image}{"\n"}{end}'
----

6. Upgrade atómico (con rollback automático en error):
+
[source,bash]
----
# Si algo falla, automáticamente hace rollback
helm upgrade my-app bitnami/nginx \
  --set replicaCount=3 \
  --atomic

# Ver nuevo status
helm status my-app
----

7. Ver cambios que haría un upgrade (--dry-run):
+
[source,bash]
----
# Simulación: ver qué cambiaría sin hacerlo
helm upgrade my-app bitnami/nginx \
  --set replicaCount=5 \
  --dry-run \
  --debug | head -100

# No aplica los cambios, solo muestra
----

8. Obtener valores actuales:
+
[source,bash]
----
# Ver solo valores personalizados
helm get values my-app

# Ver todos (incluyendo defaults)
helm get values my-app --all | grep replicaCount
----

9. Limpiar:
+
[source,bash]
----
helm uninstall my-app
----

**Preguntas de reflexión:**

1. ¿Cuál fue la diferencia entre la primera instalación y el primer upgrade?
2. ¿Cuántos Pods se recrearon durante el upgrade?
3. ¿Para qué sirve el modo --atomic?

---

=== Ejercicio 10.5: Historial y Rollback de Releases

**Objetivo de aprendizaje:**
Comprender cómo Helm mantiene historial de cambios y cómo revertir a versiones anteriores.

**Descripción:**
Realizarás varios upgrades y luego harás rollback a revisiones anteriores.

**Tareas:**

1. Instalar una aplicación:
+
[source,bash]
----
helm install myapp bitnami/nginx \
  --set replicaCount=1 \
  --set image.tag="1.24.0"

helm history myapp
----

2. Realizar varios upgrades:
+
[source,bash]
----
# Upgrade 1
helm upgrade myapp bitnami/nginx \
  --set replicaCount=2 \
  --set image.tag="1.24.0"

# Upgrade 2
helm upgrade myapp bitnami/nginx \
  --set replicaCount=3 \
  --set image.tag="1.25.0"

# Upgrade 3
helm upgrade myapp bitnami/nginx \
  --set replicaCount=4 \
  --set image.tag="1.25.1"

# Ver historial completo
helm history myapp

# Salida esperada:
# REVISION STATUS      CHART  APP VERSION DESCRIPTION
# 1        superseded  ...    1.24.0      Install complete
# 2        superseded  ...    1.24.0      Upgrade complete
# 3        superseded  ...    1.25.0      Upgrade complete
# 4        deployed    ...    1.25.1      Upgrade complete
----

3. Ver estado actual:
+
[source,bash]
----
kubectl get pods -l app.kubernetes.io/instance=myapp | wc -l

# Debería mostrar 5 (header + 4 pods)
----

4. Hacer rollback a la revisión anterior:
+
[source,bash]
----
# Rollback sin argumentos = vuelve a la revisión anterior
helm rollback myapp

# Ver status
helm status myapp

# Ver historial (nueva revisión 5 con los valores de revisión 3)
helm history myapp
----

5. Rollback a una revisión específica:
+
[source,bash]
----
# Rollback a revisión 2 específicamente
helm rollback myapp 2

# Ver valores (replicaCount=2)
helm get values myapp | grep replicaCount

# Ver historial (nueva revisión 6)
helm history myapp
----

6. Ver diferencias entre revisiones:
+
[source,bash]
----
# Comparar revisión actual con revisión 1
helm get manifest myapp > current.yaml
helm get manifest myapp --revision 1 > revision1.yaml

# Ver diferencias (si tienes diff)
# diff revision1.yaml current.yaml | head -30
----

7. Limpiar:
+
[source,bash]
----
rm -f current.yaml revision1.yaml
helm uninstall myapp
----

**Preguntas de reflexión:**

1. ¿Cuántas revisiones se crearon después de 3 upgrades?
2. ¿Qué pasó con los Pods al hacer rollback?
3. ¿Cuál es la diferencia entre `helm rollback` sin argumentos y `helm rollback revision N`?

---

=== Ejercicio 10.6: Crear un Chart Básico desde Cero

**Objetivo de aprendizaje:**
Crear un Chart de Helm manualmente. Entender la estructura de un Chart y sus componentes.

**Descripción:**
Crearás un Chart simple para una aplicación nginx personalizada.

**Tareas:**

1. Crear la estructura del Chart:
+
[source,bash]
----
# Crear directorio del Chart
mkdir -p my-simple-app/templates

# Estructura:
# my-simple-app/
# ├── Chart.yaml
# ├── values.yaml
# ├── templates/
# │   ├── deployment.yaml
# │   ├── service.yaml
# │   └── _helpers.tpl
----

2. Crear Chart.yaml:
+
[source,bash]
----
cat > my-simple-app/Chart.yaml << 'EOF'
apiVersion: v2
name: my-simple-app
description: "Mi primer Chart de Helm"
type: application
version: 1.0.0
appVersion: "1.0.0"
maintainers:
  - name: Mi Nombre
    email: mi@email.com
EOF
----

3. Crear values.yaml:
+
[source,bash]
----
cat > my-simple-app/values.yaml << 'EOF'
# Configuración por defecto

replicaCount: 2

image:
  repository: nginx
  pullPolicy: IfNotPresent
  tag: "1.24.0"

service:
  type: ClusterIP
  port: 80
  targetPort: 80

resources:
  limits:
    cpu: 200m
    memory: 128Mi
  requests:
    cpu: 100m
    memory: 64Mi

nodeSelector: {}

tolerations: []

affinity: {}
EOF
----

4. Crear deployment.yaml template:
+
[source,bash]
----
cat > my-simple-app/templates/deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-{{ .Chart.Name }}
  labels:
    app: {{ .Chart.Name }}
    release: {{ .Release.Name }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: {{ .Chart.Name }}
      release: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app: {{ .Chart.Name }}
        release: {{ .Release.Name }}
    spec:
      containers:
      - name: {{ .Chart.Name }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        ports:
        - containerPort: {{ .Values.service.targetPort }}
        resources:
          {{- toYaml .Values.resources | nindent 10 }}
EOF
----

5. Crear service.yaml template:
+
[source,bash]
----
cat > my-simple-app/templates/service.yaml << 'EOF'
apiVersion: v1
kind: Service
metadata:
  name: {{ .Release.Name }}-{{ .Chart.Name }}
  labels:
    app: {{ .Chart.Name }}
    release: {{ .Release.Name }}
spec:
  type: {{ .Values.service.type }}
  selector:
    app: {{ .Chart.Name }}
    release: {{ .Release.Name }}
  ports:
  - port: {{ .Values.service.port }}
    targetPort: {{ .Values.service.targetPort }}
    protocol: TCP
EOF
----

6. Validar el Chart:
+
[source,bash]
----
# Validar sintaxis
helm lint my-simple-app

# Salida esperada:
# ==> Linting my-simple-app
# [OK] Chart.yaml is consistent
# ...
----

7. Ver manifiestos generados:
+
[source,bash]
----
# Ver YAML final sin instalar
helm template my-release my-simple-app

# Salida esperada: manifiestos YAML compilados
----

8. Instalar tu Chart:
+
[source,bash]
----
# Instalar desde Chart local
helm install my-test-release ./my-simple-app

# Ver resultados
helm list
helm status my-test-release

# Ver recursos creados
kubectl get deployment,svc -l chart=my-simple-app
----

9. Actualizar el Chart y hacer upgrade:
+
[source,bash]
----
# Cambiar valores
helm upgrade my-test-release ./my-simple-app \
  --set replicaCount=3 \
  --set image.tag="1.25.0"

# Ver cambios
helm get values my-test-release
----

10. Limpiar:
+
[source,bash]
----
helm uninstall my-test-release
rm -rf my-simple-app
----

**Preguntas de reflexión:**

1. ¿Cuál es la diferencia entre `{{ .Release.Name }}` y `{{ .Chart.Name }}`?
2. ¿Qué hace `{{ toYaml .Values.resources | nindent 10 }}`?
3. ¿Cómo cambiarías el Chart para soportar HPA?

---

=== Ejercicio 10.7: Templates con Lógica (Condicionales y Loops)

**Objetivo de aprendizaje:**
Usar lógica de Go templating en Helm para crear templates más flexibles. Comprender condicionales, loops y funciones.

**Descripción:**
Extenderás el Chart anterior con conditionals y loops para mayor flexibilidad.

**Tareas:**

1. Crear un Chart mejorado:
+
[source,bash]
----
mkdir -p advanced-chart/templates
----

2. Crear Chart.yaml:
+
[source,bash]
----
cat > advanced-chart/Chart.yaml << 'EOF'
apiVersion: v2
name: advanced-app
version: 1.0.0
appVersion: "1.0.0"
EOF
----

3. Crear values.yaml mejorado:
+
[source,bash]
----
cat > advanced-chart/values.yaml << 'EOF'
replicaCount: 2

image:
  repository: nginx
  tag: "1.25.0"

service:
  enabled: true
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  hostname: myapp.example.com

autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 70

env:
  DEBUG: "false"
  LOG_LEVEL: "info"

volumeMounts:
  - name: config
    mountPath: /etc/config

volumes:
  - name: config
    emptyDir: {}
EOF
----

4. Crear deployment con condicionales:
+
[source,bash]
----
cat > advanced-chart/templates/deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-advanced
  labels:
    app: advanced
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: advanced
  template:
    metadata:
      labels:
        app: advanced
    spec:
      containers:
      - name: app
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        ports:
        - containerPort: 8080

        # Env variables condicionales
        {{- if .Values.env }}
        env:
        {{- range $key, $value := .Values.env }}
        - name: {{ $key }}
          value: "{{ $value }}"
        {{- end }}
        {{- end }}

        # Volume mounts condicionales
        {{- if .Values.volumeMounts }}
        volumeMounts:
        {{- toYaml .Values.volumeMounts | nindent 8 }}
        {{- end }}

      # Volumes condicionales
      {{- if .Values.volumes }}
      volumes:
      {{- toYaml .Values.volumes | nindent 6 }}
      {{- end }}
EOF
----

5. Crear service con condicionales:
+
[source,bash]
----
cat > advanced-chart/templates/service.yaml << 'EOF'
{{- if .Values.service.enabled }}
apiVersion: v1
kind: Service
metadata:
  name: {{ .Release.Name }}-service
spec:
  type: {{ .Values.service.type }}
  selector:
    app: advanced
  ports:
  - port: {{ .Values.service.port }}
    targetPort: 8080
{{- end }}
EOF
----

6. Crear HPA condicional:
+
[source,bash]
----
cat > advanced-chart/templates/hpa.yaml << 'EOF'
{{- if .Values.autoscaling.enabled }}
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{ .Release.Name }}-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ .Release.Name }}-advanced
  minReplicas: {{ .Values.autoscaling.minReplicas }}
  maxReplicas: {{ .Values.autoscaling.maxReplicas }}
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }}
{{- end }}
EOF
----

7. Validar el Chart:
+
[source,bash]
----
helm lint advanced-chart
----

8. Ver manifiestos con todas las opciones habilitadas:
+
[source,bash]
----
helm template myapp advanced-chart

# Debe mostrar: Deployment, Service, HPA
----

9. Ver manifiestos con algunas opciones deshabilitadas:
+
[source,bash]
----
helm template myapp advanced-chart \
  --set service.enabled=false \
  --set autoscaling.enabled=false

# Debe mostrar solo: Deployment (sin Service ni HPA)
----

10. Instalar y verificar:
+
[source,bash]
----
# Instalar con HPA habilitado
helm install adv-app advanced-chart \
  --set autoscaling.enabled=true

# Ver recursos
kubectl get deployment,service,hpa -l app=advanced

# Ver valores de env
kubectl describe deployment adv-app-advanced | grep -A 5 "ENV:"

# Desinstalar
helm uninstall adv-app
rm -rf advanced-chart
----

**Preguntas de reflexión:**

1. ¿Cuál es la diferencia entre `{{ if }}...{{ end }}` y `{{- if }}...{{- end }}`?
2. ¿Cómo hace Helm para iterar sobre diccionarios con `range`?
3. ¿Por qué es útil poder habilitar/deshabilitar componentes con condicionales?

---

=== Ejercicio 10.8: Chart Dependencies (Subcharts)

**Objetivo de aprendizaje:**
Usar subcharts dentro de un Chart principal para reutilizar componentes comunes. Comprender cómo Helm maneja dependencias.

**Descripción:**
Crearás un Chart que depende de PostgreSQL desde Bitnami.

**Tareas:**

1. Crear directorio del Chart principal:
+
[source,bash]
----
mkdir -p multi-tier-app/templates
mkdir -p multi-tier-app/charts
----

2. Crear Chart.yaml con dependencias:
+
[source,bash]
----
cat > multi-tier-app/Chart.yaml << 'EOF'
apiVersion: v2
name: multi-tier-app
version: 1.0.0
appVersion: "1.0.0"

# Dependencias (subcharts)
dependencies:
  - name: postgresql
    version: "11.x.x"
    repository: "https://charts.bitnami.com/bitnami"
    condition: postgresql.enabled
    alias: db
EOF
----

3. Crear values.yaml con configuración de subcharts:
+
[source,bash]
----
cat > multi-tier-app/values.yaml << 'EOF'
# Configuración de la aplicación principal
app:
  image: nginx
  tag: "1.25.0"
  replicas: 2

# Configuración del subchart postgresql
postgresql:
  enabled: true
  auth:
    username: appuser
    password: changeme123
    database: myappdb
  primary:
    persistence:
      enabled: false
      # En producción: enabled: true, size: 10Gi
EOF
----

4. Actualizar dependencias:
+
[source,bash]
----
# Descargar subchart de postgresql
helm dependency update multi-tier-app

# Verificar que se descargó
ls -la multi-tier-app/charts/

# Salida esperada:
# postgresql-11.x.x/
# Chart.lock
----

5. Crear plantilla de aplicación que usa el subchart:
+
[source,bash]
----
cat > multi-tier-app/templates/deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-app
spec:
  replicas: {{ .Values.app.replicas }}
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: "{{ .Values.app.image }}:{{ .Values.app.tag }}"
        ports:
        - containerPort: 8080
        {{- if .Values.postgresql.enabled }}
        env:
        - name: DATABASE_HOST
          value: "{{ .Release.Name }}-db"
        - name: DATABASE_PORT
          value: "5432"
        - name: DATABASE_NAME
          value: "{{ .Values.postgresql.auth.database }}"
        - name: DATABASE_USER
          value: "{{ .Values.postgresql.auth.username }}"
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: {{ .Release.Name }}-db
              key: password
        {{- end }}
EOF
----

6. Crear ConfigMap con variables de entorno:
+
[source,bash]
----
cat > multi-tier-app/templates/configmap.yaml << 'EOF'
{{- if .Values.postgresql.enabled }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-app-config
data:
  database_host: "{{ .Release.Name }}-db"
  database_name: "{{ .Values.postgresql.auth.database }}"
  app_version: "{{ .Chart.AppVersion }}"
{{- end }}
EOF
----

7. Validar el Chart:
+
[source,bash]
----
helm lint multi-tier-app
----

8. Ver manifiestos (incluye postgres):
+
[source,bash]
----
helm template mystack multi-tier-app | head -100

# Debe mostrar manifiestos de:
# - PostgreSQL (subchart)
# - Deployment de aplicación (Chart principal)
# - ConfigMap
----

9. Ver manifiestos sin PostgreSQL:
+
[source,bash]
----
helm template mystack multi-tier-app \
  --set postgresql.enabled=false

# Solo debe mostrar Deployment y ConfigMap
----

10. Instalar el Chart:
+
[source,bash]
----
# Instalar con todas las dependencias
helm install mystack multi-tier-app

# Ver status
helm status mystack

# Ver recursos creados (postgres + app)
kubectl get pods -l app.kubernetes.io/instance=mystack

# Desinstalar
helm uninstall mystack
rm -rf multi-tier-app
----

**Preguntas de reflexión:**

1. ¿Qué contiene el archivo Chart.lock?
2. ¿Cómo especificas en values.yaml la configuración del subchart postgresql?
3. ¿Por qué es útil tener una condición `postgresql.enabled`?

---

=== Ejercicio 10.9: Empaquetar y Distribuir Charts

**Objetivo de aprendizaje:**
Empaquetar un Chart en formato .tgz y prepararlo para distribuir en un repositorio.

**Descripción:**
Empaquetarás un Chart y crearás un índice para un repositorio.

**Tareas:**

1. Crear un Chart simple:
+
[source,bash]
----
helm create simple-web-app

# Estructura creada automáticamente
ls -la simple-web-app/
----

2. Validar el Chart:
+
[source,bash]
----
helm lint simple-web-app

# Salida esperada: [OK]
----

3. Empaquetar el Chart:
+
[source,bash]
----
# Crear archivo .tgz
helm package simple-web-app

# Salida esperada:
# Successfully packaged chart and saved it to: /path/simple-web-app-0.1.0.tgz

# Verificar archivo
ls -lh simple-web-app-*.tgz
----

4. Crear repositorio local:
+
[source,bash]
----
# Crear directorio para repositorio
mkdir -p my-helm-repo
cp simple-web-app-0.1.0.tgz my-helm-repo/

# Crear índice de repositorio
helm repo index my-helm-repo/

# Verificar archivos creados
ls -la my-helm-repo/

# Debe contener:
# - simple-web-app-0.1.0.tgz
# - index.yaml
----

5. Ver contenido del index.yaml:
+
[source,bash]
----
cat my-helm-repo/index.yaml

# Contiene metadatos del Chart y ubicación
----

6. Agregar repositorio local:
+
[source,bash]
----
# Agregar repositorio local (asume servidor HTTP o path local)
# Para HTTP necesitarías: helm repo add myrepo http://myserver/my-helm-repo

# Para path local (solo para demostración):
helm repo add mylocal-repo file://$(pwd)/my-helm-repo

# Actualizar
helm repo update

# Buscar Chart
helm search repo mylocal-repo
----

7. Instalar desde repositorio personalizado:
+
[source,bash]
----
# Instalar desde repositorio local
# helm install myapp mylocal-repo/simple-web-app

# O instalar desde .tgz directamente
helm install myapp ./simple-web-app-0.1.0.tgz

# Ver detalles
helm status myapp

# Desinstalar
helm uninstall myapp
----

8. Crear versión mejorada del Chart:
+
[source,bash]
----
# Modificar simple-web-app/Chart.yaml
# Cambiar version: 0.1.0 a version: 0.2.0
# Cambiar appVersion: "1.16.0" a appVersion: "1.17.0"

sed -i 's/version: 0.1.0/version: 0.2.0/' simple-web-app/Chart.yaml
sed -i 's/appVersion: "1.16.0"/appVersion: "1.17.0"/' simple-web-app/Chart.yaml

# Empaquetar nueva versión
helm package simple-web-app

# Actualizar índice (agrega la nueva versión)
helm repo index my-helm-repo/

# Ver ambas versiones en index.yaml
cat my-helm-repo/index.yaml | grep version
----

9. Limpiar:
+
[source,bash]
----
helm repo remove mylocal-repo
rm -rf simple-web-app my-helm-repo simple-web-app-*.tgz
----

**Preguntas de reflexión:**

1. ¿Qué información contiene el archivo index.yaml?
2. ¿Cómo se distribuiría un repositorio privado de Helm?
3. ¿Por qué es importante mantener múltiples versiones de Charts?

---

=== Ejercicio 10.10: Helm en CI/CD (Despliegues Automatizados)

**Objetivo de aprendizaje:**
Integrar Helm en pipelines de CI/CD para automatizar despliegues. Entender cómo Helm facilita GitOps.

**Descripción:**
Simularás un script de despliegue que usa Helm para actualizar aplicaciones en múltiples ambientes.

**Tareas:**

1. Crear estructura de proyecto:
+
[source,bash]
----
mkdir -p helm-cicd/{charts,values}

# Crear valores por ambiente
cat > helm-cicd/values/dev.yaml << 'EOF'
replicaCount: 1
image:
  tag: "latest"
service:
  type: ClusterIP
EOF

cat > helm-cicd/values/staging.yaml << 'EOF'
replicaCount: 2
image:
  tag: "v1.0.0"
service:
  type: ClusterIP
EOF

cat > helm-cicd/values/prod.yaml << 'EOF'
replicaCount: 3
image:
  tag: "v1.0.0"
service:
  type: LoadBalancer
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
EOF
----

2. Crear Chart simple:
+
[source,bash]
----
helm create helm-cicd/charts/myapp

# Estructura:
# charts/myapp/
# ├── Chart.yaml
# ├── values.yaml
# ├── templates/...
----

3. Crear script de despliegue (simulate CI/CD):
+
[source,bash]
----
cat > helm-cicd/deploy.sh << 'SCRIPT'
#!/bin/bash

set -e

APP_NAME="myapp"
CHART_PATH="./charts/myapp"

# Validar Chart
echo "Validando Chart..."
helm lint "$CHART_PATH"

# Función para desplegar a ambiente
deploy_to_environment() {
  local ENV=$1
  local NAMESPACE=$2

  echo "Desplegando a $ENV..."

  # Crear namespace si no existe
  kubectl create namespace "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f -

  # Usar helm upgrade --install (idempotente)
  helm upgrade --install "$APP_NAME-$ENV" "$CHART_PATH" \
    --namespace "$NAMESPACE" \
    --values "values/$ENV.yaml" \
    --wait \
    --timeout 5m

  echo "$ENV desplegado exitosamente"
  echo ""
}

# Desplegar a dev, staging, prod
deploy_to_environment "dev" "dev"
deploy_to_environment "staging" "staging"
deploy_to_environment "prod" "production"

echo "Todos los ambientes actualizados exitosamente"

# Ver releases
echo ""
echo "Releases instaladas:"
helm list -A
SCRIPT

chmod +x helm-cicd/deploy.sh
----

4. Crear script de despliegue selectivo:
+
[source,bash]
----
cat > helm-cicd/deploy-single.sh << 'SCRIPT'
#!/bin/bash

# Uso: ./deploy-single.sh dev
# Despliega solo a un ambiente

if [ $# -eq 0 ]; then
  echo "Uso: $0 <dev|staging|prod>"
  exit 1
fi

ENV=$1
NAMESPACE=$([ "$ENV" = "prod" ] && echo "production" || echo "$ENV")
CHART_PATH="./charts/myapp"
APP_NAME="myapp"

echo "Desplegando $ENV a namespace $NAMESPACE..."

# Validar values file
if [ ! -f "values/$ENV.yaml" ]; then
  echo "Error: values/$ENV.yaml no existe"
  exit 1
fi

# Validación
echo "Validando Chart..."
helm lint "$CHART_PATH"

# Ver cambios que se harían
echo ""
echo "Cambios que se aplicarán:"
helm upgrade --install "$APP_NAME-$ENV" "$CHART_PATH" \
  --namespace "$NAMESPACE" \
  --values "values/$ENV.yaml" \
  --dry-run

# Ejecutar deployment
echo ""
read -p "¿Continuar con despliegue? (s/n) " -n 1 -r
echo
if [[ $REPLY =~ ^[Ss]$ ]]; then
  helm upgrade --install "$APP_NAME-$ENV" "$CHART_PATH" \
    --namespace "$NAMESPACE" \
    --values "values/$ENV.yaml" \
    --atomic
  echo "Despliegue completado"
else
  echo "Despliegue cancelado"
  exit 1
fi
SCRIPT

chmod +x helm-cicd/deploy-single.sh
----

5. Ver estructura completa:
+
[source,bash]
----
tree helm-cicd/ 2>/dev/null || find helm-cicd/ -type f
----

6. Ejecutar despliegue a un ambiente (simulación):
+
[source,bash]
----
cd helm-cicd/

# Primero validar Chart
helm lint charts/myapp

# Ver qué haría el despliegue (dry-run)
helm upgrade --install myapp-dev charts/myapp \
  --namespace dev \
  --create-namespace \
  --values values/dev.yaml \
  --dry-run --debug | head -50
----

7. Crear script de rollback:
+
[source,bash]
----
cat > helm-cicd/rollback.sh << 'SCRIPT'
#!/bin/bash

# rollback.sh <app-name> [revision]

if [ $# -eq 0 ]; then
  echo "Uso: $0 <app-name> [revision]"
  exit 1
fi

APP_NAME=$1
REVISION=${2:-0}  # 0 = previous

echo "Historial de $APP_NAME:"
helm history "$APP_NAME" -a

if [ "$REVISION" -eq 0 ]; then
  echo ""
  echo "Haciendo rollback a revisión anterior..."
  helm rollback "$APP_NAME"
else
  echo ""
  echo "Haciendo rollback a revisión $REVISION..."
  helm rollback "$APP_NAME" "$REVISION"
fi

echo "Rollback completado"
SCRIPT

chmod +x helm-cicd/rollback.sh
----

8. Ver los valores que se usarían:
+
[source,bash]
----
# Ver valores de dev
cat helm-cicd/values/dev.yaml

# Ver valores de prod
cat helm-cicd/values/prod.yaml
----

9. Simular despliegue a dev:
+
[source,bash]
----
# Ver manifiestos (sin aplicar)
helm template myapp-dev helm-cicd/charts/myapp \
  --values helm-cicd/values/dev.yaml | head -50
----

10. Limpiar:
+
[source,bash]
----
cd ..
rm -rf helm-cicd

# Si habías instalado releases, desinstalalas:
# helm uninstall myapp-dev -n dev
# helm uninstall myapp-staging -n staging
# helm uninstall myapp-prod -n production
----

**Preguntas de reflexión:**

1. ¿Por qué `helm upgrade --install` es mejor que `helm install` en CI/CD?
2. ¿Cómo asegurarías que los valores específicos de producción no se cambian accidentalmente?
3. ¿Cómo integrarías este script en un pipeline de GitHub Actions o GitLab CI?

---

== Resumen de Conceptos

Estos ejercicios del Módulo 10 cubren:

* **Ejercicio 10.1**: Gestión de repositorios de Helm
* **Ejercicio 10.2**: Instalación de Charts desde repositorio
* **Ejercicio 10.3**: Personalización con valores
* **Ejercicio 10.4**: Upgrades y cambios de configuración
* **Ejercicio 10.5**: Historial y rollback de releases
* **Ejercicio 10.6**: Crear Charts desde cero
* **Ejercicio 10.7**: Templates con lógica (condicionales/loops)
* **Ejercicio 10.8**: Usar subcharts y dependencias
* **Ejercicio 10.9**: Empaquetar y distribuir Charts
* **Ejercicio 10.10**: Automatización con CI/CD

== Módulo 11: CI/CD con Kubernetes

=== Ejercicio 11.1: Blue-Green Deployment

**Objetivo de aprendizaje:**
Implementar una estrategia Blue-Green para desplegar nueva versión sin downtime. Comprender cómo cambiar tráfico instantáneamente.

**Descripción:**
Desplegarás dos versiones idénticas de una aplicación (Blue y Green) y cambiarás el tráfico entre ellas.

**Tareas:**

1. Crear el deployment Blue (versión actual en producción):
+
[source,bash]
----
cat > blue-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-blue
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
      version: blue
  template:
    metadata:
      labels:
        app: myapp
        version: blue
    spec:
      containers:
      - name: app
        image: nginx:1.24.0
        ports:
        - containerPort: 80
        env:
        - name: VERSION
          value: "1.0.0 (Blue)"
EOF

kubectl apply -f blue-deployment.yaml
----

2. Crear el deployment Green (versión candidata):
+
[source,bash]
----
cat > green-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-green
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
      version: green
  template:
    metadata:
      labels:
        app: myapp
        version: green
    spec:
      containers:
      - name: app
        image: nginx:1.25.0
        ports:
        - containerPort: 80
        env:
        - name: VERSION
          value: "2.0.0 (Green)"
EOF

kubectl apply -f green-deployment.yaml
----

3. Crear el Service apuntando a Blue:
+
[source,bash]
----
cat > service.yaml << 'EOF'
apiVersion: v1
kind: Service
metadata:
  name: myapp
spec:
  selector:
    app: myapp
    version: blue
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer
EOF

kubectl apply -f service.yaml
----

4. Verificar que ambos deployments están corriendo:
+
[source,bash]
----
# Ver Pods de Blue
kubectl get pods -l version=blue

# Ver Pods de Green
kubectl get pods -l version=green

# Ver Service actual
kubectl describe svc myapp
----

5. Verificar tráfico en Blue:
+
[source,bash]
----
# El tráfico actual va a Blue
kubectl get endpoints myapp

# Acceder al servicio (si es posible)
# curl <service-ip>
----

6. Conmutar tráfico a Green (cambiar selector del Service):
+
[source,bash]
----
# Patch del Service para apuntar a Green
kubectl patch service myapp -p '{"spec":{"selector":{"version":"green"}}}'

# Verificar que endpoints cambió
kubectl get endpoints myapp

# Ahora todo el tráfico va a Green
----

7. Verificar que Green está recibiendo tráfico:
+
[source,bash]
----
kubectl get pods -l version=green
----

8. Rollback instantáneo si hay problemas:
+
[source,bash]
----
# Cambiar de vuelta a Blue
kubectl patch service myapp -p '{"spec":{"selector":{"version":"blue"}}}'

# Verificar endpoints
kubectl get endpoints myapp
----

9. Una vez que Green es estable, eliminar Blue:
+
[source,bash]
----
kubectl delete deployment app-blue

# Cambiar definitivamente a Green
kubectl patch service myapp -p '{"spec":{"selector":{"version":"green"}}}'
----

10. Limpiar:
+
[source,bash]
----
kubectl delete -f green-deployment.yaml
kubectl delete -f service.yaml
rm -f blue-deployment.yaml green-deployment.yaml service.yaml
----

**Preguntas de reflexión:**

1. ¿Cuál es el ventaja principal de Blue-Green vs Rolling Update?
2. ¿Qué sucede con las conexiones existentes cuando cambias el selector del servicio?
3. ¿Cómo sincronizarías datos entre Blue y Green si la aplicación tiene estado?

---

=== Ejercicio 11.2: Canary Deployment usando Réplicas

**Objetivo de aprendizaje:**
Implementar un Canary Deployment controlando el número de replicas. Gradualmente aumentar tráfico a nueva versión.

**Descripción:**
Desplegarás versión estable con muchas replicas y versión canaria con pocas replicas, luego cambiarás el ratio.

**Tareas:**

1. Crear Deployment con versión estable (v1):
+
[source,bash]
----
cat > canary-v1.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-v1
spec:
  replicas: 9        # 90% del tráfico
  selector:
    matchLabels:
      app: myapp
      version: v1
  template:
    metadata:
      labels:
        app: myapp
        version: v1
    spec:
      containers:
      - name: app
        image: nginx:1.24.0
        ports:
        - containerPort: 80
        env:
        - name: VERSION
          value: "v1 - stable"
EOF

kubectl apply -f canary-v1.yaml
----

2. Crear Deployment con versión canaria (v2):
+
[source,bash]
----
cat > canary-v2.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-v2
spec:
  replicas: 1        # 10% del tráfico (1 pod de 10 total)
  selector:
    matchLabels:
      app: myapp
      version: v2
  template:
    metadata:
      labels:
        app: myapp
        version: v2
    spec:
      containers:
      - name: app
        image: nginx:1.25.0
        ports:
        - containerPort: 80
        env:
        - name: VERSION
          value: "v2 - canary"
EOF

kubectl apply -f canary-v2.yaml
----

3. Crear Service que balancea entre ambas versiones:
+
[source,bash]
----
cat > canary-service.yaml << 'EOF'
apiVersion: v1
kind: Service
metadata:
  name: myapp-canary
spec:
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer
EOF

kubectl apply -f canary-service.yaml
----

4. Verificar el ratio de tráfico:
+
[source,bash]
----
# Ver Pods totales
kubectl get pods -l app=myapp

# Contar por versión
kubectl get pods -l version=v1 | wc -l   # Debe ser 10 (header + 9 pods)
kubectl get pods -l version=v2 | wc -l   # Debe ser 2 (header + 1 pod)

# Ver endpoints (proporcional al número de replicas)
kubectl get endpoints myapp-canary
----

5. Monitorear canario (simular métricas):
+
[source,bash]
----
# Crear logs para simular monitoreo
kubectl logs -l version=v2 --tail=5

# En la práctica:
# - Observar error rate de v2
# - Comparar latencia
# - Validar que v2 funciona correctamente
----

6. Aumentar tráfico a v2 (aumentar replicas):
+
[source,bash]
----
# Escalar v1 a 5 replicas (50%)
kubectl scale deployment myapp-v1 --replicas=5

# Escalar v2 a 5 replicas (50%)
kubectl scale deployment myapp-v2 --replicas=5

# Ver cambio
kubectl get pods -l app=myapp
----

7. Continuar aumentando tráfico a v2:
+
[source,bash]
----
# Escalar v1 a 1 replica (10%)
kubectl scale deployment myapp-v1 --replicas=1

# Escalar v2 a 9 replicas (90%)
kubectl scale deployment myapp-v2 --replicas=9

# Ver proporción actual
kubectl get pods -l app=myapp
----

8. Completar rollout a v2:
+
[source,bash]
----
# Escalar v1 a 0 (eliminar)
kubectl scale deployment myapp-v1 --replicas=0

# Escalar v2 a 10 (100%)
kubectl scale deployment myapp-v2 --replicas=10

# Verificar
kubectl get pods -l version=v2
----

9. Limpiar:
+
[source,bash]
----
kubectl delete -f canary-v1.yaml canary-v2.yaml canary-service.yaml
rm -f canary-v1.yaml canary-v2.yaml canary-service.yaml
----

**Preguntas de reflexión:**

1. ¿Cuál es la desventaja de usar número de replicas para control de tráfico?
2. ¿Cómo automatizarías estos cambios de replicas basado en métricas?
3. ¿Qué problemas surgen si v2 tiene un bug y afecta al 50% de usuarios?

---

=== Ejercicio 11.3: Rolling Update Avanzado

**Objetivo de aprendizaje:**
Comprender cómo Kubernetes realiza rolling updates nativamente. Controlar la estrategia de actualización.

**Descripción:**
Crearás un Deployment con rolling update y observarás cómo actualiza Pods gradualmente.

**Tareas:**

1. Crear Deployment con rolling update personalizado:
+
[source,bash]
----
cat > rolling-update.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-rolling
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1          # Máximo 1 pod adicional
      maxUnavailable: 1    # Máximo 1 pod unavailable
  minReadySeconds: 5       # Esperar 5s antes de considerar ready
  selector:
    matchLabels:
      app: rolling-app
  template:
    metadata:
      labels:
        app: rolling-app
      annotations:
        timestamp: "initial"
    spec:
      containers:
      - name: app
        image: nginx:1.24.0
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 2
          periodSeconds: 2
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
EOF

kubectl apply -f rolling-update.yaml
----

2. Verificar el Deployment inicial:
+
[source,bash]
----
# Ver Pods
kubectl get pods -l app=rolling-app

# Ver replicaset
kubectl get rs -l app=rolling-app

# Ver status
kubectl rollout status deployment/app-rolling
----

3. Registrar información de las replicas iniciales:
+
[source,bash]
----
# Ver imágenes en uso
kubectl describe pods -l app=rolling-app | grep Image
----

4. Iniciar rolling update a nueva versión:
+
[source,bash]
----
# Actualizar imagen (trigger rolling update)
kubectl set image deployment/app-rolling app=nginx:1.25.0 --record

# Ver estado en tiempo real
kubectl rollout status deployment/app-rolling --watch
----

5. Monitorear el update en progreso:
+
[source,bash]
----
# En otra ventana, ver cambios de Pods
kubectl get pods -l app=rolling-app -w

# Ver ReplicaSets durante update
kubectl get rs -l app=rolling-app
----

6. Ver historial de updates:
+
[source,bash]
----
kubectl rollout history deployment/app-rolling

# Ver detalles de una revisión específica
kubectl rollout history deployment/app-rolling --revision=1
----

7. Pausar el rolling update (si hay problemas):
+
[source,bash]
----
# Parar en mitad del update
# kubectl rollout pause deployment/app-rolling

# Después de investigar, resumir:
# kubectl rollout resume deployment/app-rolling
----

8. Hacer rollback a versión anterior:
+
[source,bash]
----
# Rollback a revisión anterior
kubectl rollout undo deployment/app-rolling

# Ver status
kubectl rollout status deployment/app-rolling

# Verificar imagen actualizada
kubectl describe pods -l app=rolling-app | grep Image
----

9. Rollback a revisión específica:
+
[source,bash]
----
# Ver historial
kubectl rollout history deployment/app-rolling

# Rollback a revisión específica
kubectl rollout undo deployment/app-rolling --to-revision=1

# Verificar
kubectl get rs -l app=rolling-app
----

10. Limpiar:
+
[source,bash]
----
kubectl delete -f rolling-update.yaml
rm -f rolling-update.yaml
----

**Preguntas de reflexión:**

1. ¿Qué significa `maxSurge: 1` en tu configuración?
2. ¿Cómo controla Kubernetes que los Pods nuevos estén listos?
3. ¿Cuál es la diferencia entre pausar y deshacer un rolling update?

---

=== Ejercicio 11.4: GitOps con ArgoCD (Simulación)

**Objetivo de aprendizaje:**
Entender el flujo de trabajo GitOps sin instalar ArgoCD. Simular cómo un operador sincroniza Git con cluster.

**Descripción:**
Crearás un script que simula el comportamiento de GitOps: obtener manifiestos de Git y aplicarlos.

**Tareas:**

1. Crear estructura de repositorio simulado:
+
[source,bash]
----
mkdir -p gitops-repo/k8s

cat > gitops-repo/k8s/deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gitops-app
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: gitops-app
  template:
    metadata:
      labels:
        app: gitops-app
    spec:
      containers:
      - name: app
        image: nginx:1.24.0
        ports:
        - containerPort: 80
EOF

cat > gitops-repo/k8s/service.yaml << 'EOF'
apiVersion: v1
kind: Service
metadata:
  name: gitops-app
spec:
  selector:
    app: gitops-app
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer
EOF
----

2. Simular un "Application" de ArgoCD:
+
[source,bash]
----
cat > gitops-application.yaml << 'EOF'
# Simulación de Application de ArgoCD
# Campos key:
# - source.path: dónde están los manifiestos en Git
# - destination.namespace: dónde desplegar
# - syncPolicy.automated: sincronización automática

metadata:
  name: gitops-app
spec:
  project: default
  source:
    repoURL: file://$(pwd)/gitops-repo
    path: k8s/
    targetRevision: main
  destination:
    server: https://kubernetes.default.svc
    namespace: default
  syncPolicy:
    automated:
      prune: true      # Eliminar recursos no en Git
      selfHeal: true   # Resincronizar si diverge
    syncOptions:
    - CreateNamespace=true
EOF
----

3. Simular sincronización inicial (obtener manifiestos de Git):
+
[source,bash]
----
echo "=== Simulando: ArgoCD obtiene manifiestos de Git ==="

# Aplicar todos los manifiestos del "repositorio"
kubectl apply -f gitops-repo/k8s/

echo "=== Aplicación desplegada ==="
kubectl get deployment,svc -l app=gitops-app
----

4. Simular cambio en Git (actualizar imagen):
+
[source,bash]
----
# Cambiar imagen en "repositorio"
sed -i 's/nginx:1.24.0/nginx:1.25.0/' gitops-repo/k8s/deployment.yaml

# Mostrar el cambio
echo "=== Cambio en Git detectado ==="
cat gitops-repo/k8s/deployment.yaml | grep image:
----

5. Simular sincronización automática (ArgoCD detecta cambio):
+
[source,bash]
----
echo "=== Simulando: ArgoCD detecta cambio en Git ==="

# Aplicar cambios nuevamente (simula sync automático)
kubectl apply -f gitops-repo/k8s/

echo "=== Cluster sincronizado con Git ==="
kubectl get deployment gitops-app -o jsonpath='{.spec.template.spec.containers[0].image}'
----

6. Simular drift detection (alguien cambia cluster manualmente):
+
[source,bash]
----
# Simular cambio manual en cluster (desviación de Git)
echo "=== Simulando cambio manual en cluster ==="
kubectl set image deployment/gitops-app app=nginx:1.22.0

# Verificar divergencia
echo "En Cluster:"
kubectl get deployment gitops-app -o jsonpath='{.spec.template.spec.containers[0].image}'
echo ""
echo "En Git:"
grep image: gitops-repo/k8s/deployment.yaml | grep -oP 'nginx:\d+\.\d+\.\d+'
----

7. Simular auto-healing (GitOps restaura estado):
+
[source,bash]
----
echo "=== Simulando: Auto-healing restaura estado a Git ==="

# Aplicar nuevamente (simula reconciliación automática)
kubectl apply -f gitops-repo/k8s/

echo "Cluster restaurado a estado de Git:"
kubectl get deployment gitops-app -o jsonpath='{.spec.template.spec.containers[0].image}'
----

8. Simular rollback via Git:
+
[source,bash]
----
# En Git, revertir a versión anterior
git -C gitops-repo checkout HEAD~1 k8s/deployment.yaml 2>/dev/null || \
  sed -i 's/nginx:1.25.0/nginx:1.24.0/' gitops-repo/k8s/deployment.yaml

echo "=== Cambio reverted en Git ==="

# Sincronizar cluster
kubectl apply -f gitops-repo/k8s/

echo "Cluster rollback a versión anterior:"
kubectl get deployment gitops-app -o jsonpath='{.spec.template.spec.containers[0].image}'
----

9. Verificar prune (eliminar recursos no en Git):
+
[source,bash]
----
# Crear recurso que NO está en Git
cat > orphan.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: orphan-config
  labels:
    app: gitops-app
data:
  key: value
EOF

kubectl apply -f orphan.yaml

echo "ConfigMap creado (no en Git):"
kubectl get configmap orphan-config

# Simular prune: eliminar recursos que no están en Git
kubectl delete configmap orphan-config

echo "ConfigMap eliminado (prune simulado)"
----

10. Limpiar:
+
[source,bash]
----
kubectl delete -f gitops-repo/k8s/
rm -rf gitops-repo gitops-application.yaml orphan.yaml
----

**Preguntas de reflexión:**

1. ¿Cuál es la ventaja de tener Git como "fuente de verdad"?
2. ¿Cómo ArgoCD detecta cambios? ¿Polling o webhooks?
3. ¿Qué pasaría si alguien hace kubectl apply mientras GitOps está sincronizando?

---

=== Ejercicio 11.5: Estrategia de Image Pull Secrets

**Objetivo de aprendizaje:**
Crear y usar secrets para registros privados. Comprender cómo Kubernetes autentica en registros de imágenes.

**Descripción:**
Crearás un secret de Docker y lo usarás en un Deployment para obtener imágenes de un registro privado.

**Tareas:**

1. Crear un Docker Registry secret:
+
[source,bash]
----
# Crear secret para autenticación privada
kubectl create secret docker-registry private-registry \
  --docker-server=registry.example.com \
  --docker-username=myuser \
  --docker-password=mypassword \
  --docker-email=user@example.com

# Verificar que se creó
kubectl get secret private-registry
----

2. Ver el contenido del secret (codificado):
+
[source,bash]
----
# El secret contiene .dockerconfigjson en base64
kubectl get secret private-registry -o jsonpath='{.data.\.dockerconfigjson}' | base64 -d | jq .
----

3. Crear un Deployment que usa la imagen privada:
+
[source,bash]
----
cat > private-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: private-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: private-app
  template:
    metadata:
      labels:
        app: private-app
    spec:
      # Referencia al secret de autenticación
      imagePullSecrets:
      - name: private-registry

      containers:
      - name: app
        # Imagen en registro privado
        image: registry.example.com/myapp:v1.0.0
        ports:
        - containerPort: 8080
        imagePullPolicy: IfNotPresent
EOF

kubectl apply -f private-deployment.yaml
----

4. Verificar el estado (probablemente fallo de pull):
+
[source,bash]
----
# Los Pods fallarán porque registry.example.com no existe
kubectl get pods -l app=private-app

# Ver eventos de error
kubectl describe pod -l app=private-app | grep -A 5 "Events:"

# El error debe mencionar: "ImagePullBackOff" o similar
----

5. Crear secret para registro privado local (simulación):
+
[source,bash]
----
# Crear secreto para usar imágenes públicas como si fueran privadas
# (Para demostración, usamos Docker Hub pero como si fuera privado)

kubectl create secret docker-registry docker-hub-secret \
  --docker-server=docker.io \
  --docker-username=myuser \
  --docker-password=mytoken \
  --docker-email=user@example.com \
  --dry-run=client \
  -o yaml > docker-secret.yaml

# Ver el secret
cat docker-secret.yaml
----

6. Usar secret en ServiceAccount para todos los Pods:
+
[source,bash]
----
# Actualizar el ServiceAccount default para incluir secret
kubectl patch serviceaccount default \
  -p '{"imagePullSecrets": [{"name": "private-registry"}]}'

# Verificar
kubectl get serviceaccount default -o jsonpath='{.imagePullSecrets}'
----

7. Crear Deployment que hereda el secret del ServiceAccount:
+
[source,bash]
----
cat > app-with-sa.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-with-sa
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app-with-sa
  template:
    metadata:
      labels:
        app: app-with-sa
    spec:
      serviceAccountName: default  # Usa secret del SA
      containers:
      - name: app
        image: nginx:1.24.0
        ports:
        - containerPort: 80
EOF

kubectl apply -f app-with-sa.yaml

# Este Pod usará el secret del ServiceAccount
kubectl get pods -l app=app-with-sa
----

8. Ver imagen pull policies:
+
[source,bash]
----
cat > pull-policies.yaml << 'EOF'
# Always: siempre pull (para :latest)
apiVersion: v1
kind: Pod
metadata:
  name: always-pull
spec:
  imagePullSecrets:
  - name: private-registry
  containers:
  - name: app
    image: registry.example.com/myapp:latest
    imagePullPolicy: Always
---
# IfNotPresent: pull solo si no existe localmente
apiVersion: v1
kind: Pod
metadata:
  name: if-not-present
spec:
  imagePullSecrets:
  - name: private-registry
  containers:
  - name: app
    image: registry.example.com/myapp:v1.0.0
    imagePullPolicy: IfNotPresent
---
# Never: no pull, usar imagen local o fallar
apiVersion: v1
kind: Pod
metadata:
  name: never-pull
spec:
  containers:
  - name: app
    image: myapp:v1.0.0
    imagePullPolicy: Never
EOF

# No aplicar (fallaría), solo para referencia
cat pull-policies.yaml
----

9. Listar todos los secrets:
+
[source,bash]
----
kubectl get secrets

# Filtrar solo docker-registry
kubectl get secrets -o jsonpath='{range .items[?(@.type=="kubernetes.io/dockercfg")]}{.metadata.name}{"\n"}{end}'
----

10. Limpiar:
+
[source,bash]
----
kubectl delete secret private-registry docker-hub-secret
kubectl delete -f private-deployment.yaml app-with-sa.yaml
kubectl patch serviceaccount default --type='json' \
  -p='[{"op": "remove", "path": "/imagePullSecrets"}]'

rm -f private-deployment.yaml app-with-sa.yaml docker-secret.yaml pull-policies.yaml
----

**Preguntas de reflexión:**

1. ¿Cuál es la diferencia entre `imagePullSecrets` en Pod vs en ServiceAccount?
2. ¿Por qué es peligroso usar `imagePullPolicy: Always` con imágenes privadas?
3. ¿Cómo rotatarías credenciales sin romper Pods que ya están corriendo?

---

=== Ejercicio 11.6: Image Scanning con Simulación

**Objetivo de aprendizaje:**
Entender cómo funciona el image scanning. Simular detección de vulnerabilidades.

**Descripción:**
Crearás un script que simula el scanning de vulnerabilidades en imágenes.

**Tareas:**

1. Simular descarga de imagen y análisis:
+
[source,bash]
----
cat > image-scan-simulator.sh << 'EOF'
#!/bin/bash

# Simulador de Trivy (image scanning)

IMAGE=${1:-nginx:1.24.0}

echo "=== Trivy Image Scan Simulator ==="
echo "Image: $IMAGE"
echo ""

# Simular escaneo
case "$IMAGE" in
  *1.22* | *1.23*)
    echo "High and Critical vulnerabilities found!"
    echo ""
    echo "Vulnerabilities:"
    echo "  CVE-2023-1234 CRITICAL - in openssl"
    echo "  CVE-2023-5678 HIGH - in curl"
    echo ""
    exit 1
    ;;
  *1.24*)
    echo "Medium vulnerabilities found"
    echo ""
    echo "Vulnerabilities:"
    echo "  CVE-2024-1111 MEDIUM - in apt"
    echo ""
    exit 0
    ;;
  *1.25*)
    echo "No vulnerabilities found"
    echo ""
    echo "Status: PASSED"
    exit 0
    ;;
  *)
    echo "Unknown image"
    exit 1
    ;;
esac
EOF

chmod +x image-scan-simulator.sh
----

2. Simular scanning de diferentes versiones:
+
[source,bash]
----
# Versión antigua (vulnerabilidades críticas)
echo "=== Scanning nginx:1.22.0 ==="
./image-scan-simulator.sh nginx:1.22.0
echo ""

# Versión media (vulnerabilidades medium)
echo "=== Scanning nginx:1.24.0 ==="
./image-scan-simulator.sh nginx:1.24.0
echo ""

# Versión nueva (sin vulnerabilidades)
echo "=== Scanning nginx:1.25.0 ==="
./image-scan-simulator.sh nginx:1.25.0
----

3. Crear política de admission que bloquea imágenes:
+
[source,bash]
----
cat > image-policy.yaml << 'EOF'
# Política de Kyverno para bloquear imágenes vulnerables
# (Simulación - no ejecutar realmente)

apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: block-vulnerable-images
spec:
  validationFailureAction: enforce
  rules:
  - name: check-image-version
    match:
      resources:
        kinds:
        - Pod
    validate:
      message: "Image must be >= 1.24.0 for nginx"
      pattern:
        spec:
          containers:
          - image: "nginx:*"
            # Bloquear versiones antiguas
            # Este es un ejemplo simplificado
EOF

cat image-policy.yaml
----

4. Crear script que genera reporte de scanning:
+
[source,bash]
----
cat > scan-report.sh << 'EOF'
#!/bin/bash

# Generar reporte de scan de varias imágenes

IMAGES=(
  "nginx:1.22.0"
  "nginx:1.24.0"
  "nginx:1.25.0"
)

echo "=== Image Vulnerability Report ==="
echo "Generated: $(date)"
echo ""

PASSED=0
FAILED=0

for image in "${IMAGES[@]}"; do
  echo "Scanning: $image"
  if ./image-scan-simulator.sh "$image" > /dev/null 2>&1; then
    echo "Status: PASSED ✓"
    ((PASSED++))
  else
    echo "Status: FAILED ✗"
    ((FAILED++))
  fi
  echo ""
done

echo "=== Summary ==="
echo "Total images: ${#IMAGES[@]}"
echo "Passed: $PASSED"
echo "Failed: $FAILED"

if [ $FAILED -gt 0 ]; then
  exit 1
fi
EOF

chmod +x scan-report.sh
----

5. Ejecutar el reporte:
+
[source,bash]
----
./scan-report.sh
----

6. Simular integración en CI/CD (bloquear builds):
+
[source,bash]
----
cat > ci-with-scanning.sh << 'EOF'
#!/bin/bash

# Simulación de CI/CD con scanning integrado

IMAGE_TO_BUILD="nginx:1.23.0"

echo "=== CI/CD Pipeline with Image Scanning ==="
echo "Building image: $IMAGE_TO_BUILD"
echo ""

# Paso 1: Build (simulado)
echo "Step 1: Building Docker image..."
echo "docker build -t $IMAGE_TO_BUILD ."
echo ""

# Paso 2: Scan
echo "Step 2: Scanning image for vulnerabilities..."
if ./image-scan-simulator.sh "$IMAGE_TO_BUILD" > /dev/null; then
  echo "✓ Image passed scanning"
else
  echo "✗ Image scanning FAILED"
  echo "Blocking deployment due to vulnerabilities"
  exit 1
fi
echo ""

# Paso 3: Push (no se alcanza si scan falla)
echo "Step 3: Pushing to registry..."
echo "docker push $IMAGE_TO_BUILD"
EOF

chmod +x ci-with-scanning.sh
----

7. Ejecutar CI/CD con scan fallido:
+
[source,bash]
----
./ci-with-scanning.sh
----

8. Mostrar cómo pasar el scan:
+
[source,bash]
----
# Cambiar a versión sin vulnerabilidades
sed -i 's/nginx:1.23.0/nginx:1.25.0/' ci-with-scanning.sh

# Ejecutar nuevamente
./ci-with-scanning.sh
----

9. Crear tabla de políticas de vulnerabilidades:
+
[source,bash]
----
cat > vulnerability-policy.yaml << 'EOF'
# Políticas de remediación de vulnerabilidades

policies:
  critical:
    action: "block"           # Bloquear despliegue
    maxAge: "0 days"         # No permitir imágenes antiguas

  high:
    action: "require-approval" # Requiere aprobación manual
    maxAge: "7 days"         # Revisar cada 7 días

  medium:
    action: "warn"           # Solo advertencia
    maxAge: "30 days"        # Revisar cada 30 días

  low:
    action: "allow"          # Permitido
    maxAge: "90 days"

exception_process:
  - name: "Security team reviews"
  - name: "Risk assessment"
  - name: "Approved by CISO"
  - name: "Documented exception"
EOF

cat vulnerability-policy.yaml
----

10. Limpiar:
+
[source,bash]
----
rm -f image-scan-simulator.sh scan-report.sh ci-with-scanning.sh \
      image-policy.yaml vulnerability-policy.yaml
----

**Preguntas de reflexión:**

1. ¿Cómo actualizarías automáticamente imágenes base cuando hay vulnerabilidades?
2. ¿Qué diferencia hay entre scanning en CI vs scanning de imágenes en ejecución?
3. ¿Cómo balancearías seguridad (bloquear todo) vs velocidad (permitir todo)?

---

=== Ejercicio 11.7: Rolling Update con Health Checks

**Objetivo de aprendizaje:**
Implementar health checks que guían el rolling update. Comprender readiness y liveness probes.

**Descripción:**
Crearás un Deployment con health checks que controlan si un Pod está listo para recibir tráfico.

**Tareas:**

1. Crear Deployment con readiness probe fallando:
+
[source,bash]
----
cat > health-check.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-health
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: app-health
  template:
    metadata:
      labels:
        app: app-health
    spec:
      containers:
      - name: app
        image: nginx:1.24.0
        ports:
        - containerPort: 80

        # Readiness Probe: ¿está listo para recibir tráfico?
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 2
          periodSeconds: 2
          timeoutSeconds: 1
          failureThreshold: 3

        # Liveness Probe: ¿sigue vivo el contenedor?
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 3
EOF

kubectl apply -f health-check.yaml
----

2. Verificar que todos los Pods están ready:
+
[source,bash]
----
# Ver estado de readiness
kubectl get pods -l app=app-health

# READY debe ser 1/1 para todos
----

3. Simular fallo de readiness probe:
+
[source,bash]
----
# Ejecutar comando que hace que nginx falle health check
# (simulación: cambiar endpoint)

# En realidad esto requeriría un pod especial, pero podemos simular:
cat > readiness-failure.yaml << 'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: failing-readiness
spec:
  containers:
  - name: app
    image: nginx:1.24.0
    readinessProbe:
      httpGet:
        path: /nonexistent  # Esta ruta no existe
        port: 80
      initialDelaySeconds: 2
      periodSeconds: 2
      failureThreshold: 1
EOF

kubectl apply -f readiness-failure.yaml

# Ver que no está ready
sleep 5
kubectl get pod failing-readiness
----

4. Ver endpoints durante readiness failure:
+
[source,bash]
----
# Los Pods no-ready no aparecen en endpoints
kubectl get endpoints -l app=app-health 2>/dev/null || \
  kubectl get service app-health 2>/dev/null || \
  echo "No service created"
----

5. Crear Service para ver endpoints:
+
[source,bash]
----
cat > health-service.yaml << 'EOF'
apiVersion: v1
kind: Service
metadata:
  name: app-health
spec:
  selector:
    app: app-health
  ports:
  - port: 80
    targetPort: 80
EOF

kubectl apply -f health-service.yaml
----

6. Ver endpoints (solo Pods ready):
+
[source,bash]
----
kubectl get endpoints app-health
----

7. Actualizar imagen con health check:
+
[source,bash]
----
echo "=== Iniciando rolling update ==="

# El nuevo Pod debe pasar readiness probe antes de ser incluido en endpoints
kubectl set image deployment/app-health app=nginx:1.25.0 --record

# Monitorear en tiempo real
kubectl rollout status deployment/app-health --watch
----

8. Ver Pods nuevos pasando readiness:
+
[source,bash]
----
# Los nuevos Pods pasan el readiness check gradualmente
kubectl get pods -l app=app-health -o wide

# Ver cambio de endpoints
kubectl get endpoints app-health
----

9. Simular un update que falla health check:
+
[source,bash]
----
# Actualizar a imagen que tiene endpoint incorrecto
cat > bad-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bad-health
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: bad-health
  template:
    metadata:
      labels:
        app: bad-health
    spec:
      containers:
      - name: app
        image: nginx:1.24.0
        readinessProbe:
          httpGet:
            path: /admin  # Ruta que no existe por defecto
            port: 80
          initialDelaySeconds: 1
          periodSeconds: 1
          failureThreshold: 1
EOF

kubectl apply -f bad-deployment.yaml

# Ver que Pods nunca llegan a ready
sleep 10
kubectl get pods -l app=bad-health
----

10. Limpiar:
+
[source,bash]
----
kubectl delete deployment app-health bad-health
kubectl delete pod failing-readiness
kubectl delete -f health-service.yaml
rm -f health-check.yaml readiness-failure.yaml bad-deployment.yaml health-service.yaml
----

**Preguntas de reflexión:**

1. ¿Cuál es la diferencia entre readiness y liveness probes?
2. ¿Qué sucede si initialDelaySeconds es muy bajo para tu aplicación?
3. ¿Cómo asegurarías que todos los Pods sean ready antes de remover los antiguos?

---

=== Ejercicio 11.8: Recreate Strategy vs Rolling Update

**Objetivo de aprendizaje:**
Comprender cuándo usar cada estrategia de deployment. Conocer trade-offs.

**Descripción:**
Compararás dos deployment strategies: Recreate (downtime) vs RollingUpdate (sin downtime).

**Tareas:**

1. Crear Deployment con estrategia Recreate:
+
[source,bash]
----
cat > recreate-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-recreate
spec:
  replicas: 3
  strategy:
    type: Recreate    # Eliminar todos, luego crear nuevos
  selector:
    matchLabels:
      app: app-recreate
  template:
    metadata:
      labels:
        app: app-recreate
      annotations:
        timestamp: "start"
    spec:
      containers:
      - name: app
        image: nginx:1.24.0
        ports:
        - containerPort: 80
EOF

kubectl apply -f recreate-deployment.yaml

# Verificar
kubectl get pods -l app=app-recreate
----

2. Crear Deployment con estrategia RollingUpdate:
+
[source,bash]
----
cat > rolling-deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-rolling
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: app-rolling
  template:
    metadata:
      labels:
        app: app-rolling
    spec:
      containers:
      - name: app
        image: nginx:1.24.0
        ports:
        - containerPort: 80
EOF

kubectl apply -f rolling-deployment.yaml

# Verificar
kubectl get pods -l app=app-rolling
----

3. Crear Services para ambos:
+
[source,bash]
----
cat > services.yaml << 'EOF'
apiVersion: v1
kind: Service
metadata:
  name: app-recreate
spec:
  selector:
    app: app-recreate
  ports:
  - port: 80
    targetPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: app-rolling
spec:
  selector:
    app: app-rolling
  ports:
  - port: 80
    targetPort: 80
EOF

kubectl apply -f services.yaml
----

4. Monitorear Recreate strategy durante update:
+
[source,bash]
----
# En una ventana
echo "=== Monitoreo de Recreate Strategy ==="
kubectl get pods -l app=app-recreate -w

# En otra ventana, iniciar update
# (esperar a que la primera ventana muestre el comando)
----

5. Realizar update con Recreate (window 2):
+
[source,bash]
----
echo "=== Iniciando Recreate Update ==="
kubectl set image deployment/app-recreate app=nginx:1.25.0 --record

# Ver estado
kubectl rollout status deployment/app-recreate
----

6. Observar el downtime:
+
[source,bash]
----
# Durante el update, todos los Pods están siendo eliminados
# y luego recreados con nueva imagen
# Habrá momento donde no hay Pods listos (downtime)

kubectl get pods -l app=app-recreate
----

7. Monitorear RollingUpdate strategy (ventana 1):
+
[source,bash]
----
# En una ventana
echo "=== Monitoreo de RollingUpdate Strategy ==="
kubectl get pods -l app=app-rolling -w

# Esperar...
----

8. Realizar update con RollingUpdate (ventana 2):
+
[source,bash]
----
echo "=== Iniciando RollingUpdate Update ==="
kubectl set image deployment/app-rolling app=nginx:1.25.0 --record

# Ver estado
kubectl rollout status deployment/app-rolling
----

9. Comparar disponibilidad durante updates:
+
[source,bash]
----
# Recreate: todos los Pods eliminados simultáneamente = downtime
# Rolling: siempre hay Pods listos = sin downtime

kubectl get deployment app-recreate app-rolling -o wide
----

10. Limpiar:
+
[source,bash]
----
kubectl delete deployment app-recreate app-rolling
kubectl delete -f services.yaml
rm -f recreate-deployment.yaml rolling-deployment.yaml services.yaml
----

**Preguntas de reflexión:**

1. ¿Cuándo usarías Recreate a pesar del downtime?
2. ¿Cómo afectaría cada strategy a una aplicación con estado (database)?
3. ¿Cuál es más rápido: Recreate o RollingUpdate?

---

=== Ejercicio 11.9: Multi-Environment Deployment

**Objetivo de aprendizaje:**
Desplegar la misma aplicación en múltiples ambientes (dev, staging, prod) con configuraciones diferentes.

**Descripción:**
Crearás manifiestos para dev, staging y prod, cada uno con valores apropiados.

**Tareas:**

1. Crear namespaces para ambientes:
+
[source,bash]
----
kubectl create namespace dev
kubectl create namespace staging
kubectl create namespace prod

# Verificar
kubectl get namespaces
----

2. Crear manifiestos base (compartidos):
+
[source,bash]
----
cat > base-app.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: nginx:1.24.0
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: myapp
spec:
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 80
EOF
----

3. Desplegar a DEV con réplica mínima:
+
[source,bash]
----
cat > dev-override.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 1  # DEV: mínimo
  template:
    spec:
      containers:
      - name: app
        image: nginx:1.24.0
        resources:
          requests:
            cpu: 50m
            memory: 32Mi
          limits:
            cpu: 100m
            memory: 64Mi
EOF

kubectl apply -n dev -f base-app.yaml
kubectl patch deployment myapp -n dev --patch-file dev-override.yaml

# Verificar
kubectl get pods -n dev
----

4. Desplegar a STAGING con configuración media:
+
[source,bash]
----
cat > staging-override.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 2  # STAGING: dos replicas
  template:
    spec:
      containers:
      - name: app
        image: nginx:1.24.0
        resources:
          requests:
            cpu: 200m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi
EOF

kubectl apply -n staging -f base-app.yaml
kubectl patch deployment myapp -n staging --patch-file staging-override.yaml

# Verificar
kubectl get pods -n staging
----

5. Desplegar a PROD con alta disponibilidad:
+
[source,bash]
----
cat > prod-override.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 5  # PROD: 5 replicas
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 0
  template:
    spec:
      containers:
      - name: app
        image: nginx:1.24.0
        resources:
          requests:
            cpu: 500m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 512Mi
        readinessProbe:
          httpGet:
            path: /
            port: 80
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /
            port: 80
          periodSeconds: 10
EOF

kubectl apply -n prod -f base-app.yaml
kubectl patch deployment myapp -n prod --patch-file prod-override.yaml

# Verificar
kubectl get pods -n prod
----

6. Ver resumen de todos los ambientes:
+
[source,bash]
----
echo "=== DEV ==="
kubectl get deployment -n dev
kubectl get pods -n dev | wc -l

echo ""
echo "=== STAGING ==="
kubectl get deployment -n staging
kubectl get pods -n staging | wc -l

echo ""
echo "=== PROD ==="
kubectl get deployment -n prod
kubectl get pods -n prod | wc -l
----

7. Actualizar versión solo en STAGING primero:
+
[source,bash]
----
# Staging es ambiente de "canary" antes de prod
kubectl set image deployment/myapp app=nginx:1.25.0 -n staging

# Verificar
kubectl rollout status deployment/myapp -n staging
----

8. Después de validar, actualizar PROD:
+
[source,bash]
----
# Una vez validado en staging, desplegar a producción
kubectl set image deployment/myapp app=nginx:1.25.0 -n prod

# Monitorear el rolling update
kubectl rollout status deployment/myapp -n prod --watch
----

9. Mantener DEV con versión de desarrollo:
+
[source,bash]
----
# DEV puede tener latest para pruebas rápidas
kubectl set image deployment/myapp app=nginx:latest -n dev

# Ver diferencias
echo "DEV:"
kubectl get deployment myapp -n dev -o jsonpath='{.spec.template.spec.containers[0].image}'

echo ""
echo "STAGING:"
kubectl get deployment myapp -n staging -o jsonpath='{.spec.template.spec.containers[0].image}'

echo ""
echo "PROD:"
kubectl get deployment myapp -n prod -o jsonpath='{.spec.template.spec.containers[0].image}'
----

10. Limpiar:
+
[source,bash]
----
kubectl delete namespace dev staging prod
rm -f base-app.yaml dev-override.yaml staging-override.yaml prod-override.yaml
----

**Preguntas de reflexión:**

1. ¿Cuál es el beneficio de tener STAGING idéntico a PROD?
2. ¿Cómo automatizarías la promoción de versiones dev → staging → prod?
3. ¿Qué pasaría si DEV y PROD comparten la misma base de datos?

---

=== Ejercicio 11.10: Rollout Control y Monitoring

**Objetivo de aprendizaje:**
Controlar el proceso de rollout con pausas y reanudaciones. Monitorear el estado de despliegues.

**Descripción:**
Crearás un rollout y practicarás pausing/resuming para control manual del proceso.

**Tareas:**

1. Crear Deployment para rollout control:
+
[source,bash]
----
cat > rollout-control.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-rollout
spec:
  replicas: 8
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 2
  selector:
    matchLabels:
      app: app-rollout
  template:
    metadata:
      labels:
        app: app-rollout
    spec:
      containers:
      - name: app
        image: nginx:1.24.0
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 2
          periodSeconds: 2
EOF

kubectl apply -f rollout-control.yaml

# Verificar
kubectl get pods -l app=app-rollout
----

2. Ver historial de rollouts:
+
[source,bash]
----
kubectl rollout history deployment/app-rollout

# Salida esperada:
# REVISION  CHANGE-CAUSE
# 1         <none>
----

3. Iniciar rollout a nueva versión:
+
[source,bash]
----
# En una ventana, monitorear
kubectl get pods -l app=app-rollout -w

# En otra ventana, iniciar update
# (esperar a que primera ventana esté lista)
----

4. Iniciar el update (ventana 2):
+
[source,bash]
----
kubectl set image deployment/app-rollout app=nginx:1.25.0 --record

# Ver estado
kubectl rollout status deployment/app-rollout
----

5. Pausar el rollout a mitad de camino:
+
[source,bash]
----
# Esperar a que el update esté a mitad
# (se verá en la ventana de monitoreo con pods antiguos y nuevos)

# Pausar el update
kubectl rollout pause deployment/app-rollout

echo "Update paused - observar estado"
kubectl get pods -l app=app-rollout

# Ver replicasets
kubectl get rs -l app=app-rollout
----

6. Investigar durante pausa:
+
[source,bash]
----
# Mientras está pausado, verificar status
kubectl rollout status deployment/app-rollout

# Ver que algunos Pods tienen versión nueva, otros vieja
kubectl get pods -l app=app-rollout -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[0].image}{"\n"}{end}'
----

7. Resumir el rollout:
+
[source,bash]
----
echo "Resumiendo update..."

kubectl rollout resume deployment/app-rollout

# Monitorear hasta completar
kubectl rollout status deployment/app-rollout --watch
----

8. Ver historial después del rollout:
+
[source,bash]
----
kubectl rollout history deployment/app-rollout

# Ahora debería haber 2 revisiones
# REVISION  CHANGE-CAUSE
# 1         <none>
# 2         kubectl set image ...
----

9. Ver detalles de cada revisión:
+
[source,bash]
----
# Revisión 1 (original)
kubectl rollout history deployment/app-rollout --revision=1

# Revisión 2 (actualizada)
kubectl rollout history deployment/app-rollout --revision=2
----

10. Hacer rollback y ver nuevo rollout:
+
[source,bash]
----
echo "=== Rollback a revisión anterior ==="

kubectl rollout undo deployment/app-rollout

# Ver status del undo (es otro rollout)
kubectl rollout status deployment/app-rollout

# Verificar que volvimos a imagen antigua
kubectl get pods -l app=app-rollout -o jsonpath='{range .items[*]}{.spec.containers[0].image}{"\n"}{end}' | sort | uniq -c
----

11. Ver historial después del rollback:
+
[source,bash]
----
kubectl rollout history deployment/app-rollout

# Ahora hay 3 revisiones (el undo crea una nueva revisión)
----

12. Limpiar:
+
[source,bash]
----
kubectl delete -f rollout-control.yaml
rm -f rollout-control.yaml
----

**Preguntas de reflexión:**

1. ¿Cuál es el caso de uso para pausar un rollout?
2. ¿Cómo sabría que está seguro resumir un rollout pausado?
3. ¿Cómo monitorizarías un rollout para detectar problemas automáticamente?

---

== Resumen de Conceptos

Estos ejercicios del Módulo 11 cubren:

* **Ejercicio 11.1**: Blue-Green Deployment
* **Ejercicio 11.2**: Canary Deployment
* **Ejercicio 11.3**: Rolling Update Avanzado
* **Ejercicio 11.4**: GitOps (ArgoCD Simulation)
* **Ejercicio 11.5**: Image Pull Secrets
* **Ejercicio 11.6**: Image Scanning
* **Ejercicio 11.7**: Health Checks Impacting Rollouts
* **Ejercicio 11.8**: Recreate vs Rolling Strategy
* **Ejercicio 11.9**: Multi-Environment Deployments
* **Ejercicio 11.10**: Rollout Control y Monitoring

== Próximos Pasos

Una vez completes estos ejercicios, estarás listo para:
* Módulo 12: Service Mesh
* Módulo 13: Advanced Operations
* Módulo 14: Disaster Recovery & Backup
* Y módulos posteriores...
